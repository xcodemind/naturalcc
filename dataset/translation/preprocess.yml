preprocess:
  no_progress_bar: 0 # action='store_true', help='disable progress bar'
  log_interval: 100
  log_format: ~ # ', default=None, help='log format to use', choices=['json', 'none', 'simple', 'tqdm']
  tensorboard_logdir: '' # metavar='DIR', default='', help='path to save logs for tensorboard, should match --logdir of running tensorboard (default: no tensorboard logging)'
  seed: 1 # default=1, type=int, metavar='N', help='pseudo random number generator seed'
  cpu: 0 # ', action='store_true', help='use CPU instead of CUDA'
  fp16: 0 # ', action='store_true', help='use FP16'
  memory_efficient_fp16: 0 # ', action='store_true', help='use a memory-efficient version of FP16 training; implies --fp16'
  fp16_no_flatten_grads: 0 #', action='store_true', help='don\'t flatten FP16 grads tensor'
  fp16_init_scale: 128 #', default=2 ** 7, type=int, help='default FP16 loss scale'
  fp16_scale_window: ~ #', type=int, help='number of updates before increasing loss scale'
  fp16_scale_tolerance: 0.0 #', default=0.0, type=float, help='pct of updates that can overflow before decreasing the loss scale'
  min_loss_scale: 1e-4 #', default=1e-4, type=float, metavar='D', help='minimum FP16 loss scale, after which training is stopped'
  threshold_loss_scale: ~ #', type=float, help='threshold FP16 loss scale from below'
  user_dir: ~ #, default=None, help='path to a python module containing custom extensions (tasks and/or architectures)'
  empty_cache_freq: 0 #', default=0, type=int, help='how often to clear the PyTorch CUDA cache (0 to disable)'
  all_gather_list_size: 16384 #', default=16384, type=int, help='number of bytes reserved for gathering stats from workers'

  task: translation # task', metavar='TASK', default="translation", choices=TASK_REGISTRY.keys(), help='task'
  source_lang: de  #", default=None, metavar="SRC", help="source language"
  target_lang: en #", default=None, metavar="TARGET", help="target language"
  trainpref: /data/wanyao/ghproj_d/naturalcodev3/iwslt14/iwslt14.tokenized.de-en/train #", metavar="FP", default=None, help="train file prefix"
  validpref: /data/wanyao/ghproj_d/naturalcodev3/iwslt14/iwslt14.tokenized.de-en/valid #", metavar="FP", default=None, help="comma separated, valid file prefixes"
  testpref: /data/wanyao/ghproj_d/naturalcodev3/iwslt14/iwslt14.tokenized.de-en/test # ", metavar="FP", default=None, help="comma separated, test file prefixes"
  align_suffix: ~ # ", metavar="FP", default=None, help="alignment file suffix"
  destdir: /data/wanyao/ghproj_d/naturalcodev3/iwslt14/iwslt14.tokenized.de-en/dest-bin #", metavar="DIR", default="data-bin", help="destination dir"
  thresholdtgt: 0 #", metavar="N", default=0, type=int, help="map words appearing less than threshold times to unknown"
  thresholdsrc: 0 #", metavar="N", default=0, type=int, help="map words appearing less than threshold times to unknown"
  tgtdict: ~ # ", metavar="FP", help="reuse given target dictionary"
  srcdict: ~ #", metavar="FP", help="reuse given source dictionary"
  nwordstgt: -1 #", metavar="N", default=-1, type=int, help="number of target words to retain"
  nwordssrc: -1 #", metavar="N", default=-1, type=int, help="number of source words to retain"
  alignfile: ~ #", metavar="ALIGN", default=None, help="an alignment file (optional)"
  dataset_impl: raw #', metavar='FORMAT', default='mmap', choices=get_available_dataset_impl(), help='output dataset implementation'
  joined_dictionary: 0 # ", action="store_true", help="Generate joined dictionary"
  only_source: 0 # ", action="store_true", help="Only process the source language"
  padding_factor: 8 #", metavar="N", default=8, type=int, help="Pad dictionary size to be multiple of N"
  workers: 1 # ", metavar="N", default=1, type=int, help="number of parallel workers"

## cli_main()
#    # parser = get_parser("Preprocessing", default_task)
#    parser = argparse.ArgumentParser()
#    # Before creating the true parser, we need to import optional user module
#    # in order to eagerly import custom tasks, optimizers, architectures, etc.
#    usr_parser = argparse.ArgumentParser(add_help=False, allow_abbrev=False)
#    usr_parser.add_argument("--user-dir", default=None)
#    usr_args, _ = usr_parser.parse_known_args()
#    # utils.import_user_module(usr_args)
#
#    parser = argparse.ArgumentParser(allow_abbrev=False)
#    # fmt: off
#    parser.add_argument('--no-progress-bar', action='store_true', help='disable progress bar')
#    parser.add_argument('--log-interval', type=int, default=100, metavar='N',
#                        help='log progress every N batches (when progress bar is disabled)')
#    parser.add_argument('--log-format', default=None, help='log format to use',
#                        choices=['json', 'none', 'simple', 'tqdm'])
#    parser.add_argument('--tensorboard-logdir', metavar='DIR', default='',
#                        help='path to save logs for tensorboard, should match --logdir '
#                             'of running tensorboard (default: no tensorboard logging)')
#    parser.add_argument('--seed', default=1, type=int, metavar='N',
#                        help='pseudo random number generator seed')
#    parser.add_argument('--cpu', action='store_true', help='use CPU instead of CUDA')
#    parser.add_argument('--fp16', action='store_true', help='use FP16')
#    parser.add_argument('--memory-efficient-fp16', action='store_true',
#                        help='use a memory-efficient version of FP16 training; implies --fp16')
#    parser.add_argument('--fp16-no-flatten-grads', action='store_true',
#                        help='don\'t flatten FP16 grads tensor')
#    parser.add_argument('--fp16-init-scale', default=2 ** 7, type=int,
#                        help='default FP16 loss scale')
#    parser.add_argument('--fp16-scale-window', type=int,
#                        help='number of updates before increasing loss scale')
#    parser.add_argument('--fp16-scale-tolerance', default=0.0, type=float,
#                        help='pct of updates that can overflow before decreasing the loss scale')
#    parser.add_argument('--min-loss-scale', default=1e-4, type=float, metavar='D',
#                        help='minimum FP16 loss scale, after which training is stopped')
#    parser.add_argument('--threshold-loss-scale', type=float,
#                        help='threshold FP16 loss scale from below')
#    parser.add_argument('--user-dir', default=None,
#                        help='path to a python module containing custom extensions (tasks and/or architectures)')
#    parser.add_argument('--empty-cache-freq', default=0, type=int,
#                        help='how often to clear the PyTorch CUDA cache (0 to disable)')
#    parser.add_argument('--all-gather-list-size', default=16384, type=int,
#                        help='number of bytes reserved for gathering stats from workers')
#
#    from ncc.registry import REGISTRIES
#
#    for registry_name, REGISTRY in REGISTRIES.items():
#        parser.add_argument(
#            '--' + registry_name.replace('_', '-'),
#            default=REGISTRY['default'],
#            choices=REGISTRY['registry'].keys(),
#        )
#
#    # Task definitions can be found under fairseq/tasks/
#    from ncc.tasks import TASK_REGISTRY
#
#    parser.add_argument('--task', metavar='TASK', default="translation",
#                        choices=TASK_REGISTRY.keys(),
#                        help='task')
#    # fmt: on
#
#    # ===========================
#    group = parser.add_argument_group("Preprocessing")
#    # fmt: off
#    group.add_argument("-s", "--source-lang", default=None, metavar="SRC",
#                       help="source language")
#    group.add_argument("-t", "--target-lang", default=None, metavar="TARGET",
#                       help="target language")
#    group.add_argument("--trainpref", metavar="FP", default=None,
#                       help="train file prefix")
#    group.add_argument("--validpref", metavar="FP", default=None,
#                       help="comma separated, valid file prefixes")
#    group.add_argument("--testpref", metavar="FP", default=None,
#                       help="comma separated, test file prefixes")
#    group.add_argument("--align-suffix", metavar="FP", default=None,
#                       help="alignment file suffix")
#    group.add_argument("--destdir", metavar="DIR", default="data-bin",
#                       help="destination dir")
#    group.add_argument("--thresholdtgt", metavar="N", default=0, type=int,
#                       help="map words appearing less than threshold times to unknown")
#    group.add_argument("--thresholdsrc", metavar="N", default=0, type=int,
#                       help="map words appearing less than threshold times to unknown")
#    group.add_argument("--tgtdict", metavar="FP",
#                       help="reuse given target dictionary")
#    group.add_argument("--srcdict", metavar="FP",
#                       help="reuse given source dictionary")
#    group.add_argument("--nwordstgt", metavar="N", default=-1, type=int,
#                       help="number of target words to retain")
#    group.add_argument("--nwordssrc", metavar="N", default=-1, type=int,
#                       help="number of source words to retain")
#    group.add_argument("--alignfile", metavar="ALIGN", default=None,
#                       help="an alignment file (optional)")
#    parser.add_argument('--dataset-impl', metavar='FORMAT', default='mmap',
#                        choices=get_available_dataset_impl(),
#                        help='output dataset implementation')
#    group.add_argument("--joined-dictionary", action="store_true",
#                       help="Generate joined dictionary")
#    group.add_argument("--only-source", action="store_true",
#                       help="Only process the source language")
#    group.add_argument("--padding-factor", metavar="N", default=8, type=int,
#                       help="Pad dictionary size to be multiple of N")
#    group.add_argument("--workers", metavar="N", default=1, type=int,
#                       help="number of parallel workers")
#    args = parser.parse_args()
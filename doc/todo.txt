naturalcodev2

尝试模型加速： https://www.aiuai.cn/aifarm1205.html

（->代表命名为）
ParserZips目录改为小写 parser_zips会不会更好点

数据处理步骤：
1. download_and_process_dataset.py->explore_dataset.py，然后内部实现拆封成几个函数
包括三个内容：
（1）download()  if data存在 pass，else download dataset
（2）preprocess() 对下载下来的dataset加入我们的一些preprocess
（3）explore() 主要可以实现对数据的一些探索，包括数据统计，最大长度最小长度统计之类的。之后也可以慢慢加入功能。

(yang)已经完成

2.目前的step2和step3我觉得可以合并一下，因为step2其实就是构建了一个能够parse各种语言的parser。

3. parse_ast.py->parse_code.py parse_code.py应该支持parse code的AST，SBT，以后可能加入CFG等等。里面的deepcom()和deepcom2()命名上
把deepcom换成sbt。因为deepcom是论文名，里面的核心就是将code转成sbt序列。在parse阶段我们不用太依赖这个论文名。

4. parse_baseline_dataset.py -> build_dataset.py 这个主要功能就是将各个步骤运行一下，得到最终的pt文件。parse_dict.py我觉得也可以合并
到这个文件里面，作为一个函数。（build_dataset.py我觉得就代表默认就行，没必要加上baseline这个词，因为baseline这个词表示啥有歧义，baseline
一般指的是别人论文中的方法，有点confused）

5. parse_baseline_dataset.py -> build_dataset_deepcom.py 两种不同的sbt方式在这一个文件里面实现就可以了。

这样dataset目录下只需要explore_dataset.py，parse_code.py，build_dataset.py三个文件即可

(yang回答2~5)jd写的sbt代码不是连贯的,我对此经验不足,暂时不会考虑把这些整合.名字已经改得比较简洁了.
我写了dataset/run_scripts/run.sh脚本,用于debug和学习使用.
目前时间比较紧,我看你也不会该这些内容了,主要是我们要考虑数据读写的问题.也就是问题6

6. AST目前存储是以json存下来的，然后加了一个NODEFIX的常量，这个是我最开始的设计，有没有更好的设计方式？AST本来是一个类，由于类在save成pt文
件的时候会报错，涉及到对象的序列化，所以我当时就设计了一个这样的json格式，不一定是最好的方式。

(yang)目前把NODE_FIX改为'NODEFIX',可以节省一点空间和序列号时间.无疑torch.load的读取是最快的,这个不好改.

========
目前急需做的 (due: Sep. 25)
说明：各自按任务优先级进行开发，完成了的项我们就移动到已完成部分，把前面的+变成-，自己的计划也及时添加上来，这样相互之间能知道对方大致计划和进展。
+(JD) KD在seq模态下的迁移性能
+(JD) 把KD合并到smtl-www20.py文件中，通过opt.train_mode == 'train_kd': 进行KD训练切换
+(JD) 把做icse论文的一些画图脚本整理到exp中，然后提炼出一些画图工具包，然后我们到时候可以快速调用。
+(JD) _load_data 和load_data 可以合并成一个函数，然后稍微精简一点。


+(Yao) nhid和ninp的区别，这两个变量用的有点乱。
+(Yao) GlobalAttention，包括dot_product, bilinear等几种方式。
+(Yao) 保证A2C能运行。
+(Yao) train_rl_sc的evaluator无法运行
+(Yao) 对decoder.forward_pg生成的mask质疑，Yang认可的正确方案参考smtl_trainer.py中的mask(line 122-126)生成（该方案参考TRL源码）。

+(Yang) 把baseline-hy.py, debug_baseline-hy.py等文件去掉，move到smtl-www20.py中，通过run文件来配置参数运行，如果觉得debug起来麻烦可以另起一个文件smtl-www20-hy.py，debug好了之后再改到run文件中，因为目前Yao&JD都是run文件进行debug。
+(Yang) 把MAML合并到smtl-www20.py文件中，通过opt.train_mode == 'train_maml': 进行MAML训练切换
+(Yang) 合并自己的工程到naturalcode中，保证能运行
+(Yang) DRTL在seq模态下的迁移性能
+(Yang) MAML在seq模态下的迁移性能


已完成：
- (Yao) 保证SC，PG能运行。
- (Yang) maml能运行，但是需要额外的脚本优化
- (JD) encoder_seq, encoder_ast, encoder_cfg 的输出应该是一样的。包括enc_output, (enc_hidden_state,enc_cell_state) , mask从batch里取
- (JD) PGCriterion_REINFORCE 和 PGCriterion_REINFORCE_no_gather 合并到一个函数PGCriterion_REINFORCE，加一个flag应该能解决，和PGCriterion_REINFORCE_no_gather现在好像没用到，估计用不上了，可以直接删掉。
- (JD) python数据的batch获取有问题。在第二次batch获取时，code与comment的维度不一致。目前只有python数据集存在这个问题
- (Yang) TRL和MAML初始代码完成。TRL存在loss收敛过快问题，需要evaluator。麻烦Yao尽快完成evaluator，用于测试。MAML的最终方案无法确定，看论文中。。。。

以后要慢慢考虑进来的TODO：
1. 主要模块，例如attention之类的，可以在对应的文件中加main函数，把测试内容写在里面，这样更容易理解，也方便测试
2. 所有的dict改成vocab，因为dict和python自带的关键词dict重名了。
3. 三个模态的code的nn.Embedding用同一个
4. 重构dataset，再dataset加一个main函数用于测试构造的dataset正确否。例如，取出一个样本，将其打印出来，或者可视化出来，看生成的ast cfg对不对。


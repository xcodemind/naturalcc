- CODE2SEQ: GENERATING SEQUENCES FROM STRUCTURED REPRESENTATIONS OF CODE, ICLR2019
- CodeBERT: A Pre-Trained Model for Programming and Natural Languages, arxiv 2020
- STRUCTBERT: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING, ICLR2020
- Novel positional encodings to enable tree-based transformers, NeurIPS 2019
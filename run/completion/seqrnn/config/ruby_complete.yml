##################### default args #####################
common:
#  device: 0 # GPU device. if None, as CPU
  no_progress_bar: 0 # action='store_true', help='disable progress bar'
  log_interval: 1 # type=int, default=100, metavar='N', help='log progress every N batches (when progress bar is disabled)'
  log_format: simple # default=None, help='log format to use', choices=['json', 'none', 'simple', 'tqdm']
  tensorboard_logdir: '' # default='', help='path to save logs for tensorboard, should match --logdir of running tensorboard (default: no tensorboard logging)'
#  seed: 42 # default=1, type=int, metavar='N', help='pseudo random number generator seed'
#  cpu: 0 # action='store_true', help='use CPU instead of CUDA'
#  fp16_fair: 1 # action='store_true', help='use FP16'
  memory_efficient_fp16: 0 # action='store_true', help='use a memory-efficient version of FP16 training; implies --fp16'
  fp16_no_flatten_grads: 0 # action='store_true', help='don\'t flatten FP16 grads tensor'
  fp16_init_scale: 128 # 2 ** 7 # default=2 ** 7, type=int, help='default FP16 loss scale'
  fp16_scale_window: ~ # type=int, help='number of updates before increasing loss scale'
  fp16_scale_tolerance: 0.0 # default=0.0, type=float, help='pct of updates that can overflow before decreasing the loss scale'
  min_loss_scale: 1e-4 # default=1e-4, type=float, metavar='D', help='minimum FP16 loss scale, after which training is stopped'
  threshold_loss_scale: ~ # type=float, help='threshold FP16 loss scale from below'
#  user_dir: ~ # default=None, help='path to a python module containing custom extensions (tasks and/or architectures)'
  empty_cache_freq: 0 # type=int, help='how often to clear the PyTorch CUDA cache (0 to disable)'
#  all_gather_list_size: 16384 # default=16384, type=int, help='number of bytes reserved for gathering stats from workers'
  task: completion # task

  seed: 42 # default=1, type=int, metavar='N', help='pseudo random number generator seed'
  cpu: 0 # action='store_true', help='use CPU instead of CUDA'
  fp16: 0 # action='store_true', help='use FP16'
  fp16_opt_level: '01' # type=str, default="O1", help="For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html",
  server_ip: '' # type=str, default="", help="For distant debugging."
  server_port: '' # type=str, default="", help="For distant debugging."


dataset:
  num_workers: 1 # default=1, type=int, metavar='N', help='how many subprocesses to use for data loading'
  skip_invalid_size_inputs_valid_test: 0 # action='store_true', help='ignore too long or too short lines in valid and test set'
  max_tokens: ~ # type=int, metavar='N', help='maximum number of tokens in a batch'
  max_sentences: 1 # '--batch-size', type=int, metavar='N', help='maximum number of sentences in a batch'
  required_batch_size_multiple: 8 # default=8, type=int, metavar='N', help='batch size will be a multiplier of this value'
  dataset_impl: raw # choices=get_available_dataset_impl(), help='output dataset implementation'
  train_subset: train # default='train', metavar='SPLIT', help='data subset to use for training (e.g. train, valid, test)'
  valid_subset: valid # default='valid', metavar='SPLIT', help='comma separated list of data subsets to use for validation (e.g. train, valid, test)'
  validate_interval: 1 # type=int, default=1, metavar='N', help='validate every N epochs'
  fixed_validation_seed: ~ # default=None, type=int, metavar='N', help='specified random seed for validation'
  disable_validation: 1 # action='store_true',help='disable validation'
  max_tokens_valid: ~ # type=int, metavar='N', help='maximum number of tokens in a validation batch (defaults to --max-tokens)'
  max_sentences_valid: 16 # type=int, metavar='N', help='maximum number of sentences in a validation batch (defaults to --max-sentences)'
  curriculum: 0 # default=0, type=int, metavar='N', help='don\'t shuffle batches for first N epochs'
  gen_subset: 'test' # , default='test', metavar='SPLIT', help='data subset to generate (train, valid, test)'
  num_shards: 1 # ', default=1, type=int, metavar='N', help='shard generation over N shards'
  shard_id: 0 # , default=0, type=int, metavar='ID', help='id of the shard to generate (id < num_shards)'
  output_word_probs: ~ # ', action='store_true', help='if set, outputs words and their predicted log probabilities to standard output'
  output_word_stats: ~ # ', action='store_true', help='if set, outputs word statistics such as word count, average probability, etc'
  context_window: 0 # ', default=0, type=int, metavar='N', help='ensures that every evaluated token has access to a context of at least this size, if possible'
  softmax_batch: ~ # ', default=sys.maxsize, type=int, metavar='N', help='if BxT is more than this, will batch the softmax over vocab to this amount of tokens in order to fit into GPU memory'


eval:
  path: ~/.ncc/py150/seqrnn/checkpoints/ # , metavar='FILE', help='path(s) to model file(s), colon separated'
  remove_bpe: ~ # remove-bpe', nargs='?', const='@@ ', default=None, help='remove BPE tokens before scoring (can be set to sentencepiece)'
  quiet: ~ # ', action='store_true', help='only print final scores'
  model_overrides: {} # default="{}", type=str, metavar='DICT', help='a dictionary used to override model args at generation '
  results_path: ~ # , metavar='RESDIR', type=str, default=None, help='path to save eval results (optional)"


task:
  data: ~/.ncc/py150/seqrnn/data-raw # /tmp/pycharm_project_629/data-bin help='colon separated path to data directories list, will be iterated upon during epochs in round-robin manner'
  sample_break_mode: complete # choices=['none', 'complete', 'complete_doc', 'eos'], help='If omitted or "none", fills each sample with tokens-per-sample tokens. If set to "complete", splits samples only at the end of sentence, but may include multiple sentences per sample. "complete_doc" is similar but respects doc boundaries. If set to "eos", includes only one sentence per sample.'
  tokens_per_sample: 512 # max_positions, default=512, type=int, help='max number of total tokens over all segments per sample for BERT dataset'
  output_dictionary_size: -1 #', default=-1, type=int, help='limit the size of output dictionary'
  self_target: ~ #', action='store_true', help='include self target'
  future_target: ~ #', action='store_true', help='include future target'
  past_target: ~ #', action='store_true', help='include past target'
  add_bos_token: ~ #', action='store_true', help='prepend beginning of sentence token (<s>)'
  max_target_positions: ~ # ', type=int, metavar='N', help='max number of tokens in the target sequence'
  truncate_sequence: ~ #', action='store_true', default=False, help='truncate sequences to --tokens-per-sample'


distributed:
  distributed_world_size: 1 # default=max(1, torch.cuda.device_count()
  distributed_rank: 0 # default=0, type=int, help='rank of the current worker'
  distributed_backend: nccl # default='nccl', type=str, help='distributed backend'
  distributed_init_method: ~ # default=None, type=str,help='typically tcp://hostname:port that will be used to establish initial connetion'
  distributed_port: -1 # default=-1, type=int, help='port number (not required if using --distributed-init-method)'
  device_id: 8 # '--local_rank', default=0, type=int, help='which GPU to use (usually configured automatically)'
#  local_rank: 0 #
  distributed_no_spawn: 0 # action='store_true', help='do not spawn multiple processes even if multiple GPUs are visible'
  ddp_backend: c10d # default='c10d', type=str, choices=['c10d', 'no_c10d'], help='DistributedDataParallel backend'
  bucket_cap_mb: 25 # default=25, type=int, metavar='MB', help='bucket size for reduction'
  fix_batches_to_gpus: ~ # action='store_true', help='don\'t shuffle batches between GPUs; this reduces overall randomness and may affect precision but avoids the cost of re-reading the data'
  find_unused_parameters: 0 # default=False, action='store_true', help='disable unused parameter detection (not applicable to no_c10d ddp-backend'
  fast_stat_sync: 0 # default=False, action='store_true', help='[deprecated] this is now defined per Criterion'
  broadcast_buffers: 0 # default=False, action='store_true', help='Copy non-trainable parameters between GPUs, such as batchnorm population statistics'
  global_sync_iter: 50 # default=50, type=int, help="Iteration for syncing global model",
  warmup_iterations: 500 # default=500, type=int, help="warmup iterations for model to broadcast",
  local_rank: -1 # type=int, default=-1, help="For distributed training: local_rank"
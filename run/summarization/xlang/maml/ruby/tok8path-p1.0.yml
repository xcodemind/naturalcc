##################### default args #####################
common:
  thread_num: 1 # load and process data with multi-process
  seed: 666 # init seed for numpy/torch/random/scipy and so on
  device: 0 # GPU device. if None, as CPU

  #  init_weights: ~  # None，xulu， xulu2，xulu3

  #  init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/maml_java-javascript-php-python_ruby_Adam(0.0004)_SGD(0.001)-10-1/train_maml/tok8path-bs128-Adam(0.0004)-mSGD(0.001)-EPOCH10-1-5.pt'
  #  init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/maml_java-javascript-php-python_ruby_Adam(0.0004)_SGD(0.001)-10-1/train_maml/tok8path-bs128-Adam(0.0004)-mSGD(0.001)-EPOCH10-1-245.pt'

  #  all
  #  init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/maml_java-javascript-php-python_ruby_Adam(0.0004)_SGD(0.001)--1--1.new/train_maml/tok8path-bs128-Adam(0.0004)-mSGD(0.001)-EPOCH-1--1-115.pt'

  #  few-shot， 0%, 0.01%, 0.1%,  1%
  #  init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/maml_java-javascript-php-python_ruby_Adam(0.0004)_SGD(0.001)-10-1.new/train_maml/tok8path-bs128-Adam(0.0004)-mSGD(0.001)-EPOCH10-1-290.pt'
  #  portion = 0,               pt= '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/maml_java-javascript-php-python_ruby_Adam(0.0004)_SGD(0.001)-10-1.new/train_maml/tok8path-bs128-Adam(0.0004)-mSGD(0.001)-EPOCH10-1-290.pt'
  #  portion = 0.0001, lr=1e-4, pt= '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/maml_ft_0.0001_Adam(0.0001)/tok8path-bs128-lr0.0001-attndot-pointerTrue-ttFTTrainer-ep3.pt'
  #  portion = 0.001,  lr=5e-4, pt= '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/maml_ft_0.001_Adam(0.0005)/tok8path-bs128-lr0.0005-attndot-pointerTrue-ttFTTrainer-ep4.pt'
  #  portion = 0.01,  lr=5e-4, pt=

  #  sl
  #  init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/maml_java-javascript-php-python_ruby_Adam(0.0004)_SGD(0.001)-10-1.new/train_maml/tok8path-bs128-Adam(0.0004)-mSGD(0.001)-EPOCH10-1-1220.pt'

  #  0.2, 0.4, 0.6, 0.8
  #  init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/maml_java-javascript-php-python_ruby_Adam(0.0004)_SGD(0.001)-10-1.new/train_maml/tok8path-bs128-Adam(0.0004)-mSGD(0.001)-EPOCH10-1-1340.pt'

  #  1.0
#  init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/maml_java-javascript-php-python_ruby_Adam(0.0004)_SGD(0.001)-10-1.new/train_maml/tok8path-bs128-Adam(0.0004)-mSGD(0.001)-EPOCH10-1-3000.pt'
#  init_weights: '/data/wanyao/yang/ghproj_d/GitHub/naturalcodev2/save_pt/tok8path-bs128-lr0.0005-attndot-pointerTrue-ttFTTrainer-ep8.pt'
  #  portion = 1.0,  lr=5e-4, pt= '/data/wanyao/yang/ghproj_d/GitHub/naturalcodev2/save_pt/tok8path-bs128-lr0.0005-attndot-pointerTrue-ttFTTrainer-ep8.pt'


#  zero-shot
#  init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/maml_java-javascript-php-python_ruby_Adam(0.0004)_SGD(0.001)-10-1.new/train_maml/tok8path-bs128-Adam(0.0004)-mSGD(0.001)-EPOCH10-1-250.pt'
#  p=0,      | 0.2062 | 0.0544 | 0.0116 | 0.003 |    0.099 | 0.2348 | 0.0466 | 0.0108 | 0.0036 | 0.2164 |  0.3687 |
#  p=0.0001, | 0.2144 | 0.0554 | 0.0108 | 0.0026 |   0.1001 | 0.2343 | 0.0469 | 0.01 | 0.0032 | 0.2135 |  0.3663 |
#  p=0.001,  | 0.1988 | 0.0506 | 0.0112 | 0.0031 |   0.0989 | 0.2349 | 0.0461 | 0.0118 | 0.0037 | 0.2158 |  0.3907 |
#  p=0.01,   | 0.2043 | 0.0546 | 0.014 | 0.0043 |   0.1014 | 0.2448 | 0.0495 | 0.014 | 0.0037 | 0.2257 |  0.4352 |
#  p=0.2,    | 0.1977 | 0.0544 | 0.0115 | 0.0024 |   0.1021 | 0.2508 | 0.0505 | 0.0122 | 0.0023 | 0.2323 |  0.4381 |
#  p=0.4,    | 0.2175 | 0.0676 | 0.0185 | 0.005 |   0.1071 | 0.2542 | 0.0588 | 0.0181 | 0.0054 | 0.2345 |  0.4551 |
#  p=0.6,    | 0.2287 | 0.0709 | 0.0228 | 0.0067 |   0.1094 | 0.261 | 0.0655 | 0.021 | 0.0049 | 0.2378 |  0.4797 |
#  p=0.8,    | 0.2293 | 0.0745 | 0.0215 | 0.0078 |   0.1122 | 0.2656 | 0.0641 | 0.0193 | 0.0046 | 0.2439 |  0.5034 |

#sc
#  bleu
#  init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/maml_ft_0.01_Adam(0.0005)/tok8path-bs128-lr0.0005-attndot-pointerTrue-ttFTTrainer-ep6.pt'
#  init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/python-javascript-php-java_p0.01_biTrue/train_maml_sc/tok8path-bs128-lr0.0001-attndot-pointerTrue-ttSCTrainer-ep1.pt'
#  | 0.2264 | 0.0643 | 0.0147 | 0.0042 |   0.1055 | 0.2394 | 0.0517 | 0.014 | 0.004 | 0.2205 |  0.4003 |
#
#  cider
  init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/maml_ft_0.01_Adam(0.0005)/tok8path-bs128-lr0.0005-attndot-pointerTrue-ttFTTrainer-ep4.pt'
#  init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/python-javascript-php-java_p0.01_biTrue/train_maml_sc/tok8path-bs128-lr0.0001-attndot-pointerTrue-ttSCTrainer-ep1.pt'


  init_bound: 1e-1
  init_normal_std: 1e-4
  result_table_format: 'github' # latex (https://pypi.org/project/tabulate/)

dataset:
  dataset_dir: '/data/wanyao/yang/ghproj_d/GitHub/datasetv2/key/100_small'
  # save your model and eval files
  save_dir: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization'
  ##################### dataset and other args #####################
  # dict files(tok, tree, path sbt, sbt2, code_tokens, docstring_tokens, method)
  # load from code_modalities

  tree_leaf_subtoken: True

  #  only 1 batch for ft, while portion=0.1%, 0.01%
  portion: 0.01 # only for evaluate meta-learner, using part data is quick


  leaf_path_k: 30
  #  lr: 1e-3, k=30
  #  | 0.2314 | 0.0736 | 0.0229 | 0.0075 |   0.1104 | 0.2579 | 0.065 | 0.0207 | 0.006 | 0.2351 |  0.4703 |
  #  | 0.2292 | 0.073 | 0.0238 | 0.0097 |   0.1118 | 0.26 | 0.069 | 0.0247 | 0.0089 | 0.2382 |  0.5025 |

  #  lr: 1e-3, k=50
  #| 0.2281 | 0.0677 | 0.0192 | 0.0067 |    0.108 | 0.2527 | 0.0619 | 0.0179 | 0.0055 | 0.2328 |    0.45 |
  #| 0.2236 | 0.0726 | 0.0219 | 0.0095 |   0.1101 | 0.2633 | 0.0703 | 0.0237 | 0.0076 | 0.2411 |  0.5097 |

  source:
    dataset_lng:
      - 'python'
      - 'javascript'
      - 'php'
      - 'java'

    mode:
      - 'train'
      - 'valid'

  target:
    dataset_lng:
      - 'ruby'

    mode:
      - 'train'
      - 'valid'
      - 'test'


training:
  ##################### model args #####################
  #  summarization: tok, ast, path sbt, sbt2
  #  retrieval: code_tokens, docstring_tokens, method
  code_modalities:
    - tok
    - path

  train_epoch: 20
  batch_size: 128
  log_interval: 5 # write log info per log_interval iteration

  # network: encoder
  rnn_type: 'LSTM' # RNN type: 'GRU','LSTM', LSTM as default
  rnn_layer_num: 1 # RNN layer num
  rnn_hidden_size: 512 # RNN hidden size
  rnn_bidirectional: True

  embed_size: 300 # word-embedding size
  embed_pooling: ~

  tree_lstm_cell_type: 'nary' # DGL tree LSTM cell, if nary -> TreeLSTMCell, else    -> ChildSumTreeLSTMCell
  code_modal_transform: True # code modalities transform with FCs
  conv2d_out_channels: 512
  conv2d_kernels:
    - 2
    - 3
    - 4
    - 5

  # encoder RNN's output as decoder RNN's hidden_state(h, c)
  # 1) None, for zero_init as decoder RNN's hidden_state
  # 2) h, for only encoder RNN's output as decoder RNN's hidden_state(h)
  # 3) c, for only encoder RNN's output as decoder RNN's hidden_state(c)
  # 3) hc, for encoder RNN's output as decoder RNN's hidden_state(h, c)
  enc_hc2dec_hc: 'h'

  # network: decoder
  attn_type: 'dot' # ~, general, dot, mlp, intra
  attn_unit: 512
  self_attn_size: 50
  intra_attn_eps: 1e-12
  pointer: True # pointer-generator
  max_predict_length: 65 # max generation length for decoder
  dropout: 0.2 # dropout
  decoder_input_feed: False # （固定，不要调）True, False


# inference
testing:
  beam_size: 10
  max_predict_length: ~ # max generation length for decoder

  metrics:
    - 'bleu'
#    - 'meteor'
#    - 'rouge'
    - 'cider'

# supervised learning
sl:
  optim: 'Adam'
#  optim: 'SGD'
  lr: 5e-4 # 0.5179
#  lr: 1e-4 # 0.5179

  lr_gamma: 0.5
  lr_milestones: # epochs when lr => lr * lr_gamma
    - 5 # 不要改 not for portion=1.0
    - 50

  warmup_factor: 0.2
  warmup_epochs: 1

  max_grad_norm: -1 # -1: no gradient clips  1.0



# supervised learning
pg:
  optim: 'Adam'
  lr: 1e-4
  reward_func: 'bleu'

#  lr_gamma: 0.1 # 0.1
#  lr_milestones: # epochs when lr => lr * lr_gamma
#    - 20 # 1e-4
#    - 40 # 1e-5
#  warmup_factor: 0.2
#  warmup_epochs: 1

sc:
  optim: 'Adam'
  lr: 1e-4
  reward_func: 'bleu'

  lr_gamma: 0.1 # 0.1
  lr_milestones: # epochs when lr => lr * lr_gamma
    - 20 # 1e-4
    - 40 # 1e-5
  warmup_factor: 0.2
  warmup_epochs: 1

  max_grad_norm: -1 # -1: no gradient clips  1.0

  rl_weight: 1.0



ac:
  optim: 'Adam'
  optim_critic: 'Adam'
  lr: 1e-4
  lr_critic: 1e-4
  #  init_weights_critic: ~ # ~ None，xulu， xulu2，xulu3
  init_weights_critic: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/ruby/train_critic/critic-tok-bs128-lr0.001-attnNone-pointerFalse-ep50.pt'
  train_epoch_critic: 50
  train_epoch_ac: 50

gan:
  optim: 'Adam'
  optim_disc: 'Adam'
  lr: 1e-4
  lr_disc: 1e-4
  comment_fake_path: '/data/wanyao/yang/ghproj_d/GitHub/datasetv2/key/100/ruby/comment_fake'
  init_weights_disc: ~ # ~ None，xulu， xulu2，xulu3
  #  init_weights_disc: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/ruby/train_critic/critic-tok-bs128-lr0.001-attnNone-pointerFalse-ep50.pt'


  train_epoch_disc: 50
  train_epoch_gan: 50

arel:
  optim: 'Adam'
  optim_reward_model: 'Adam'
  lr: 1e-4
  lr_reward_model: 1e-4
  init_weights_reward_model: ~ # ~ None，xulu， xulu2，xulu3
  #  init_weights_reward_model: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/ruby/train_critic/critic-tok-bs128-lr0.001-attnNone-pointerFalse-ep50.pt'
  train_epoch_reward_model: 50
  train_epoch_arel: 50

  D_iter: 100
  G_iter: 100
  always: False
# fine tuning


# meta learning
maml:
  #  meta_optim: 'Adam' # meta-optimizer
  meta_optim: 'SGD' # meta-optimizer
  meta_lr: 1e-3 # meta-learning rate

  meta_train_size: -1
  meta_val_size: -1

  mini_finetune_epoch: 10



dtrl:
  # if you need 2 or more than 2 languages to merge together before feeding networks, set it as True
  # generally used for depp transfer reinforcement learning (DTRL)
  merge_xlng: True

  optim: 'Adam'
  lr: 4e-4
  lr_gamma: 0.1 # 0.1
  lr_milestones: # epochs when lr => lr * lr_gamma
    - 20 # 1e-4
    - 40 # 1e-5
  warmup_factor: 0.2
  warmup_epochs: 1
  max_grad_norm: -1 # -1: no gradient clips  1.0

  reward_func: 'bleu'

kd:
  xxx: ~

##################### default args #####################
common:
  thread_num: 2   # （固定，不要调）load and process data with multi-process
  seed: 666 # （固定，不要调）init seed for numpy/torch/random/scipy and so on
  device: 0   # GPU device. if None, as CPU

  init_weights: ~  # None, xulu, xulu2, xulu3
#  init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/ruby/train_sl/tok8path_pointerTrue/tok8path-bs128-lr0.0004-attndot-pointerTrue-best-bleu1.pt'


  init_bound: 1e-1 # （固定，不要调）
  init_normal_std: 1e-4 # （固定，不要调）
  result_table_format: 'github' # （固定，不要调）latex (https://pypi.org/project/tabulate/)


dataset:

  # 100_small
  save_dir: '/data/sjd/d/p_d/fse20all/100_small/result'  # realdoctor
  dataset_dir: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/key/100_small' # realdoctor
  #dir: '/data/wy/ghproj_d/fse20all/100_small' # rtx
  #dataset_dir: '/data/wanyao/yang/ghproj_d/GitHub/datasetv2/key/100_small' # rtx

  ##################### dataset and other args #####################
  # dict files(tok, tree, path sbt, sbt2, code_tokens, docstring_tokens, method)
  # load from code_modalities

  tree_leaf_subtoken: True
  portion: ~ # 0.4 # train data portion, 0~1.0
  leaf_path_k: 30 #（固定，不要调）
  # if you need 2 or more than 2 languages to merge together before feeding networks, set it as True
  # generally used for transfer reinforcement learning (TRL)
  merge_xlng: False

  source_domain:
    source:
      dataset_lng:
        - 'python'
        - 'javascript'
        - 'php'
        - 'java'

      select_lng: # must be list type to adapt to other code in the project
        - 'python' # ~


      mode:
        - 'train'
        - 'valid'
        - 'test'
    target: # for kd summerization , target is None , then be converted to ['en'] in naturalcodev2/src/dataset/kd_dataloader.py
      ~
  target_domain:
    source:
      dataset_lng:
        - 'ruby'
      mode:
        - 'train'
        - 'valid'
        - 'test'
    target: # for kd summerization , target is None , then be converted to ['en'] in naturalcodev2/src/dataset/kd_dataloader.py
      ~
training:
  ##################### model args #####################
  #  summarization: tok, ast, path sbt, sbt2
  #  retrieval: code_tokens, docstring_tokens, method
  code_modalities:
    - tok
#    - ast
    - path

  train_epoch: 50 # （固定，不要调）50已经足够，一般20-30epoch就基本收敛了
#  batch_size: 128 # 128
  batch_size: 128     # 128 16
  log_interval: 5 # （固定，不要调）write log info per log_interval iteration

  # network: encoder
  rnn_type: 'LSTM' # （固定，不要调）RNN type: 'GRU','LSTM', LSTM as default
  rnn_layer_num: 1 # （固定，不要调）RNN layer num
  rnn_hidden_size: 512 # （固定，不要调）RNN hidden size
#  rnn_bidirectional: False # （固定，不要调）
  rnn_bidirectional: True  # （固定，不要调）
  embed_size: 300 # （固定，不要调）word-embedding size
  tree_lstm_cell_type: 'nary' # （固定，不要调）DGL tree LSTM cell, if nary -> TreeLSTMCell, else    -> ChildSumTreeLSTMCell
  code_modal_transform: False # （固定，不要调）False # code modalities transform with FCs
  conv2d_out_channels: 512 # （固定，不要调）
  conv2d_kernels: # （固定，不要调）
    - 2
    - 3
    - 4
    - 5

  # encoder RNN's output as decoder RNN's hidden_state(h, c)
  # 1) None, for zero_init as decoder RNN's hidden_state
  # 2) h, for only encoder RNN's output as decoder RNN's hidden_state(h)
  # 3) c, for only encoder RNN's output as decoder RNN's hidden_state(c)
  # 3) hc, for encoder RNN's output as decoder RNN's hidden_state(h, c)
  enc_hc2dec_hc: 'h' # （固定，不要调）'hc', 'h'

  # network: decoder
  attn_type: dot # ~, general, dot, mlp, intra
  attn_unit: 512 # （固定，不要调）
  self_attn_size: 50 # （固定，不要调）
#  intra_attn_eps: 1e-12
  pointer: True # pointer-generator, True, False
  max_predict_length: 65  # （固定，不要调）max generation length for decoder
  dropout: 0.2 # （固定，不要调）dropout
  decoder_input_feed: False # （固定，不要调）True, False

  # used to create the corresponding saved folder
  # ~, enc_hc2dec_hc, optim_AdamW, optim_Adagrad, optim_RMSprop, optim_SGD, decoder_input_feed_True
  # lr_0.01, lr_0.0001, lr_0.0004, lr_0.001_milestones1040,
  # max_grad_norm1.0, max_grad_norm10, max_grad_norm20, max_grad_norm100,
  # dropout_0.2, dropout_0.4, dropout_0.6, dropout_0.8,
  # rnn_layer_num_2, rnn_bidirectional_True
  # tok8path_lr_0.001, tok8path_lr_0.0001
  # tok8path_lr0.01_gamma0.1_milestones1040, tok8path_lr0.001_gamma0.5_milestones1040, tok8path_lr0.001_gamma0.5_milestones10203040,     tok8path_lr0.0004_gamma0.5_milestones2040, tok8path_lr0.0004_gamma0.5_milestones2040_code_modal_transformFalse
#  tok8path_lr0.001ep20-lr0.0004
#  attnDot, pointerTrue, tok8path_attnDot, tok8path_pointerTrue
  tuning: ~ # 这里的tuning是parameter tuning，我在这里加一个组合名字后，我的模型会存到相应的目录中，因为我们现在的model_name有点简单，容易重名字


# inference
testing:
  beam_size: 10
#  max_predict_length: ~ # max generation length for decoder
  init_weights : ~
#  init_weights: 'tok8path-bs128-lr0.0004-attndot-pointerTrue-orin-slm1-best-bleu1.pt' # after sl fintune student on ruby
#  init_weights: 'tok8path-bs128-lr0.0004-attndot-pointerTrue-orin-scm1-ttSCTrainer-best-bleu1.pt' # after sc finetune sl model(already finetuned with student model) on ruby
#  init_weights: 'tok8path-bs128-lr0.0004-attndot-pointerTrue-ep17-tttrain_sl-diFalse-slngjava-d0.2-hch-afsthreshold-ka0.5-dk8-dp0.6-ls0.1-kt0.05-sFalse-oNone-se[2, 1]-pr1.0-biFalse.pt' #
#  init_weights: 'tok8path-bs128-lr0.0004-attndot-pointerTrue-ep14-tttrain_sl-diFalse-slngjava-d0.2-hch-afsthreshold-ka0.5-dk8-dp0.6-ls0.1-kt0.05-sFalse-oNone-seNone-pr1.0-biTrue.pt' #
  metrics:
    - 'bleu'
    - 'cider'
#    - 'rouge'
#    - 'meteor'

# supervised learning
sl:
  optim: 'Adam' # （固定，不要调）'Adam', 'AdamW', 'Adagrad', 'RMSprop', 'SGD'

# default
  lr: 4e-4 # 默认最优
  lr_gamma: 0.5 # （固定，不要调）0.1
  lr_milestones: # 默认最优，epochs when lr => lr * lr_gamma
    - 20 # 1e-4
    - 40 # 1e-5

  warmup_factor: 0.2 #（固定，不要调）
  warmup_epochs: -1 # （固定，不要调）-1: no warmup
  max_grad_norm: -1 # （固定，不要调）-1: no gradient clips  1.0, 10, 20, 100

  oriname2finetune: ~
  init_weights: ~
#  oriname2finetune: 'slm1' # for kd finetune # dj
#  init_weights: 'tok8path-bs128-lr0.0004-attndot-pointerTrue-ep4-tttrain_sl-diTrue-slngjava-d0.2-hch-afsthreshold-ka0.5-dk8-dp0.6-ls0.1-kt0.05-sTrue-oNone-se7_17-pr1.0-biFalse.pt'
#  oriname2finetune: 'slm2' # for kd finetune # dj
#  init_weights: 'tok8path-bs128-lr0.0004-attndot-pointerTrue-ep5-tttrain_sl-diTrue-slngpython-d0.2-hch-afsthreshold-ka0.5-dk8-dp0.6-ls0.1-kt0.05-sTrue-oNone-se10_14-pr1.0-biTrue.pt'


# self-critical
sc:
  optim: 'Adam'
  lr: 1e-4
  max_grad_norm: -1 # -1: no gradient clips  1.0
  reward_func: 'bleu'
#  rl_weight: 0.8
  rl_weight: 1

  oriname2finetune: ~ # for kd finetune # dj
  init_weights: ~
#  oriname2finetune: 'scm1' # for kd finetune # dj
#  init_weights: 'tok8path-bs128-lr0.0004-attndot-pointerTrue-orin-slm1-best-bleu1.pt'
#  oriname2finetune: 'scm2' # for kd finetune # dj
#  init_weights: 'tok8path-bs128-lr0.0004-attndot-pointerTrue-orin-slm2-ttSLTrainer-best-bleu1.pt'


kd: # dj
  distill: False # False

  sources_epoch: ~ #
#  sources_epoch: # 100_small
#    - 7
#    - 17
#  sources_epoch: # 100_small
#    - 10
#    - 14


  distill_topk: 8
#  distill_temp: 1
#  distill_temp: 1.4
  distill_temp: 0.6
  kd_threshold: 0.05
  kd_default_alpha : 0.5
  alpha_strategy: 'threshold'
  label_smooth_rate: 0.1

  #  rarely changed
#  shuffle: True
  shuffle: False



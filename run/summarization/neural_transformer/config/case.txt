["def getRadialPath(begin, center, end, path):\n\tbeginComplex = begin.dropAxis()\n\tendComplex = end.dropAxis()\n\tcenterComplex = center.dropAxis()\n\tbeginMinusCenterComplex = (beginComplex - centerComplex)\n\tendMinusCenterComplex = (endComplex - centerComplex)\n\tbeginMinusCenterComplexRadius = abs(beginMinusCenterComplex)\n\tendMinusCenterComplexRadius = abs(endMinusCenterComplex)\n\tif ((beginMinusCenterComplexRadius == 0.0) or (endMinusCenterComplexRadius == 0.0)):\n\t\treturn [begin]\n\tbeginMinusCenterComplex \/= beginMinusCenterComplexRadius\n\tendMinusCenterComplex \/= endMinusCenterComplexRadius\n\tangleDifference = euclidean.getAngleDifferenceByComplex(endMinusCenterComplex, beginMinusCenterComplex)\n\tradialPath = []\n\tfor point in path:\n\t\tweightEnd = point.x\n\t\tweightBegin = (1.0 - weightEnd)\n\t\tweightedRadius = ((beginMinusCenterComplexRadius * weightBegin) + ((endMinusCenterComplexRadius * weightEnd) * (1.0 + point.y)))\n\t\tradialComplex = ((weightedRadius * euclidean.getWiddershinsUnitPolar((angleDifference * point.x))) * beginMinusCenterComplex)\n\t\tpolygonPoint = (center + Vector3(radialComplex.real, radialComplex.imag, point.z))\n\t\tradialPath.append(polygonPoint)\n\treturn radialPath\n",["get radial path ."]]
["def getArcDistance(relativeLocation, splitLine):\n\thalfPlaneLineDistance = (0.5 * abs(relativeLocation.dropAxis(2)))\n\tradius = getDoubleFromCharacterSplitLine('R', splitLine)\n\tif (radius == None):\n\t\tiFloat = getDoubleFromCharacterSplitLine('I', splitLine)\n\t\tjFloat = getDoubleFromCharacterSplitLine('J', splitLine)\n\t\tradius = abs(complex(iFloat, jFloat))\n\tangle = 0.0\n\tif (radius > 0.0):\n\t\thalfPlaneLineDistanceOverRadius = (halfPlaneLineDistance \/ radius)\n\t\tif (halfPlaneLineDistance < radius):\n\t\t\tangle = (2.0 * math.asin(halfPlaneLineDistanceOverRadius))\n\t\telse:\n\t\t\tangle = (math.pi * halfPlaneLineDistanceOverRadius)\n\treturn abs(complex((angle * radius), relativeLocation.z))\n",["get arc distance ."]]
["def service_delete(service_id=None, name=None, profile=None, **connection_args):\n\tkstone = auth(profile, **connection_args)\n\tif name:\n\t\tservice_id = service_get(name=name, profile=profile, **connection_args)[name]['id']\n\tkstone.services.delete(service_id)\n\treturn 'Keystone\tservice\tID\t\"{0}\"\tdeleted'.format(service_id)\n",["service delete ."]]
["def win32_utf8_argv():\n\ttry:\n\t\tfrom ctypes import POINTER, byref, cdll, c_int, windll\n\t\tfrom ctypes.wintypes import LPCWSTR, LPWSTR\n\t\tGetCommandLineW = cdll.kernel32.GetCommandLineW\n\t\tGetCommandLineW.argtypes = []\n\t\tGetCommandLineW.restype = LPCWSTR\n\t\tCommandLineToArgvW = windll.shell32.CommandLineToArgvW\n\t\tCommandLineToArgvW.argtypes = [LPCWSTR, POINTER(c_int)]\n\t\tCommandLineToArgvW.restype = POINTER(LPWSTR)\n\t\tcmd = GetCommandLineW()\n\t\targc = c_int(0)\n\t\targv = CommandLineToArgvW(cmd, byref(argc))\n\t\tif (argc.value > 0):\n\t\t\treturn [argv[i] for i in xrange(0, argc.value)]\n\texcept Exception:\n\t\tpass\n",["uses shell32 ."]]
["def localOutp(images, hidSums, targets, numModulesX, paddingStart, filterSizeX, moduleStride, numImgColors):\n\tnumGroups = 1\n\tpartialSum = 0\n\tnumImages = images.shape[0]\n\tnumFilters = (hidSums.shape[1] \/ (numModulesX ** 2))\n\tassert (targets.shape == (numFilters, (((numModulesX ** 2) * numImgColors) * (filterSizeX ** 2)))), ('%s\t%d\t%d-%d-%d' % (targets.shape.__str__(), numFilters, numImgColors, filterSizeX, filterSizeX))\n\t_ConvNet.localOutp(images.p_mat, hidSums.p_mat, targets.p_mat, numModulesX, filterSizeX, (- paddingStart), moduleStride, numImgColors, numGroups, partialSum)\n",["images - hidsums - targets - ."]]
["def positional(max_positional_args):\n\tdef positional_decorator(wrapped):\n\t\t@functools.wraps(wrapped)\n\t\tdef positional_wrapper(*args, **kwargs):\n\t\t\tif (len(args) > max_positional_args):\n\t\t\t\tplural_s = ''\n\t\t\t\tif (max_positional_args != 1):\n\t\t\t\t\tplural_s = 's'\n\t\t\t\tmessage = ('%s()\ttakes\tat\tmost\t%d\tpositional\targument%s\t(%d\tgiven)' % (wrapped.__name__, max_positional_args, plural_s, len(args)))\n\t\t\t\tif (positional_parameters_enforcement == POSITIONAL_EXCEPTION):\n\t\t\t\t\traise TypeError(message)\n\t\t\t\telif (positional_parameters_enforcement == POSITIONAL_WARNING):\n\t\t\t\t\tlogger.warning(message)\n\t\t\t\telse:\n\t\t\t\t\tpass\n\t\t\treturn wrapped(*args, **kwargs)\n\t\treturn positional_wrapper\n\tif isinstance(max_positional_args, six.integer_types):\n\t\treturn positional_decorator\n\telse:\n\t\t(args, _, _, defaults) = inspect.getargspec(max_positional_args)\n\t\treturn positional((len(args) - len(defaults)))(max_positional_args)\n",["a decorator to declare that only the first n arguments my be positional ."]]
["def load_grammar(gt='Grammar.txt', gp=None, save=True, force=False, logger=None):\n\tif (logger is None):\n\t\tlogger = logging.getLogger()\n\tif (gp is None):\n\t\t(head, tail) = os.path.splitext(gt)\n\t\tif (tail == '.txt'):\n\t\t\ttail = ''\n\t\tgp = (((head + tail) + '.'.join(map(str, sys.version_info))) + '.pickle')\n\tif (force or (not _newer(gp, gt))):\n\t\tlogger.info('Generating\tgrammar\ttables\tfrom\t%s', gt)\n\t\tg = pgen.generate_grammar(gt)\n\t\tif save:\n\t\t\tlogger.info('Writing\tgrammar\ttables\tto\t%s', gp)\n\t\t\ttry:\n\t\t\t\tg.dump(gp)\n\t\t\texcept IOError as e:\n\t\t\t\tlogger.info(('Writing\tfailed:' + str(e)))\n\telse:\n\t\tg = grammar.Grammar()\n\t\tg.load(gp)\n\treturn g\n",["load the grammar ."]]
["def make_thumbnail(in_fname, out_fname, width, height):\n\timg = Image.open(in_fname)\n\t(width_in, height_in) = img.size\n\tscale_w = (width \/ float(width_in))\n\tscale_h = (height \/ float(height_in))\n\tif ((height_in * scale_w) <= height):\n\t\tscale = scale_w\n\telse:\n\t\tscale = scale_h\n\twidth_sc = int(round((scale * width_in)))\n\theight_sc = int(round((scale * height_in)))\n\timg.thumbnail((width_sc, height_sc), Image.ANTIALIAS)\n\tthumb = Image.new('RGB', (width, height), (255, 255, 255))\n\tpos_insert = (((width - width_sc) \/ 2), ((height - height_sc) \/ 2))\n\tthumb.paste(img, pos_insert)\n\tthumb.save(out_fname)\n",["make a thumbnail with the same aspect ratio centered in an image with a given width and height ."]]
["def verify(user, password):\n\tdef verify_user(user_name, user_password):\n\t\tif ((user_name == user) and (user_password == password)):\n\t\t\treturn user_name\n\t\treturn False\n\treturn verify_user\n",["verifies that the signature matches the message ."]]
["def getCarving(fileName):\n\tpluginModule = fabmetheus_interpret.getInterpretPlugin(fileName)\n\tif (pluginModule == None):\n\t\treturn None\n\treturn pluginModule.getCarving(fileName)\n",["get the triangle mesh for the stl file ."]]
["def test_rgb_to_hsl_part_15():\n\tassert (rgb_to_hsl(0, 51, 0) == (120, 100, 10))\n\tassert (rgb_to_hsl(0, 102, 0) == (120, 100, 20))\n\tassert (rgb_to_hsl(0, 153, 0) == (120, 100, 30))\n\tassert (rgb_to_hsl(0, 204, 0) == (120, 100, 40))\n\tassert (rgb_to_hsl(0, 255, 0) == (120, 100, 50))\n\tassert (rgb_to_hsl(51, 255, 51) == (120, 100, 60))\n\tassert (rgb_to_hsl(102, 255, 102) == (120, 100, 70))\n\tassert (rgb_to_hsl(153, 255, 153) == (120, 100, 80))\n\tassert (rgb_to_hsl(204, 255, 204) == (120, 100, 90))\n",["test rgb to hsl color function ."]]
["def get_netrc_auth(url):\n\ttry:\n\t\tlocations = (os.path.expanduser('~\/{0}'.format(f)) for f in NETRC_FILES)\n\t\tnetrc_path = None\n\t\tfor loc in locations:\n\t\t\tif (os.path.exists(loc) and (not netrc_path)):\n\t\t\t\tnetrc_path = loc\n\t\tif (netrc_path is None):\n\t\t\treturn netrc_path\n\t\tri = urlparse(url)\n\t\thost = ri.netloc.split(':')[0]\n\t\ttry:\n\t\t\t_netrc = netrc(netrc_path).authenticators(host)\n\t\t\tif _netrc:\n\t\t\t\tlogin_i = (0 if _netrc[0] else 1)\n\t\t\t\treturn (_netrc[login_i], _netrc[2])\n\t\texcept (NetrcParseError, IOError):\n\t\t\tpass\n\texcept (ImportError, AttributeError):\n\t\tpass\n",["returns the requests tuple auth for a given url from netrc ."]]
["def Error(filename, linenum, category, confidence, message):\n\tif _ShouldPrintError(category, confidence, linenum):\n\t\t_cpplint_state.IncrementErrorCount(category)\n\t\tif (_cpplint_state.output_format == 'vs7'):\n\t\t\tsys.stderr.write(('%s(%s):\t\t%s\t\t[%s]\t[%d]\\n' % (filename, linenum, message, category, confidence)))\n\t\telif (_cpplint_state.output_format == 'eclipse'):\n\t\t\tsys.stderr.write(('%s:%s:\twarning:\t%s\t\t[%s]\t[%d]\\n' % (filename, linenum, message, category, confidence)))\n\t\telse:\n\t\t\tsys.stderr.write(('%s:%s:\t\t%s\t\t[%s]\t[%d]\\n' % (filename, linenum, message, category, confidence)))\n",["logs the fact weve found a lint error ."]]
["def create_connection(address, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None):\n\t(host, port) = address\n\terr = None\n\tfor res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):\n\t\t(af, socktype, proto, canonname, sa) = res\n\t\tsock = None\n\t\ttry:\n\t\t\tsock = socket.socket(af, socktype, proto)\n\t\t\tsock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n\t\t\tif (timeout is not socket._GLOBAL_DEFAULT_TIMEOUT):\n\t\t\t\tsock.settimeout(timeout)\n\t\t\tif source_address:\n\t\t\t\tsock.bind(source_address)\n\t\t\tsock.connect(sa)\n\t\t\treturn sock\n\t\texcept socket.error as _:\n\t\t\terr = _\n\t\t\tif (sock is not None):\n\t\t\t\tsock.close()\n\tif (err is not None):\n\t\traise err\n\telse:\n\t\traise socket.error('getaddrinfo\treturns\tan\tempty\tlist')\n",["connect to *address* and return the socket object ."]]
["def mapping(data_source, geom_name='geom', layer_key=0, multi_geom=False):\n\tif isinstance(data_source, six.string_types):\n\t\tdata_source = DataSource(data_source)\n\telif isinstance(data_source, DataSource):\n\t\tpass\n\telse:\n\t\traise TypeError('Data\tsource\tparameter\tmust\tbe\ta\tstring\tor\ta\tDataSource\tobject.')\n\t_mapping = {}\n\tfor field in data_source[layer_key].fields:\n\t\tmfield = field.lower()\n\t\tif (mfield[(-1):] == '_'):\n\t\t\tmfield += 'field'\n\t\t_mapping[mfield] = field\n\tgtype = data_source[layer_key].geom_type\n\tif (multi_geom and (gtype.num in (1, 2, 3))):\n\t\tprefix = 'MULTI'\n\telse:\n\t\tprefix = ''\n\t_mapping[geom_name] = (prefix + str(gtype).upper())\n\treturn _mapping\n",["given a datasource ."]]
["def script_from_examples(s):\n\toutput = []\n\tfor piece in DocTestParser().parse(s):\n\t\tif isinstance(piece, Example):\n\t\t\toutput.append(piece.source[:(-1)])\n\t\t\twant = piece.want\n\t\t\tif want:\n\t\t\t\toutput.append('#\tExpected:')\n\t\t\t\toutput += [('##\t' + l) for l in want.split('\\n')[:(-1)]]\n\t\telse:\n\t\t\toutput += [_comment_line(l) for l in piece.split('\\n')[:(-1)]]\n\twhile (output and (output[(-1)] == '#')):\n\t\toutput.pop()\n\twhile (output and (output[0] == '#')):\n\t\toutput.pop(0)\n\treturn ('\\n'.join(output) + '\\n')\n",["extract script from text with examples ."]]
["def s3_unicode(s, encoding='utf-8'):\n\tif (type(s) is unicode):\n\t\treturn s\n\ttry:\n\t\tif (not isinstance(s, basestring)):\n\t\t\tif hasattr(s, '__unicode__'):\n\t\t\t\ts = unicode(s)\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\ts = unicode(str(s), encoding, 'strict')\n\t\t\t\texcept UnicodeEncodeError:\n\t\t\t\t\tif (not isinstance(s, Exception)):\n\t\t\t\t\t\traise\n\t\t\t\t\ts = '\t'.join([s3_unicode(arg, encoding) for arg in s])\n\t\telse:\n\t\t\ts = s.decode(encoding)\n\texcept UnicodeDecodeError:\n\t\tif (not isinstance(s, Exception)):\n\t\t\traise\n\t\ts = '\t'.join([s3_unicode(arg, encoding) for arg in s])\n\treturn s\n",["convert an object into an unicode instance ."]]
["def get_image_dimensions(file_or_path, close=False):\n\tfrom PIL import ImageFile as PillowImageFile\n\tp = PillowImageFile.Parser()\n\tif hasattr(file_or_path, 'read'):\n\t\tfile = file_or_path\n\t\tfile_pos = file.tell()\n\t\tfile.seek(0)\n\telse:\n\t\tfile = open(file_or_path, 'rb')\n\t\tclose = True\n\ttry:\n\t\tchunk_size = 1024\n\t\twhile 1:\n\t\t\tdata = file.read(chunk_size)\n\t\t\tif (not data):\n\t\t\t\tbreak\n\t\t\ttry:\n\t\t\t\tp.feed(data)\n\t\t\texcept zlib.error as e:\n\t\t\t\tif e.args[0].startswith('Error\t-5'):\n\t\t\t\t\tpass\n\t\t\t\telse:\n\t\t\t\t\traise\n\t\t\texcept struct.error:\n\t\t\t\tpass\n\t\t\tif p.image:\n\t\t\t\treturn p.image.size\n\t\t\tchunk_size *= 2\n\t\treturn (None, None)\n\tfinally:\n\t\tif close:\n\t\t\tfile.close()\n\t\telse:\n\t\t\tfile.seek(file_pos)\n",["returns the of an image ."]]
["def _mkstemp_inner(dir, pre, suf, flags):\n\tnames = _get_candidate_names()\n\tfor seq in xrange(TMP_MAX):\n\t\tname = names.next()\n\t\tfile = _os.path.join(dir, ((pre + name) + suf))\n\t\ttry:\n\t\t\tfd = _os.open(file, flags, 384)\n\t\t\t_set_cloexec(fd)\n\t\t\treturn (fd, _os.path.abspath(file))\n\t\texcept OSError as e:\n\t\t\tif (e.errno == _errno.EEXIST):\n\t\t\t\tcontinue\n\t\t\tif ((_os.name == 'nt') and (e.errno == _errno.EACCES)):\n\t\t\t\tcontinue\n\t\t\traise\n\traise IOError, (_errno.EEXIST, 'No\tusable\ttemporary\tfile\tname\tfound')\n",["code common to mkstemp ."]]
["def check_call(*popenargs, **kwargs):\n\tretcode = call(*popenargs, **kwargs)\n\tcmd = kwargs.get('args')\n\tif (cmd is None):\n\t\tcmd = popenargs[0]\n\tif retcode:\n\t\traise CalledProcessError(retcode, cmd)\n\treturn retcode\n",["takes a command to be passed to subprocess ."]]
["@contextlib.contextmanager\ndef temp_cwd(name='tempcwd', quiet=False):\n\tif (have_unicode and isinstance(name, unicode)):\n\t\ttry:\n\t\t\tname = name.encode((sys.getfilesystemencoding() or 'ascii'))\n\t\texcept UnicodeEncodeError:\n\t\t\tif (not quiet):\n\t\t\t\traise unittest.SkipTest('unable\tto\tencode\tthe\tcwd\tname\twith\tthe\tfilesystem\tencoding.')\n\tsaved_dir = os.getcwd()\n\tis_temporary = False\n\ttry:\n\t\tos.mkdir(name)\n\t\tos.chdir(name)\n\t\tis_temporary = True\n\texcept OSError:\n\t\tif (not quiet):\n\t\t\traise\n\t\twarnings.warn(('tests\tmay\tfail,\tunable\tto\tchange\tthe\tCWD\tto\t' + name), RuntimeWarning, stacklevel=3)\n\ttry:\n\t\t(yield os.getcwd())\n\tfinally:\n\t\tos.chdir(saved_dir)\n\t\tif is_temporary:\n\t\t\trmtree(name)\n",["context manager that creates a temporary directory and set it as cwd ."]]
["def MessageEncoder(field_number, is_repeated, is_packed):\n\ttag = TagBytes(field_number, wire_format.WIRETYPE_LENGTH_DELIMITED)\n\tlocal_EncodeVarint = _EncodeVarint\n\tassert (not is_packed)\n\tif is_repeated:\n\t\tdef EncodeRepeatedField(write, value):\n\t\t\tfor element in value:\n\t\t\t\twrite(tag)\n\t\t\t\tlocal_EncodeVarint(write, element.ByteSize())\n\t\t\t\telement._InternalSerialize(write)\n\t\treturn EncodeRepeatedField\n\telse:\n\t\tdef EncodeField(write, value):\n\t\t\twrite(tag)\n\t\t\tlocal_EncodeVarint(write, value.ByteSize())\n\t\t\treturn value._InternalSerialize(write)\n\t\treturn EncodeField\n",["returns an encoder for a message field ."]]
["def get_date_formats():\n\twarnings.warn(\"'django.utils.translation.get_date_formats'\tis\tdeprecated.\tPlease\tupdate\tyour\tcode\tto\tuse\tthe\tnew\ti18n\taware\tformatting.\", PendingDeprecationWarning)\n\tfrom google.appengine._internal.django.conf import settings\n\tdate_format = ugettext('DATE_FORMAT')\n\tdatetime_format = ugettext('DATETIME_FORMAT')\n\ttime_format = ugettext('TIME_FORMAT')\n\tif (date_format == 'DATE_FORMAT'):\n\t\tdate_format = settings.DATE_FORMAT\n\tif (datetime_format == 'DATETIME_FORMAT'):\n\t\tdatetime_format = settings.DATETIME_FORMAT\n\tif (time_format == 'TIME_FORMAT'):\n\t\ttime_format = settings.TIME_FORMAT\n\treturn (date_format, datetime_format, time_format)\n",["checks whether translation files provide a translation for some technical message id to store date and time formats ."]]
["def _get_head_types(pat):\n\tif isinstance(pat, (pytree.NodePattern, pytree.LeafPattern)):\n\t\tif (pat.type is None):\n\t\t\traise _EveryNode\n\t\treturn {pat.type}\n\tif isinstance(pat, pytree.NegatedPattern):\n\t\tif pat.content:\n\t\t\treturn _get_head_types(pat.content)\n\t\traise _EveryNode\n\tif isinstance(pat, pytree.WildcardPattern):\n\t\tr = set()\n\t\tfor p in pat.content:\n\t\t\tfor x in p:\n\t\t\t\tr.update(_get_head_types(x))\n\t\treturn r\n\traise Exception((\"Oh\tno!\tI\tdon't\tunderstand\tpattern\t%s\" % pat))\n",["accepts a pytree pattern node and returns a set of the pattern types which will match first ."]]
["def user_cache_dir(appname):\n\tif WINDOWS:\n\t\tpath = os.path.normpath(_get_win_folder('CSIDL_LOCAL_APPDATA'))\n\t\tpath = os.path.join(path, appname, 'Cache')\n\telif (sys.platform == 'darwin'):\n\t\tpath = expanduser('~\/Library\/Caches')\n\t\tpath = os.path.join(path, appname)\n\telse:\n\t\tpath = os.getenv('XDG_CACHE_HOME', expanduser('~\/.cache'))\n\t\tpath = os.path.join(path, appname)\n\treturn path\n",["return full path to the user-specific cache dir for this application ."]]
["def setcopyright():\n\t__builtin__.copyright = _Printer('copyright', sys.copyright)\n\tif (sys.platform[:4] == 'java'):\n\t\t__builtin__.credits = _Printer('credits', 'Jython\tis\tmaintained\tby\tthe\tJython\tdevelopers\t(www.jython.org).')\n\telif (sys.platform == 'cli'):\n\t\t__builtin__.credits = _Printer('credits', 'IronPython\tis\tmaintained\tby\tthe\tIronPython\tdevelopers\t(www.ironpython.net).')\n\telse:\n\t\t__builtin__.credits = _Printer('credits', '\t\t\t\tThanks\tto\tCWI,\tCNRI,\tBeOpen.com,\tZope\tCorporation\tand\ta\tcast\tof\tthousands\\n\t\t\t\tfor\tsupporting\tPython\tdevelopment.\t\tSee\twww.python.org\tfor\tmore\tinformation.')\n\there = os.path.dirname(os.__file__)\n\t__builtin__.license = _Printer('license', 'See\thttps:\/\/www.python.org\/psf\/license\/', ['LICENSE.txt', 'LICENSE'], [os.path.join(here, os.pardir), here, os.curdir])\n",["set copyright and credits in __builtin__ ."]]
["def reserve_vlanid():\n\tLOG.debug(_('reserve_vlanid()\tcalled'))\n\tsession = db.get_session()\n\ttry:\n\t\trvlan = session.query(network_models_v2.VlanID).filter_by(vlan_used=False).first()\n\t\tif (not rvlan):\n\t\t\traise exc.NoResultFound\n\t\trvlanid = session.query(network_models_v2.VlanID).filter_by(vlan_id=rvlan['vlan_id']).one()\n\t\trvlanid['vlan_used'] = True\n\t\tsession.merge(rvlanid)\n\t\tsession.flush()\n\t\treturn rvlan['vlan_id']\n\texcept exc.NoResultFound:\n\t\traise c_exc.VlanIDNotAvailable()\n",["reserves the first unused vlanid ."]]
["def getCarveIntersectionFromEdge(edge, vertexes, z):\n\tfirstVertex = vertexes[edge.vertexIndexes[0]]\n\tfirstVertexComplex = firstVertex.dropAxis(2)\n\tsecondVertex = vertexes[edge.vertexIndexes[1]]\n\tsecondVertexComplex = secondVertex.dropAxis(2)\n\tzMinusFirst = (z - firstVertex.z)\n\tup = (secondVertex.z - firstVertex.z)\n\treturn (((zMinusFirst * (secondVertexComplex - firstVertexComplex)) \/ up) + firstVertexComplex)\n",["get the complex where the carve intersects the edge ."]]
["def _string_dist_basic(str1, str2):\n\tstr1 = unidecode(str1)\n\tstr2 = unidecode(str2)\n\tstr1 = re.sub('[^a-z0-9]', '', str1.lower())\n\tstr2 = re.sub('[^a-z0-9]', '', str2.lower())\n\tif ((not str1) and (not str2)):\n\t\treturn 0.0\n\treturn (levenshtein(str1, str2) \/ float(max(len(str1), len(str2))))\n",["basic edit distance between two strings ."]]
["def pathmatch(pattern, filename):\n\tsymbols = {'?': '[^\/]', '?\/': '[^\/]\/', '*': '[^\/]+', '*\/': '[^\/]+\/', '**\/': '(?:.+\/)*?', '**': '(?:.+\/)*?[^\/]+'}\n\tbuf = []\n\tfor (idx, part) in enumerate(re.split('([?*]+\/?)', pattern)):\n\t\tif (idx % 2):\n\t\t\tbuf.append(symbols[part])\n\t\telif part:\n\t\t\tbuf.append(re.escape(part))\n\tmatch = re.match((''.join(buf) + '$'), filename.replace(os.sep, '\/'))\n\treturn (match is not None)\n",["extended pathname pattern matching ."]]
["def egg_link_path(dist):\n\treturn (os.path.join(site_packages, dist.project_name) + '.egg-link')\n",["return the path for the ."]]
["def move(src, dst):\n\treal_dst = dst\n\tif os.path.isdir(dst):\n\t\tif _samefile(src, dst):\n\t\t\tos.rename(src, dst)\n\t\t\treturn\n\t\treal_dst = os.path.join(dst, _basename(src))\n\t\tif os.path.exists(real_dst):\n\t\t\traise Error, (\"Destination\tpath\t'%s'\talready\texists\" % real_dst)\n\ttry:\n\t\tos.rename(src, real_dst)\n\texcept OSError:\n\t\tif os.path.isdir(src):\n\t\t\tif _destinsrc(src, dst):\n\t\t\t\traise Error, (\"Cannot\tmove\ta\tdirectory\t'%s'\tinto\titself\t'%s'.\" % (src, dst))\n\t\t\tcopytree(src, real_dst, symlinks=True)\n\t\t\trmtree(src)\n\t\telse:\n\t\t\tcopy2(src, real_dst)\n\t\t\tos.unlink(src)\n",["filelist \u662f\u4e00\u4e2alist ."]]
["def split_code_at_show(text):\n\tparts = []\n\tis_doctest = contains_doctest(text)\n\tpart = []\n\tfor line in text.split(u'\\n'):\n\t\tif (((not is_doctest) and (line.strip() == u'plt.show()')) or (is_doctest and (line.strip() == u'>>>\tplt.show()'))):\n\t\t\tpart.append(line)\n\t\t\tparts.append(u'\\n'.join(part))\n\t\t\tpart = []\n\t\telse:\n\t\t\tpart.append(line)\n\tif u'\\n'.join(part).strip():\n\t\tparts.append(u'\\n'.join(part))\n\treturn parts\n",["split code at plt ."]]
["def openwindow(object):\n\tfinder = _getfinder()\n\tobject = Carbon.File.FSRef(object)\n\tobject_alias = object.FSNewAliasMinimal()\n\targs = {}\n\tattrs = {}\n\t_code = 'aevt'\n\t_subcode = 'odoc'\n\taeobj_0 = aetypes.ObjectSpecifier(want=aetypes.Type('cfol'), form='alis', seld=object_alias, fr=None)\n\targs['----'] = aeobj_0\n\t(_reply, args, attrs) = finder.send(_code, _subcode, args, attrs)\n\tif args.has_key('errn'):\n\t\traise Error, aetools.decodeerror(args)\n",["open a finder window for object ."]]
["@pytest.mark.django_db\ndef test_verify_user_after_update_email(trans_member):\n\twith pytest.raises(EmailAddress.DoesNotExist):\n\t\tEmailAddress.objects.get(user=trans_member)\n\tassert (trans_member.email == '')\n\taccounts.utils.update_user_email(trans_member, 'trans_member@this.test')\n\taccounts.utils.verify_user(trans_member)\n\tEmailAddress.objects.get(user=trans_member, primary=True, verified=True)\n",["test verifying user using verify_user function ."]]
["def get_short_module_name(module_name, obj_name):\n\tparts = module_name.split('.')\n\tshort_name = module_name\n\tfor i in range((len(parts) - 1), 0, (-1)):\n\t\tshort_name = '.'.join(parts[:i])\n\t\ttry:\n\t\t\texec ('from\t%s\timport\t%s' % (short_name, obj_name))\n\t\texcept Exception:\n\t\t\tshort_name = '.'.join(parts[:(i + 1)])\n\t\t\tbreak\n\treturn short_name\n",["get the shortest possible module name ."]]
["@register.tag(u'if')\ndef do_if(parser, token):\n\tbits = token.split_contents()[1:]\n\tcondition = TemplateIfParser(parser, bits).parse()\n\tnodelist = parser.parse((u'elif', u'else', u'endif'))\n\tconditions_nodelists = [(condition, nodelist)]\n\ttoken = parser.next_token()\n\twhile token.contents.startswith(u'elif'):\n\t\tbits = token.split_contents()[1:]\n\t\tcondition = TemplateIfParser(parser, bits).parse()\n\t\tnodelist = parser.parse((u'elif', u'else', u'endif'))\n\t\tconditions_nodelists.append((condition, nodelist))\n\t\ttoken = parser.next_token()\n\tif (token.contents == u'else'):\n\t\tnodelist = parser.parse((u'endif',))\n\t\tconditions_nodelists.append((None, nodelist))\n\t\ttoken = parser.next_token()\n\tassert (token.contents == u'endif')\n\treturn IfNode(conditions_nodelists)\n",["the {% if %} tag evaluates a variable ."]]
["def get_profit_statement(code):\n\tif code.isdigit():\n\t\trequest = Request((ct.SINA_PROFITSTATEMENT_URL % code))\n\t\ttext = urlopen(request, timeout=10).read()\n\t\ttext = text.decode('GBK')\n\t\ttext = text.replace(' DCTB \\n', '\\r\\n')\n\t\ttext = text.replace(' DCTB ', ',')\n\t\tdf = pd.read_csv(StringIO(text), dtype={'code': 'object'})\n\t\treturn df\n",["parameters code:str \u80a1\u7968\u4ee3\u7801 e ."]]
["def getCraftedTextFromText(gcodeText, exportRepository=None):\n\tif gcodec.isProcedureDoneOrFileIsEmpty(gcodeText, 'export'):\n\t\treturn gcodeText\n\tif (exportRepository == None):\n\t\texportRepository = settings.getReadRepository(ExportRepository())\n\tif (not exportRepository.activateExport.value):\n\t\treturn gcodeText\n\treturn ExportSkein().getCraftedGcode(exportRepository, gcodeText)\n",["multiply the fill text ."]]
["def getIntegerFromCharacterLengthLineOffset(character, offset, splitLine, stepLength):\n\tlineFromCharacter = gcodec.getStringFromCharacterSplitLine(character, splitLine)\n\tif (lineFromCharacter == None):\n\t\treturn 0\n\tfloatValue = ((float(lineFromCharacter) + offset) \/ stepLength)\n\treturn int(round(floatValue))\n",["get the integer after the first occurence of the character in the split line ."]]
["def split_sections(s):\n\tsection = None\n\tcontent = []\n\tfor line in yield_lines(s):\n\t\tif line.startswith('['):\n\t\t\tif line.endswith(']'):\n\t\t\t\tif (section or content):\n\t\t\t\t\t(yield (section, content))\n\t\t\t\tsection = line[1:(-1)].strip()\n\t\t\t\tcontent = []\n\t\t\telse:\n\t\t\t\traise ValueError('Invalid\tsection\theading', line)\n\t\telse:\n\t\t\tcontent.append(line)\n\t(yield (section, content))\n",["split a string or iterable thereof into pairs each section is a stripped version of the section header and each content is a list of stripped lines excluding"]]
["def test_make_imbalance_2():\n\t(X_, y_) = make_imbalance(X, Y, ratio=0.25, min_c_=1)\n\tcounter = Counter(y_)\n\tassert_equal(counter[0], 500)\n\tassert_equal(counter[1], 125)\n\tassert_true(np.all([(X_i in X) for X_i in X_]))\n",["test make_imbalance ."]]
["def resolve_url(to, *args, **kwargs):\n\tif hasattr(to, 'get_absolute_url'):\n\t\treturn to.get_absolute_url()\n\ttry:\n\t\treturn urlresolvers.reverse(to, args=args, kwargs=kwargs)\n\texcept urlresolvers.NoReverseMatch:\n\t\tif callable(to):\n\t\t\traise\n\t\tif (('\/' not in to) and ('.' not in to)):\n\t\t\traise\n\treturn to\n",["return a url appropriate for the arguments passed ."]]
["def constant_time_compare(val1, val2):\n\tif (len(val1) != len(val2)):\n\t\treturn False\n\tresult = 0\n\tif (six.PY3 and isinstance(val1, bytes) and isinstance(val2, bytes)):\n\t\tfor (x, y) in zip(val1, val2):\n\t\t\tresult |= (x ^ y)\n\telse:\n\t\tfor (x, y) in zip(val1, val2):\n\t\t\tresult |= (ord(x) ^ ord(y))\n\treturn (result == 0)\n",["returns true if the two strings are equal ."]]
["@contextmanager\ndef temporary_file(suffix=''):\n\ttempfile_stream = NamedTemporaryFile(suffix=suffix, delete=False)\n\ttempfile = tempfile_stream.name\n\ttempfile_stream.close()\n\t(yield tempfile)\n\tos.remove(tempfile)\n",["this is a cross platform temporary file creation ."]]
["def resolve():\n\tfilename = '\/'.join(request.args)\n\tpath = apath(filename, r=request)\n\ta = safe_read(path).split('\\n')\n\ttry:\n\t\tb = safe_read((path + '.1')).split('\\n')\n\texcept IOError:\n\t\tsession.flash = 'Other\tfile,\tno\tlonger\tthere'\n\t\tredirect(URL('edit', args=request.args))\n\td = difflib.ndiff(a, b)\n\tdef leading(line):\n\t\t'\t\t'\n\t\tz = ''\n\t\tfor (k, c) in enumerate(line):\n\t\t\tif (c == '\t'):\n\t\t\t\tz += '&nbsp;'\n\t\t\telif (c == '\t DCTB '):\n\t\t\t\tz += '&nbsp;'\n\t\t\telif ((k == 0) and (c == '?')):\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tbreak\n\t\treturn XML(z)\n\tdef getclass(item):\n\t\t'\tDetermine\titem\tclass\t'\n\t\toperators = {'\t': 'normal', '+': 'plus', '-': 'minus'}\n\t\treturn operators[item[0]]\n\tif request.vars:\n\t\tc = '\\n'.join([item[2:].rstrip() for (i, item) in enumerate(d) if ((item[0] == '\t') or (('line%i' % i) in request.vars))])\n\t\tsafe_write(path, c)\n\t\tsession.flash = 'files\tmerged'\n\t\tredirect(URL('edit', args=request.args))\n\telse:\n\t\tgen_data = (lambda index, item: (((not (item[:1] in ['+', '-'])) and '') or INPUT(_type='checkbox', _name=('line%i' % index), value=(item[0] == '+'))))\n\t\tdiff = TABLE(*[TR(TD(gen_data(i, item)), TD(item[0]), TD(leading(item[2:]), TT(item[2:].rstrip())), _class=getclass(item)) for (i, item) in enumerate(d) if (item[0] != '?')])\n\treturn dict(diff=diff, filename=filename)\n",["given an object or a path to an object ."]]
["def _normalize_module(module, depth=2):\n\tif inspect.ismodule(module):\n\t\treturn module\n\telif isinstance(module, (str, unicode)):\n\t\treturn __import__(module, globals(), locals(), ['*'])\n\telif (module is None):\n\t\treturn sys.modules[sys._getframe(depth).f_globals['__name__']]\n\telse:\n\t\traise TypeError('Expected\ta\tmodule,\tstring,\tor\tNone')\n",["return the module specified by module ."]]
["def detectLinuxBrokenPipeBehavior():\n\t(r, w) = os.pipe()\n\tos.write(w, 'a')\n\t(reads, writes, exes) = select.select([w], [], [], 0)\n\tif reads:\n\t\tbrokenPipeBehavior = True\n\telse:\n\t\tbrokenPipeBehavior = False\n\tos.close(r)\n\tos.close(w)\n\treturn brokenPipeBehavior\n",["on some linux version ."]]
["def is_list(value):\n\treturn isinstance(value, list)\n",["does the node represent a list literal? ."]]
["@register.inclusion_tag(engine.get_template('inclusion.html'))\ndef inclusion_unlimited_args_from_template(one, two='hi', *args):\n\treturn {'result': ('inclusion_unlimited_args_from_template\t-\tExpected\tresult:\t%s' % ',\t'.join((str(arg) for arg in ([one, two] + list(args)))))}\n",["expected inclusion_unlimited_args_from_template __doc__ ."]]
["def base36_to_int(s):\n\tif (len(s) > 13):\n\t\traise ValueError(u'Base36\tinput\ttoo\tlarge')\n\tvalue = int(s, 36)\n\tif ((not six.PY3) and (value > sys.maxint)):\n\t\traise ValueError(u'Base36\tinput\ttoo\tlarge')\n\treturn value\n",["converts a base 36 string to an int ."]]
["@pipeline.mutator_stage\ndef lookup_candidates(session, task):\n\tif task.skip:\n\t\treturn\n\tplugins.send('import_task_start', session=session, task=task)\n\tlog.debug(u'Looking\tup:\t{0}'.format(displayable_path(task.paths)))\n\ttask.lookup_candidates()\n",["a coroutine for performing the initial musicbrainz lookup for an album ."]]
["def decode_params_utf8(params):\n\tdecoded = []\n\tfor (k, v) in params:\n\t\tdecoded.append(((k.decode('utf-8') if isinstance(k, str) else k), (v.decode('utf-8') if isinstance(v, str) else v)))\n\treturn decoded\n",["ensures that all parameters in a list of 2-element tuples are decoded to unicode using utf-8 ."]]
["def geos_version_info():\n\tver = geos_version().decode()\n\tm = version_regex.match(ver)\n\tif (not m):\n\t\traise GEOSException(('Could\tnot\tparse\tversion\tinfo\tstring\t\"%s\"' % ver))\n\treturn dict(((key, m.group(key)) for key in ('version', 'release_candidate', 'capi_version', 'major', 'minor', 'subminor')))\n",["returns a dictionary containing the various version metadata parsed from the geos version string ."]]
["@utils.arg('domain', metavar='<domain>', help=_('DNS\tdomain.'))\n@utils.arg('--availability-zone', metavar='<availability-zone>', default=None, help=_('Limit\taccess\tto\tthis\tdomain\tto\tservers\tin\tthe\tspecified\tavailability\tzone.'))\n@deprecated_network\ndef do_dns_create_private_domain(cs, args):\n\tcs.dns_domains.create_private(args.domain, args.availability_zone)\n",["create the specified dns domain ."]]
["def get_delta(name):\n\t[curr_metrics, last_metrics] = get_metrics()\n\tmetric_name_list = name.split('_')[1:]\n\tmetric_name = '_'.join(metric_name_list)\n\ttry:\n\t\tdelta = ((float(curr_metrics['data'][metric_name]) - float(last_metrics['data'][metric_name])) \/ (curr_metrics['time'] - last_metrics['time']))\n\t\tif (delta < 0):\n\t\t\tif Debug:\n\t\t\t\tprint (name + '\tis\tless\t0.\tSetting\tvalue\tto\t0.')\n\t\t\tdelta = 0\n\texcept KeyError:\n\t\tif Debug:\n\t\t\tprint (('Key\t' + name) + \"\tcan't\tbe\tfound.\")\n\t\tdelta = 0.0\n\treturn delta\n",["return change over time for the requested metric ."]]
["def maybeDeferred(f, *args, **kw):\n\ttry:\n\t\tresult = f(*args, **kw)\n\texcept:\n\t\treturn fail(failure.Failure(captureVars=Deferred.debug))\n\tif isinstance(result, Deferred):\n\t\treturn result\n\telif isinstance(result, failure.Failure):\n\t\treturn fail(result)\n\telse:\n\t\treturn succeed(result)\n",["invoke a function that may or may not return a l{deferred} ."]]
["def run_proxy(port=8080, start_ioloop=True):\n\tapp = tornado.web.Application([('.*', ProxyHandler)])\n\tapp.listen(port, address='127.0.0.1')\n\tioloop = tornado.ioloop.IOLoop.instance()\n\tif start_ioloop:\n\t\tioloop.start()\n",["run proxy on the specified port ."]]
["def test_rgb_to_hsl_part_8():\n\tassert (rgb_to_hsl(153, 153, 102) == (60, 20, 50))\n\tassert (rgb_to_hsl(204, 204, 51) == (60, 60, 50))\n\tassert (rgb_to_hsl(255, 255, 0) == (60, 100, 50))\n",["test rgb to hsl color function ."]]
["def mean(x, axis=None, keepdims=False):\n\taxis = _normalize_axis(axis, ndim(x))\n\tif (x.dtype.base_dtype == tf.bool):\n\t\tx = tf.cast(x, floatx())\n\treturn tf.reduce_mean(x, reduction_indices=axis, keep_dims=keepdims)\n",["mean of a tensor ."]]
["def execute_get_output(*command):\n\tdevnull = open(os.devnull, 'w')\n\tcommand = map(str, command)\n\tproc = subprocess.Popen(command, close_fds=True, stdout=subprocess.PIPE, stderr=devnull)\n\tdevnull.close()\n\treturn proc.stdout.read().strip()\n",["execute and return stdout ."]]
["def setup_args(config_files=[]):\n\tglobal args\n\targlist = sys.argv[1:]\n\tfor config_file in filter(os.path.isfile, config_files):\n\t\targlist.insert(0, ('@' + config_file))\n\targs = parser.parse_args(arglist)\n\tif args.stream:\n\t\targs.stream = [stream.lower() for stream in args.stream]\n",["adds additional args to allow the vm uuid to be set ."]]
["def getopenfilename(parent=None, caption='', basedir='', filters='', selectedfilter='', options=None):\n\treturn _qfiledialog_wrapper('getOpenFileName', parent=parent, caption=caption, basedir=basedir, filters=filters, selectedfilter=selectedfilter, options=options)\n",["wrapper around qtgui ."]]
["def preprocess_image(image, height, width, is_training=False, bbox=None, fast_mode=True):\n\tif is_training:\n\t\treturn preprocess_for_train(image, height, width, bbox, fast_mode)\n\telse:\n\t\treturn preprocess_for_eval(image, height, width)\n",["preprocesses the given image ."]]
["def _to_epoch_time(date):\n\tif hasattr(date, 'timestamp'):\n\t\treturn int(date.timestamp())\n\telse:\n\t\tepoch = datetime.fromtimestamp(0)\n\t\tdelta = (date - epoch)\n\t\treturn int(delta.total_seconds())\n",["convert a datetime object to an integer number of seconds since the unix epoch ."]]
["def libvlc_audio_get_track(p_mi):\n\tf = (_Cfunctions.get('libvlc_audio_get_track', None) or _Cfunction('libvlc_audio_get_track', ((1,),), None, ctypes.c_int, MediaPlayer))\n\treturn f(p_mi)\n",["get current audio track ."]]
["@retry_on_failure\ndef test_inet_pton():\n\tif (not is_cli):\n\t\treturn\n\tsocket.inet_pton(socket.AF_INET, '127.0.0.1')\n\tAssertError(socket.error, socket.inet_pton, socket.AF_INET, 'garbage\tdkfjdkfjdkfj')\n",["tests socket ."]]
["def tempfilepager(text, cmd):\n\timport tempfile\n\tfilename = tempfile.mktemp()\n\tfile = open(filename, 'w')\n\tfile.write(_encode(text))\n\tfile.close()\n\ttry:\n\t\tos.system((((cmd + '\t\"') + filename) + '\"'))\n\tfinally:\n\t\tos.unlink(filename)\n",["page through text by invoking a program on a temporary file ."]]
["def modify_acl_group(id, **data):\n\tgroup = models.AclGroup.smart_get(id)\n\tgroup.check_for_acl_violation_acl_group()\n\tgroup.update_object(data)\n\tgroup.add_current_user_if_empty()\n",["modify acl group ."]]
["def memoize(fun):\n\t@functools.wraps(fun)\n\tdef wrapper(*args, **kwargs):\n\t\tkey = (args, frozenset(sorted(kwargs.items())))\n\t\ttry:\n\t\t\treturn cache[key]\n\t\texcept KeyError:\n\t\t\tret = cache[key] = fun(*args, **kwargs)\n\t\t\treturn ret\n\tdef cache_clear():\n\t\t'Clear\tcache.'\n\t\tcache.clear()\n\tcache = {}\n\twrapper.cache_clear = cache_clear\n\treturn wrapper\n",["wrap a function so that results for any argument tuple are stored in cache ."]]
["def info(request, message, extra_tags='', fail_silently=False):\n\tadd_message(request, constants.INFO, message, extra_tags=extra_tags, fail_silently=fail_silently)\n",["return information about a group cli example: ."]]
["def get_version(package):\n\tinit_py = open(os.path.join(package, '__init__.py')).read()\n\treturn re.search('__version__\t=\t[\\'\"]([^\\'\"]+)[\\'\"]', init_py).group(1)\n",["returns an integer representing the major ."]]
["def login_protected_view(request):\n\tt = Template('This\tis\ta\tlogin\tprotected\ttest.\tUsername\tis\t{{\tuser.username\t}}.', name='Login\tTemplate')\n\tc = Context({'user': request.user})\n\treturn HttpResponse(t.render(c))\n",["a simple view that is login protected ."]]
["def reap_threads(func):\n\tif (not thread):\n\t\treturn func\n\t@functools.wraps(func)\n\tdef decorator(*args):\n\t\tkey = threading_setup()\n\t\ttry:\n\t\t\treturn func(*args)\n\t\tfinally:\n\t\t\tthreading_cleanup(*key)\n\treturn decorator\n",["use this function when threads are being used ."]]
["def python_2_unicode_compatible(klass):\n\tif (not six.PY3):\n\t\tklass.__unicode__ = klass.__str__\n\t\tklass.__str__ = (lambda self: self.__unicode__().encode(u'utf-8'))\n\treturn klass\n",["a decorator that defines __unicode__ and __str__ methods under python 2 ."]]
["def modules(modulePath):\n\ttry:\n\t\tmod = sys.modules[modulePath]\n\t\tif (mod is None):\n\t\t\traise KeyError()\n\texcept KeyError:\n\t\t__import__(modulePath)\n\t\tmod = sys.modules[modulePath]\n\treturn mod\n",["load a module and retrieve a reference to that module ."]]
["def _has_required_boto():\n\tif (not HAS_BOTO):\n\t\treturn False\n\telif (LooseVersion(boto.__version__) < LooseVersion(required_boto_version)):\n\t\treturn False\n\telse:\n\t\treturn True\n",["returns true\/false boolean depending on if boto is installed and correct version ."]]
["def to_ulps(x):\n\tn = struct.unpack('<q', struct.pack('<d', x))[0]\n\tif (n < 0):\n\t\tn = (~ (n + (2 ** 63)))\n\treturn n\n",["convert a non-nan float x to an integer ."]]
["def _count_righthand_zero_bits(number, bits):\n\tif (number == 0):\n\t\treturn bits\n\tfor i in range(bits):\n\t\tif ((number >> i) & 1):\n\t\t\treturn i\n\treturn bits\n",["count the number of zero bits on the right hand side ."]]
["def getOverhangSpan(elementNode):\n\treturn getCascadeFloatWithoutSelf((2.0 * getLayerHeight(elementNode)), elementNode, 'overhangSpan')\n",["get the overhang span ."]]
["def setlocale(category, locale=None):\n\tif (locale and (not isinstance(locale, _builtin_str))):\n\t\tlocale = normalize(_build_localename(locale))\n\treturn _setlocale(category, locale)\n",["set the locale for the given category ."]]
["def test_rus_sample_wt_fit():\n\trus = RandomUnderSampler(random_state=RND_SEED)\n\tassert_raises(RuntimeError, rus.sample, X, Y)\n",["test either if an error is raised when sample is called before fitting ."]]
["def writeOutput(fileName=''):\n\tfileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName)\n\tif (fileName != ''):\n\t\tskeinforge_craft.writeChainTextWithNounMessage(fileName, 'unpause')\n",["jitter a gcode linear move file ."]]
["def stop(name):\n\tcmd = '\/etc\/rc.d\/{0}\t-f\tstop'.format(name)\n\treturn (not __salt__['cmd.retcode'](cmd))\n",["stop the scheduler ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only runs on freebsd systems ."]]
["def _ip_bridge_cmd(action, params, device):\n\tcmd = ['ip', 'addr', action]\n\tcmd.extend(params)\n\tcmd.extend(['dev', device])\n\treturn cmd\n",["build commands to add\/del ips to bridges\/devices ."]]
["def instance_floating_address_get_all(context, instance_uuid):\n\treturn IMPL.instance_floating_address_get_all(context, instance_uuid)\n",["get all floating ip addresses of an instance ."]]
["def getManipulatedPaths(close, elementNode, loop, prefix, sideLength):\n\tif (len(loop) < 1):\n\t\treturn [[]]\n\tderivation = BottomDerivation(elementNode, prefix)\n\ttargetMatrix = matrix.getBranchMatrixSetElementNode(elementNode)\n\ttransformedLoop = matrix.getTransformedVector3s(matrix.getIdentityTetragrid(targetMatrix.tetragrid), loop)\n\tlift = ((derivation.altitude + derivation.getAdditionalPathLift()) - euclidean.getBottomByPath(transformedLoop))\n\tfor point in loop:\n\t\tpoint.z += lift\n\treturn [loop]\n",["get equated paths ."]]
["def update_last_login(sender, user, **kwargs):\n\tuser.last_login = timezone.now()\n\tuser.save()\n",["a signal receiver which updates the last_login date for the user logging in ."]]
["def _copy_array_if_base_present(a):\n\tif (a.base is not None):\n\t\treturn a.copy()\n\treturn a\n",["copies the array if its base points to a parent array ."]]
["def DEFINE_bool(name, default, help):\n\tCONFIG.AddOption(type_info.Bool(name=name, default=default, description=help))\n",["a helper for defining boolean options ."]]
["@removals.remove(message='Use\tkeystoneclient.session.request\tinstead.', version='1.7.0', removal_version='2.0.0')\ndef request(*args, **kwargs):\n\treturn client_session.request(*args, **kwargs)\n",["constructs and sends a :class:request <request> ."]]
["@command(name='hash', usage='compute\thashes')\ndef print_hash(args):\n\timport lixian_hash\n\timport lixian_cli_parser\n\tlixian_hash.main(lixian_cli_parser.expand_command_line(args))\n",["lx hash --sha1 file ."]]
["def _read_int32(f):\n\treturn np.int32(struct.unpack('>i', f.read(4))[0])\n",["read a signed 32-bit integer ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load if win32 libraries are installed ."]]
["def render_comment_form(parser, token):\n\treturn RenderCommentFormNode.handle_token(parser, token)\n",["render the comment form through the comments\/form ."]]
["def strip_entities(value):\n\treturn re.sub('&(?:\\\\w+|#\\\\d);', '', value)\n",["returns the given html with all entities stripped ."]]
["def isXQuartz():\n\tassert (_tk_type is not None)\n\treturn (_tk_type == 'xquartz')\n",["returns true if idle is using an os x x11 tk ."]]
["def snapshot_destroy(context, snapshot_id):\n\treturn IMPL.snapshot_destroy(context, snapshot_id)\n",["destroy the snapshot or raise if it does not exist ."]]
["def provide_worker_fake_entries(group):\n\treturn _WORKER_FAKE_ENTRIES.get(group, [])\n",["give a set of fake entries for known groups ."]]
["def get_disk_size(path, format=None):\n\tsize = images.qemu_img_info(path, format).virtual_size\n\treturn int(size)\n",["get the size of a disk image ."]]
["def processElementNode(elementNode):\n\tpath.convertElementNode(elementNode, getGeometryOutput(None, elementNode))\n",["process the xml element ."]]
["def writeOutput(fileName=''):\n\tfileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName)\n\tif (fileName != ''):\n\t\tskeinforge_craft.writeChainTextWithNounMessage(fileName, 'unpause')\n",["craft a gcode file ."]]
["def _isDefaultHandler():\n\treturn (signal.getsignal(signal.SIGCHLD) == signal.SIG_DFL)\n",["determine whether the i{sigchld} handler is the default or not ."]]
["def new(rsa_key):\n\treturn PKCS115_SigScheme(rsa_key)\n",["return a fresh instance of the hash object ."]]
["def maximum(image, selem, out=None, mask=None, shift_x=False, shift_y=False):\n\treturn _apply_scalar_per_pixel(generic_cy._maximum, image, selem, out=out, mask=mask, shift_x=shift_x, shift_y=shift_y)\n",["element-wise maximum of two tensors ."]]
["def _test():\n\timport thread\n\timport time\n\tdef f(x):\n\t\treturn (x * x)\n\tdef work(seconds):\n\t\tprint ('[%d]\tStart\tto\twork\tfor\t%fs...' % (thread.get_ident(), seconds))\n\t\ttime.sleep(seconds)\n\t\tprint ('[%d]\tWork\tdone\t(%fs).' % (thread.get_ident(), seconds))\n\t\treturn ('%d\tslept\t%fs' % (thread.get_ident(), seconds))\n\tpool = Pool(9)\n\tresult = pool.apply_async(f, (10,))\n\tprint result.get(timeout=1)\n\tprint pool.map(f, range(10))\n\tit = pool.imap(f, range(10))\n\tprint it.next()\n\tprint it.next()\n\tprint it.next(timeout=1)\n\tresult = pool.apply_async(time.sleep, (3,))\n\ttry:\n\t\tprint result.get(timeout=1)\n\texcept TimeoutError:\n\t\tprint 'Good.\tGot\texpected\ttimeout\texception.'\n\telse:\n\t\tassert False, 'Expected\texception\t!'\n\tprint result.get()\n\tdef cb(s):\n\t\tprint ('Result\tready:\t%s' % s)\n\tfor res in pool.imap(work, xrange(10, 3, (-1)), chunksize=4):\n\t\tprint 'Item:', res\n\tfor res in pool.imap_unordered(work, xrange(10, 3, (-1))):\n\t\tprint 'Item:', res\n\tresult = pool.map_async(work, xrange(10), callback=cb)\n\ttry:\n\t\tprint result.get(timeout=1)\n\texcept TimeoutError:\n\t\tprint 'Good.\tGot\texpected\ttimeout\texception.'\n\telse:\n\t\tassert False, 'Expected\texception\t!'\n\tprint result.get()\n\tresult = pool.imap_async(work, xrange(3, 10), callback=cb)\n\ttry:\n\t\tprint result.get(timeout=1)\n\texcept TimeoutError:\n\t\tprint 'Good.\tGot\texpected\ttimeout\texception.'\n\telse:\n\t\tassert False, 'Expected\texception\t!'\n\tfor i in result.get():\n\t\tprint 'Item:', i\n\tprint '###\tLoop\tagain:'\n\tfor i in result.get():\n\t\tprint 'Item2:', i\n\tresult = pool.imap_unordered_async(work, xrange(10, 3, (-1)), callback=cb)\n\ttry:\n\t\tprint result.get(timeout=1)\n\texcept TimeoutError:\n\t\tprint 'Good.\tGot\texpected\ttimeout\texception.'\n\telse:\n\t\tassert False, 'Expected\texception\t!'\n\tfor i in result.get():\n\t\tprint 'Item1:', i\n\tfor i in result.get():\n\t\tprint 'Item2:', i\n\tr = result.get()\n\tfor i in r:\n\t\tprint 'Item3:', i\n\tfor i in r:\n\t\tprint 'Item4:', i\n\tfor i in r:\n\t\tprint 'Item5:', i\n\tresult = pool.imap_unordered_async(work, xrange(2, (-10), (-1)), callback=cb)\n\ttime.sleep(3)\n\ttry:\n\t\tfor i in result.get():\n\t\t\tprint 'Got\titem:', i\n\texcept IOError:\n\t\tprint 'Good.\tGot\texpected\texception:'\n\t\ttraceback.print_exc()\n\tresult = pool.imap_async(work, xrange(2, (-10), (-1)), callback=cb)\n\ttime.sleep(3)\n\ttry:\n\t\tfor i in result.get():\n\t\t\tprint 'Got\titem:', i\n\texcept IOError:\n\t\tprint 'Good.\tGot\texpected\texception:'\n\t\ttraceback.print_exc()\n\tpool.terminate()\n\tprint 'End\tof\ttests'\n",["a function that exists for test purposes ."]]
["def answer():\n\toutput = s3_rest_controller()\n\treturn output\n",["restful crud controller ."]]
["def init(mpstate):\n\treturn SerialModule(mpstate)\n",["initialise module ."]]
["def test_system_numerics_biginteger():\n\tprint 'TODO'\n",["url this should be tested minimally here ."]]
["def _test():\n\timport thread\n\timport time\n\tdef f(x):\n\t\treturn (x * x)\n\tdef work(seconds):\n\t\tprint ('[%d]\tStart\tto\twork\tfor\t%fs...' % (thread.get_ident(), seconds))\n\t\ttime.sleep(seconds)\n\t\tprint ('[%d]\tWork\tdone\t(%fs).' % (thread.get_ident(), seconds))\n\t\treturn ('%d\tslept\t%fs' % (thread.get_ident(), seconds))\n\tpool = Pool(9)\n\tresult = pool.apply_async(f, (10,))\n\tprint result.get(timeout=1)\n\tprint pool.map(f, range(10))\n\tit = pool.imap(f, range(10))\n\tprint it.next()\n\tprint it.next()\n\tprint it.next(timeout=1)\n\tresult = pool.apply_async(time.sleep, (3,))\n\ttry:\n\t\tprint result.get(timeout=1)\n\texcept TimeoutError:\n\t\tprint 'Good.\tGot\texpected\ttimeout\texception.'\n\telse:\n\t\tassert False, 'Expected\texception\t!'\n\tprint result.get()\n\tdef cb(s):\n\t\tprint ('Result\tready:\t%s' % s)\n\tfor res in pool.imap(work, xrange(10, 3, (-1)), chunksize=4):\n\t\tprint 'Item:', res\n\tfor res in pool.imap_unordered(work, xrange(10, 3, (-1))):\n\t\tprint 'Item:', res\n\tresult = pool.map_async(work, xrange(10), callback=cb)\n\ttry:\n\t\tprint result.get(timeout=1)\n\texcept TimeoutError:\n\t\tprint 'Good.\tGot\texpected\ttimeout\texception.'\n\telse:\n\t\tassert False, 'Expected\texception\t!'\n\tprint result.get()\n\tresult = pool.imap_async(work, xrange(3, 10), callback=cb)\n\ttry:\n\t\tprint result.get(timeout=1)\n\texcept TimeoutError:\n\t\tprint 'Good.\tGot\texpected\ttimeout\texception.'\n\telse:\n\t\tassert False, 'Expected\texception\t!'\n\tfor i in result.get():\n\t\tprint 'Item:', i\n\tprint '###\tLoop\tagain:'\n\tfor i in result.get():\n\t\tprint 'Item2:', i\n\tresult = pool.imap_unordered_async(work, xrange(10, 3, (-1)), callback=cb)\n\ttry:\n\t\tprint result.get(timeout=1)\n\texcept TimeoutError:\n\t\tprint 'Good.\tGot\texpected\ttimeout\texception.'\n\telse:\n\t\tassert False, 'Expected\texception\t!'\n\tfor i in result.get():\n\t\tprint 'Item1:', i\n\tfor i in result.get():\n\t\tprint 'Item2:', i\n\tr = result.get()\n\tfor i in r:\n\t\tprint 'Item3:', i\n\tfor i in r:\n\t\tprint 'Item4:', i\n\tfor i in r:\n\t\tprint 'Item5:', i\n\tresult = pool.imap_unordered_async(work, xrange(2, (-10), (-1)), callback=cb)\n\ttime.sleep(3)\n\ttry:\n\t\tfor i in result.get():\n\t\t\tprint 'Got\titem:', i\n\texcept IOError:\n\t\tprint 'Good.\tGot\texpected\texception:'\n\t\ttraceback.print_exc()\n\tresult = pool.imap_async(work, xrange(2, (-10), (-1)), callback=cb)\n\ttime.sleep(3)\n\ttry:\n\t\tfor i in result.get():\n\t\t\tprint 'Got\titem:', i\n\texcept IOError:\n\t\tprint 'Good.\tGot\texpected\texception:'\n\t\ttraceback.print_exc()\n\tpool.terminate()\n\tprint 'End\tof\ttests'\n",["run the bio ."]]
["def _get_application_default_credential_from_file(filename):\n\tfrom oauth2client import service_account\n\twith open(filename) as file_obj:\n\t\tclient_credentials = json.load(file_obj)\n\tcredentials_type = client_credentials.get('type')\n\tif (credentials_type == AUTHORIZED_USER):\n\t\trequired_fields = set(['client_id', 'client_secret', 'refresh_token'])\n\telif (credentials_type == SERVICE_ACCOUNT):\n\t\trequired_fields = set(['client_id', 'client_email', 'private_key_id', 'private_key'])\n\telse:\n\t\traise ApplicationDefaultCredentialsError(((((\"'type'\tfield\tshould\tbe\tdefined\t(and\thave\tone\tof\tthe\t'\" + AUTHORIZED_USER) + \"'\tor\t'\") + SERVICE_ACCOUNT) + \"'\tvalues)\"))\n\tmissing_fields = required_fields.difference(client_credentials.keys())\n\tif missing_fields:\n\t\t_raise_exception_for_missing_fields(missing_fields)\n\tif (client_credentials['type'] == AUTHORIZED_USER):\n\t\treturn GoogleCredentials(access_token=None, client_id=client_credentials['client_id'], client_secret=client_credentials['client_secret'], refresh_token=client_credentials['refresh_token'], token_expiry=None, token_uri=GOOGLE_TOKEN_URI, user_agent='Python\tclient\tlibrary')\n\telse:\n\t\treturn service_account._ServiceAccountCredentials(service_account_id=client_credentials['client_id'], service_account_email=client_credentials['client_email'], private_key_id=client_credentials['private_key_id'], private_key_pkcs8_text=client_credentials['private_key'], scopes=[])\n",["build the application default credentials from file ."]]
["def getGeometryOutput(elementNode):\n\tderivation = HeightmapDerivation(elementNode)\n\theightGrid = derivation.heightGrid\n\tif (derivation.fileName != ''):\n\t\theightGrid = getHeightGrid(archive.getAbsoluteFolderPath(elementNode.getOwnerDocument().fileName, derivation.fileName))\n\treturn getGeometryOutputByHeightGrid(derivation, elementNode, heightGrid)\n",["get vector3 vertexes from attribute dictionary ."]]
["def whitespace_before_comment(logical_line, tokens):\n\tprev_end = (0, 0)\n\tfor (token_type, text, start, end, line) in tokens:\n\t\tif (token_type == tokenize.COMMENT):\n\t\t\tinline_comment = line[:start[1]].strip()\n\t\t\tif inline_comment:\n\t\t\t\tif ((prev_end[0] == start[0]) and (start[1] < (prev_end[1] + 2))):\n\t\t\t\t\t(yield (prev_end, 'E261\tat\tleast\ttwo\tspaces\tbefore\tinline\tcomment'))\n\t\t\t(symbol, sp, comment) = text.partition('\t')\n\t\t\tbad_prefix = (symbol not in ('#', '#:'))\n\t\t\tif inline_comment:\n\t\t\t\tif (bad_prefix or comment[:1].isspace()):\n\t\t\t\t\t(yield (start, \"E262\tinline\tcomment\tshould\tstart\twith\t'#\t'\"))\n\t\t\telif bad_prefix:\n\t\t\t\tif (text.rstrip('#') and ((start[0] > 1) or (symbol[1] != '!'))):\n\t\t\t\t\t(yield (start, \"E265\tblock\tcomment\tshould\tstart\twith\t'#\t'\"))\n\t\telif (token_type != tokenize.NL):\n\t\t\tprev_end = end\n",["separate inline comments by at least two spaces ."]]
["def _goBooleanProxy(expression):\n\tinitTechnique(kb.technique)\n\tif conf.dnsName:\n\t\tquery = agent.prefixQuery(kb.injection.data[kb.technique].vector)\n\t\tquery = agent.suffixQuery(query)\n\t\tpayload = agent.payload(newValue=query)\n\t\toutput = _goDns(payload, expression)\n\t\tif (output is not None):\n\t\t\treturn output\n\tvector = kb.injection.data[kb.technique].vector\n\tvector = vector.replace('[INFERENCE]', expression)\n\tquery = agent.prefixQuery(vector)\n\tquery = agent.suffixQuery(query)\n\tpayload = agent.payload(newValue=query)\n\ttimeBasedCompare = (kb.technique in (PAYLOAD.TECHNIQUE.TIME, PAYLOAD.TECHNIQUE.STACKED))\n\toutput = hashDBRetrieve(expression, checkConf=True)\n\tif (output is None):\n\t\toutput = Request.queryPage(payload, timeBasedCompare=timeBasedCompare, raise404=False)\n\t\tif (output is not None):\n\t\t\thashDBWrite(expression, output)\n\treturn output\n",["retrieve the output of a boolean based sql query ."]]
["def ssl_options_to_context(ssl_options):\n\tif isinstance(ssl_options, dict):\n\t\tassert all(((k in _SSL_CONTEXT_KEYWORDS) for k in ssl_options)), ssl_options\n\tif ((not hasattr(ssl, 'SSLContext')) or isinstance(ssl_options, ssl.SSLContext)):\n\t\treturn ssl_options\n\tcontext = ssl.SSLContext(ssl_options.get('ssl_version', ssl.PROTOCOL_SSLv23))\n\tif ('certfile' in ssl_options):\n\t\tcontext.load_cert_chain(ssl_options['certfile'], ssl_options.get('keyfile', None))\n\tif ('cert_reqs' in ssl_options):\n\t\tcontext.verify_mode = ssl_options['cert_reqs']\n\tif ('ca_certs' in ssl_options):\n\t\tcontext.load_verify_locations(ssl_options['ca_certs'])\n\tif ('ciphers' in ssl_options):\n\t\tcontext.set_ciphers(ssl_options['ciphers'])\n\tif hasattr(ssl, 'OP_NO_COMPRESSION'):\n\t\tcontext.options |= ssl.OP_NO_COMPRESSION\n\treturn context\n",["try to convert an ssl_options dictionary to an ~ssl ."]]
["def notify_usage_exists(context, volume_ref, current_period=False):\n\t(begin, end) = utils.last_completed_audit_period()\n\tif current_period:\n\t\taudit_start = end\n\t\taudit_end = timeutils.utcnow()\n\telse:\n\t\taudit_start = begin\n\t\taudit_end = end\n\textra_usage_info = dict(audit_period_beginning=str(audit_start), audit_period_ending=str(audit_end))\n\tnotify_about_volume_usage(context, volume_ref, 'exists', extra_usage_info=extra_usage_info)\n",["generates exists notification for an instance for usage auditing purposes ."]]
["@instrumented_task(name='sentry.tasks.check_auth', queue='auth')\ndef check_auth(**kwargs):\n\tnow = timezone.now()\n\tcutoff = (now - timedelta(seconds=AUTH_CHECK_INTERVAL))\n\tidentity_list = list(AuthIdentity.objects.filter(last_synced__lte=cutoff))\n\tAuthIdentity.objects.filter(id__in=[i.id for i in identity_list]).update(last_synced=now)\n\tfor identity in identity_list:\n\t\tcheck_auth_identity.apply_async(kwargs={'auth_identity_id': identity.id}, expires=AUTH_CHECK_INTERVAL)\n",["if an authorization header contains credentials ."]]
["def getDescendingAreaLoops(allPoints, corners, importRadius):\n\tcenters = intercircle.getCentersFromPoints(allPoints, importRadius)\n\tdescendingAreaLoops = []\n\tloops = getLoopsInOrderOfArea(compareAreaDescending, centers)\n\tpointDictionary = {}\n\tfor loop in loops:\n\t\tif ((len(loop) > 2) and (getOverlapRatio(loop, pointDictionary) < 0.1)):\n\t\t\tintercircle.directLoop((not euclidean.getIsInFilledRegion(descendingAreaLoops, loop[0])), loop)\n\t\t\tdescendingAreaLoops.append(loop)\n\t\t\taddLoopToPointTable(loop, pointDictionary)\n\tdescendingAreaLoops = euclidean.getSimplifiedLoops(descendingAreaLoops, importRadius)\n\treturn getLoopsWithCorners(corners, importRadius, descendingAreaLoops, pointDictionary)\n",["get descending area loops which include most of the points ."]]
["def date(*args, **kwargs):\n\td = None\n\tf = None\n\tif ((len(args) == 0) and (kwargs.get('year') is not None) and kwargs.get('month') and kwargs.get('day')):\n\t\td = Date(**kwargs)\n\telif kwargs.get('week'):\n\t\tf = kwargs.pop('format', None)\n\t\td = Date(*_yyyywwd2yyyymmdd(kwargs.pop('year', ((args and args[0]) or Date.now().year)), kwargs.pop('week'), kwargs.pop('weekday', kwargs.pop('day', 1))), **kwargs)\n\telif ((len(args) == 0) or (args[0] == NOW)):\n\t\td = Date.now()\n\telif ((len(args) == 1) and isinstance(args[0], (Date, datetime))):\n\t\td = Date.fromtimestamp(int(mktime(args[0].timetuple())))\n\t\td += time(microseconds=args[0].microsecond)\n\telif ((len(args) == 1) and (isinstance(args[0], int) or (isinstance(args[0], basestring) and args[0].isdigit()))):\n\t\td = Date.fromtimestamp(int(args[0]))\n\telif ((len(args) == 1) and isinstance(args[0], basestring)):\n\t\ttry:\n\t\t\td = Date.fromtimestamp(mktime_tz(parsedate_tz(args[0])))\n\t\texcept:\n\t\t\tfor format in (((('format' in kwargs) and [kwargs['format']]) or []) + date_formats):\n\t\t\t\ttry:\n\t\t\t\t\td = Date.strptime(args[0], format)\n\t\t\t\t\tbreak\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\tif (d is None):\n\t\t\traise DateError(('unknown\tdate\tformat\tfor\t%s' % repr(args[0])))\n\telif ((len(args) == 2) and isinstance(args[0], basestring)):\n\t\td = Date.strptime(args[0], args[1])\n\telif (len(args) >= 3):\n\t\tf = kwargs.pop('format', None)\n\t\td = Date(*args[:7], **kwargs)\n\telse:\n\t\traise DateError('unknown\tdate\tformat')\n\td.format = (kwargs.get('format') or ((len(args) > 7) and args[7]) or f or Date.format)\n\treturn d\n",["builder for rebulk object ."]]
["def geo_apps(namespace=True, runtests=False):\n\tfrom django.db import connection\n\tfrom django.contrib.gis.geos import GEOS_PREPARE\n\tfrom django.contrib.gis.gdal import HAS_GDAL\n\tapps = ['geoapp', 'relatedapp']\n\tif (not connection.ops.mysql):\n\t\tapps.append('distapp')\n\tif (connection.ops.postgis and connection.ops.geography):\n\t\tapps.append('geogapp')\n\tif HAS_GDAL:\n\t\tif (connection.ops.postgis and GEOS_PREPARE):\n\t\t\tapps.append('geo3d')\n\t\tapps.append('layermap')\n\tif runtests:\n\t\treturn [('django.contrib.gis.tests', app) for app in apps]\n\telif namespace:\n\t\treturn [('django.contrib.gis.tests.%s' % app) for app in apps]\n\telse:\n\t\treturn apps\n",["returns a list of geodjango test applications that reside in django ."]]
["def get_netrc_auth(url):\n\ttry:\n\t\tlocations = (os.path.expanduser('~\/{0}'.format(f)) for f in NETRC_FILES)\n\t\tnetrc_path = None\n\t\tfor loc in locations:\n\t\t\tif (os.path.exists(loc) and (not netrc_path)):\n\t\t\t\tnetrc_path = loc\n\t\tif (netrc_path is None):\n\t\t\treturn netrc_path\n\t\tri = urlparse(url)\n\t\thost = ri.netloc.split(':')[0]\n\t\ttry:\n\t\t\t_netrc = netrc(netrc_path).authenticators(host)\n\t\t\tif _netrc:\n\t\t\t\tlogin_i = (0 if _netrc[0] else 1)\n\t\t\t\treturn (_netrc[login_i], _netrc[2])\n\t\texcept (NetrcParseError, IOError):\n\t\t\tpass\n\texcept (ImportError, AttributeError):\n\t\tpass\n",["returns the requests tuple auth for a given url from netrc ."]]
["def countedArray(expr, intExpr=None):\n\tarrayExpr = Forward()\n\tdef countFieldParseAction(s, l, t):\n\t\tn = t[0]\n\t\t(arrayExpr << ((n and Group(And(([expr] * n)))) or Group(empty)))\n\t\treturn []\n\tif (intExpr is None):\n\t\tintExpr = Word(nums).setParseAction((lambda t: int(t[0])))\n\telse:\n\t\tintExpr = intExpr.copy()\n\tintExpr.setName('arrayLen')\n\tintExpr.addParseAction(countFieldParseAction, callDuringTry=True)\n\treturn (intExpr + arrayExpr)\n",["helper to define a counted list of expressions ."]]
["def declarative_base(bind=None, metadata=None, mapper=None, cls=object, name='Base', constructor=_declarative_constructor, class_registry=None, metaclass=DeclarativeMeta):\n\tlcl_metadata = (metadata or MetaData())\n\tif bind:\n\t\tlcl_metadata.bind = bind\n\tif (class_registry is None):\n\t\tclass_registry = weakref.WeakValueDictionary()\n\tbases = (((not isinstance(cls, tuple)) and (cls,)) or cls)\n\tclass_dict = dict(_decl_class_registry=class_registry, metadata=lcl_metadata)\n\tif isinstance(cls, type):\n\t\tclass_dict['__doc__'] = cls.__doc__\n\tif constructor:\n\t\tclass_dict['__init__'] = constructor\n\tif mapper:\n\t\tclass_dict['__mapper_cls__'] = mapper\n\treturn metaclass(name, bases, class_dict)\n",["construct a base class for declarative class definitions ."]]
["def generate_jwt():\n\tnow = int(time.time())\n\theader_json = json.dumps({'typ': 'JWT', 'alg': 'RS256'})\n\tpayload_json = json.dumps({'iat': now, 'exp': (now + 3600), 'iss': DEFAUTL_SERVICE_ACCOUNT, 'scope': TARGET_AUD, 'aud': 'https:\/\/www.googleapis.com\/oauth2\/v4\/token'})\n\theaderAndPayload = '{}.{}'.format(base64.urlsafe_b64encode(header_json), base64.urlsafe_b64encode(payload_json))\n\t(key_name, signature) = app_identity.sign_blob(headerAndPayload)\n\tsigned_jwt = '{}.{}'.format(headerAndPayload, base64.urlsafe_b64encode(signature))\n\treturn signed_jwt\n",["generates a signed json web token using the google app engine default service account ."]]
["def get_test_hooks(test_files, cfg, cov=None):\n\tresults = []\n\tdirs = set(map(os.path.dirname, test_files))\n\tfor dir in list(dirs):\n\t\tif (os.path.basename(dir) == 'ftests'):\n\t\t\tdirs.add(os.path.join(os.path.dirname(dir), 'tests'))\n\tdirs = list(dirs)\n\tdirs.sort()\n\tfor dir in dirs:\n\t\tfilename = os.path.join(dir, 'checks.py')\n\t\tif os.path.exists(filename):\n\t\t\tmodule = import_module(filename, cfg, tracer=tracer)\n\t\t\tif (cov is not None):\n\t\t\t\tcov.start()\n\t\t\thooks = module.test_hooks()\n\t\t\tif (cov is not None):\n\t\t\t\tcov.stop()\n\t\t\tresults.extend(hooks)\n\treturn results\n",["returns a list of test hooks from a given list of test modules ."]]
["def parse_body_arguments(content_type, body, arguments, files, headers=None):\n\tif (headers and ('Content-Encoding' in headers)):\n\t\tgen_log.warning('Unsupported\tContent-Encoding:\t%s', headers['Content-Encoding'])\n\t\treturn\n\tif content_type.startswith('application\/x-www-form-urlencoded'):\n\t\ttry:\n\t\t\turi_arguments = parse_qs_bytes(native_str(body), keep_blank_values=True)\n\t\texcept Exception as e:\n\t\t\tgen_log.warning('Invalid\tx-www-form-urlencoded\tbody:\t%s', e)\n\t\t\turi_arguments = {}\n\t\tfor (name, values) in uri_arguments.items():\n\t\t\tif values:\n\t\t\t\targuments.setdefault(name, []).extend(values)\n\telif content_type.startswith('multipart\/form-data'):\n\t\ttry:\n\t\t\tfields = content_type.split(';')\n\t\t\tfor field in fields:\n\t\t\t\t(k, sep, v) = field.strip().partition('=')\n\t\t\t\tif ((k == 'boundary') and v):\n\t\t\t\t\tparse_multipart_form_data(utf8(v), body, arguments, files)\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\traise ValueError('multipart\tboundary\tnot\tfound')\n\t\texcept Exception as e:\n\t\t\tgen_log.warning('Invalid\tmultipart\/form-data:\t%s', e)\n",["parses a form request body ."]]
["def _translate_volume_summary_view(context, vol, image_id=None):\n\td = {}\n\td['id'] = vol['id']\n\td['status'] = vol['status']\n\td['size'] = vol['size']\n\td['availability_zone'] = vol['availability_zone']\n\td['created_at'] = vol['created_at']\n\td['attachments'] = []\n\tif (vol['attach_status'] == 'attached'):\n\t\tattachment = _translate_attachment_detail_view(context, vol)\n\t\td['attachments'].append(attachment)\n\td['display_name'] = vol['display_name']\n\td['display_description'] = vol['display_description']\n\tif (vol['volume_type_id'] and vol.get('volume_type')):\n\t\td['volume_type'] = vol['volume_type']['name']\n\telse:\n\t\td['volume_type'] = str(vol['volume_type_id'])\n\td['snapshot_id'] = vol['snapshot_id']\n\td['source_volid'] = vol['source_volid']\n\tif image_id:\n\t\td['image_id'] = image_id\n\tLOG.audit(_('vol=%s'), vol, context=context)\n\tif vol.get('volume_metadata'):\n\t\tmetadata = vol.get('volume_metadata')\n\t\td['metadata'] = dict(((item['key'], item['value']) for item in metadata))\n\telif (vol.get('metadata') and isinstance(vol.get('metadata'), dict)):\n\t\td['metadata'] = vol['metadata']\n\telse:\n\t\td['metadata'] = {}\n\tif vol.get('volume_glance_metadata'):\n\t\td['bootable'] = 'true'\n\telse:\n\t\td['bootable'] = 'false'\n\treturn d\n",["maps keys for volumes summary view ."]]
["def PBKDF1(password, salt, dkLen, count=1000, hashAlgo=None):\n\tif (not hashAlgo):\n\t\thashAlgo = SHA1\n\tpassword = tobytes(password)\n\tpHash = hashAlgo.new((password + salt))\n\tdigest = pHash.digest_size\n\tif (dkLen > digest):\n\t\traise TypeError(('Selected\thash\talgorithm\thas\ta\ttoo\tshort\tdigest\t(%d\tbytes).' % digest))\n\tif (len(salt) != 8):\n\t\traise ValueError(('Salt\tis\tnot\t8\tbytes\tlong\t(%d\tbytes\tinstead).' % len(salt)))\n\tfor i in xrange((count - 1)):\n\t\tpHash = pHash.new(pHash.digest())\n\treturn pHash.digest()[:dkLen]\n",["derive one key from a password ."]]
["def makeUnicode(text):\n\tif isinstance(text, str):\n\t\ttext = unicode(text, 'ISO-8859-1')\n\telif (not isinstance(text, unicode)):\n\t\ttry:\n\t\t\ttext = unicode(text)\n\t\texcept UnicodeError:\n\t\t\ttry:\n\t\t\t\ttext = str(text)\n\t\t\texcept Exception:\n\t\t\t\ttext = repr(text)\n\t\t\treturn makeUnicode(text)\n\ttext = regex_control_code.sub((lambda regs: controlchars[ord(regs.group(1))]), text)\n\ttext = re.sub('\\\\\\\\x0([0-7])(?=[^0-7]|$)', '\\\\\\\\\\\\1', text)\n\treturn text\n",["convert text to printable unicode string ."]]
["def dnn_gradinput3d(kerns, topgrad, img_shp, border_mode='valid', subsample=(1, 1, 1), conv_mode='conv'):\n\tctx_name = infer_context_name(kerns, topgrad)\n\tkerns = as_gpuarray_variable(kerns, ctx_name)\n\ttopgrad = as_gpuarray_variable(topgrad, ctx_name)\n\tkerns = gpu_contiguous(kerns)\n\ttopgrad = gpu_contiguous(topgrad)\n\timg_shp = as_tensor_variable(img_shp)\n\tdesc = gpu_dnn_conv_desc(border_mode=border_mode, subsample=subsample, conv_mode=conv_mode)(kerns.shape)\n\tout = gpu_alloc_empty(ctx_name, kerns.dtype)(*img_shp)\n\treturn gpu_dnn_conv_gradI()(kerns, topgrad, out, desc)\n",["gpu convolution gradient with respect to input using cudnn from nvidia ."]]
["def parse_args(arguments, apply_config=False):\n\tparser = create_parser()\n\targs = parser.parse_args(arguments)\n\tif ((not args.files) and (not args.list_fixes)):\n\t\tparser.error(u'incorrect\tnumber\tof\targuments')\n\targs.files = [decode_filename(name) for name in args.files]\n\tif apply_config:\n\t\tparser = read_config(args, parser)\n\t\targs = parser.parse_args(arguments)\n\t\targs.files = [decode_filename(name) for name in args.files]\n\tif (u'-' in args.files):\n\t\tif (len(args.files) > 1):\n\t\t\tparser.error(u'cannot\tmix\tstdin\tand\tregular\tfiles')\n\t\tif args.diff:\n\t\t\tparser.error(u'--diff\tcannot\tbe\tused\twith\tstandard\tinput')\n\t\tif args.in_place:\n\t\t\tparser.error(u'--in-place\tcannot\tbe\tused\twith\tstandard\tinput')\n\t\tif args.recursive:\n\t\t\tparser.error(u'--recursive\tcannot\tbe\tused\twith\tstandard\tinput')\n\tif ((len(args.files) > 1) and (not (args.in_place or args.diff))):\n\t\tparser.error(u'autopep8\tonly\ttakes\tone\tfilename\tas\targument\tunless\tthe\t\"--in-place\"\tor\t\"--diff\"\targs\tare\tused')\n\tif (args.recursive and (not (args.in_place or args.diff))):\n\t\tparser.error(u'--recursive\tmust\tbe\tused\twith\t--in-place\tor\t--diff')\n\tif (args.in_place and args.diff):\n\t\tparser.error(u'--in-place\tand\t--diff\tare\tmutually\texclusive')\n\tif (args.max_line_length <= 0):\n\t\tparser.error(u'--max-line-length\tmust\tbe\tgreater\tthan\t0')\n\tif args.select:\n\t\targs.select = _split_comma_separated(args.select)\n\tif args.ignore:\n\t\targs.ignore = _split_comma_separated(args.ignore)\n\telif (not args.select):\n\t\tif args.aggressive:\n\t\t\targs.select = [u'E', u'W']\n\t\telse:\n\t\t\targs.ignore = _split_comma_separated(DEFAULT_IGNORE)\n\tif args.exclude:\n\t\targs.exclude = _split_comma_separated(args.exclude)\n\telse:\n\t\targs.exclude = []\n\tif (args.jobs < 1):\n\t\timport multiprocessing\n\t\targs.jobs = multiprocessing.cpu_count()\n\tif ((args.jobs > 1) and (not args.in_place)):\n\t\tparser.error(u'parallel\tjobs\trequires\t--in-place')\n\tif args.line_range:\n\t\tif (args.line_range[0] <= 0):\n\t\t\tparser.error(u'--range\tmust\tbe\tpositive\tnumbers')\n\t\tif (args.line_range[0] > args.line_range[1]):\n\t\t\tparser.error(u'First\tvalue\tof\t--range\tshould\tbe\tless\tthan\tor\tequal\tto\tthe\tsecond')\n\treturn args\n",["parse input arguments ."]]
["def get_msvcr():\n\tmsc_pos = sys.version.find('MSC\tv.')\n\tif (msc_pos != (-1)):\n\t\tmsc_ver = sys.version[(msc_pos + 6):(msc_pos + 10)]\n\t\tif (msc_ver == '1300'):\n\t\t\treturn ['msvcr70']\n\t\telif (msc_ver == '1310'):\n\t\t\treturn ['msvcr71']\n\t\telif (msc_ver == '1400'):\n\t\t\treturn ['msvcr80']\n\t\telif (msc_ver == '1500'):\n\t\t\treturn ['msvcr90']\n\t\telif (msc_ver == '1600'):\n\t\t\treturn ['msvcr100']\n\t\telse:\n\t\t\traise ValueError(('Unknown\tMS\tCompiler\tversion\t%s\t' % msc_ver))\n",["include the appropriate msvc runtime library if python was built with msvc 7 ."]]
["def script(vm_):\n\treturn salt.utils.cloud.os_script(config.get_cloud_config_value('script', vm_, __opts__), vm_, __opts__, salt.utils.cloud.salt_config_to_yaml(salt.utils.cloud.minion_config(__opts__, vm_)))\n",["wrapper for :py:func:cmdmod ."]]
["def GetArgs():\n\tparser = argparse.ArgumentParser(description='Process\targs\tfor\tretrieving\tall\tthe\tVirtual\tMachines')\n\tparser.add_argument('-s', '--host', required=True, action='store', help='Remote\thost\tto\tconnect\tto')\n\tparser.add_argument('-o', '--port', type=int, default=443, action='store', help='Port\tto\tconnect\ton')\n\tparser.add_argument('-u', '--user', required=True, action='store', help='User\tname\tto\tuse\twhen\tconnecting\tto\thost')\n\tparser.add_argument('-p', '--password', required=False, action='store', help='Password\tto\tuse\twhen\tconnecting\tto\thost')\n\targs = parser.parse_args()\n\treturn args\n",["supports the command-line arguments listed below ."]]
["def extract_text_in(html, starttag):\n\ttry:\n\t\t(_, html) = html.split(starttag, 1)\n\texcept ValueError:\n\t\treturn\n\tlevel = 0\n\tparts = []\n\tpos = 0\n\tfor match in DIV_RE.finditer(html):\n\t\tif match.group(1):\n\t\t\tlevel -= 1\n\t\t\tif (level == 0):\n\t\t\t\tpos = match.end()\n\t\telse:\n\t\t\tif (level == 0):\n\t\t\t\tparts.append(html[pos:match.start()])\n\t\t\tlevel += 1\n\t\tif (level == (-1)):\n\t\t\tparts.append(html[pos:match.start()])\n\t\t\tbreak\n\telse:\n\t\tprint(u'no\tclosing\ttag\tfound!')\n\t\treturn\n\treturn u''.join(parts)\n",["extract the text from a <div> tag in the html starting with starttag ."]]
["def arithmeticEval(s):\n\tnode = ast.parse(s, mode=u'eval')\n\tdef _eval(node):\n\t\tif isinstance(node, ast.Expression):\n\t\t\treturn _eval(node.body)\n\t\telif isinstance(node, ast.Str):\n\t\t\treturn node.s\n\t\telif isinstance(node, ast.Num):\n\t\t\treturn node.n\n\t\telif isinstance(node, ast.BinOp):\n\t\t\treturn _binOps[type(node.op)](_eval(node.left), _eval(node.right))\n\t\telse:\n\t\t\traise Exception(u'Unsupported\ttype\t{}'.format(node))\n\treturn _eval(node.body)\n",["a safe eval supporting basic arithmetic operations ."]]
["def set_response_cookie(path=None, path_header=None, name='session_id', timeout=60, domain=None, secure=False, httponly=False):\n\tcookie = cherrypy.serving.response.cookie\n\tcookie[name] = cherrypy.serving.session.id\n\tcookie[name]['path'] = (path or cherrypy.serving.request.headers.get(path_header) or '\/')\n\tif (False and timeout):\n\t\te = (time.time() + (timeout * 60))\n\t\tcookie[name]['expires'] = httputil.HTTPDate(e)\n\tif (domain is not None):\n\t\tcookie[name]['domain'] = domain\n\tif secure:\n\t\tcookie[name]['secure'] = 1\n\tif httponly:\n\t\tif (not cookie[name].isReservedKey('httponly')):\n\t\t\traise ValueError('The\thttponly\tcookie\ttoken\tis\tnot\tsupported.')\n\t\tcookie[name]['httponly'] = 1\n",["set a response cookie for the client ."]]
["def get_size():\n\tplatf = platform.system()\n\tdimension = None\n\tif (platf == 'Windows'):\n\t\tdimension = _get_size_windows()\n\t\tif (dimension is None):\n\t\t\tdimension = _get_size_tput()\n\telif ((platf == 'Linux') or (platf == 'Darwin') or platf.startswith('CYGWIN')):\n\t\tdimension = _get_size_linux()\n\tif (dimension is None):\n\t\tdimension = (80, 25)\n\treturn dimension\n",["return the vms size object ."]]
["@sensitive_post_parameters()\n@csrf_protect\n@never_cache\ndef login(request, template_name='registration\/login.html', redirect_field_name=REDIRECT_FIELD_NAME, authentication_form=AuthenticationForm, current_app=None, extra_context=None):\n\tredirect_to = request.REQUEST.get(redirect_field_name, '')\n\tif (request.method == 'POST'):\n\t\tform = authentication_form(data=request.POST)\n\t\tif form.is_valid():\n\t\t\tif (not is_safe_url(url=redirect_to, host=request.get_host())):\n\t\t\t\tredirect_to = resolve_url(settings.LOGIN_REDIRECT_URL)\n\t\t\tauth_login(request, form.get_user())\n\t\t\tif request.session.test_cookie_worked():\n\t\t\t\trequest.session.delete_test_cookie()\n\t\t\treturn HttpResponseRedirect(redirect_to)\n\telse:\n\t\tform = authentication_form(request)\n\trequest.session.set_test_cookie()\n\tcurrent_site = get_current_site(request)\n\tcontext = {'form': form, redirect_field_name: redirect_to, 'site': current_site, 'site_name': current_site.name}\n\tif (extra_context is not None):\n\t\tcontext.update(extra_context)\n\treturn TemplateResponse(request, template_name, context, current_app=current_app)\n",["persist a user id and a backend in the request ."]]
["def _NewDocumentFromPb(doc_pb):\n\tlang = None\n\tif doc_pb.has_language():\n\t\tlang = _DecodeUTF8(doc_pb.language())\n\treturn Document(doc_id=_DecodeUTF8(doc_pb.id()), fields=_NewFieldsFromPb(doc_pb.field_list()), language=lang, rank=doc_pb.order_id())\n",["constructs a document from a document_pb ."]]
["@contextlib.contextmanager\ndef temporary_chown(path, owner_uid=None):\n\tif (owner_uid is None):\n\t\towner_uid = os.getuid()\n\torig_uid = os.stat(path).st_uid\n\tif (orig_uid != owner_uid):\n\t\texecute('chown', owner_uid, path, run_as_root=True)\n\ttry:\n\t\t(yield)\n\tfinally:\n\t\tif (orig_uid != owner_uid):\n\t\t\texecute('chown', orig_uid, path, run_as_root=True)\n",["temporarily chown a path ."]]
["def main():\n\terrors = 0\n\tfits_files = handle_options(sys.argv[1:])\n\tsetup_logging()\n\tfor filename in fits_files:\n\t\terrors += process_file(filename)\n\tif errors:\n\t\tlog.warning('{}\terrors'.format(errors))\n\treturn int(bool(errors))\n",["perform a single benchmark run ."]]
["def debugerror():\n\tif _hasTemplating:\n\t\tout = str(djangoerror())\n\telse:\n\t\tout = ('<p>You\\'ve\tset\tweb.py\tto\tuse\tthe\tfancier\tdebugerror\terror\t\\nmessages,\tbut\tthese\tmessages\trequire\tyou\tinstall\tthe\tCheetah\ttemplate\t\\nsystem.\tFor\tmore\tinformation,\tsee\t\\n<a\thref=\"http:\/\/webpy.org\/\">the\tweb.py\twebsite<\/a>.<\/p>\\n\\n<p>In\tthe\tmeantime,\there\\'s\ta\tplain\told\terror\tmessage:<\/p>\\n\\n<pre>%s<\/pre>\\n\\n<p>(If\tit\tsays\tsomething\tabout\t\\'Compiler\\',\tthen\tit\\'s\tprobably\\nbecause\tyou\\'re\ttrying\tto\tuse\ttemplates\tand\tyou\thaven\\'t\\ninstalled\tCheetah.\tSee\tabove.)<\/p>\\n' % htmlquote(traceback.format_exc()))\n\tctx.status = '500\tInternal\tServer\tError'\n\tctx.headers = [('Content-Type', 'text\/html')]\n\tctx.output = out\n",["a replacement for internalerror that presents a nice page with lots of debug information for the programmer ."]]
["def core_freq_config_set(kodi_setting, all_settings):\n\ttry:\n\t\tversion = PiVersion()\n\texcept IOError:\n\t\tversion = 'PiB'\n\tif (version == 'PiB'):\n\t\tif (int(kodi_setting) == 250):\n\t\t\treturn 'remove_this_line'\n\telif (version == 'Pi2'):\n\t\tif (int(kodi_setting) == 450):\n\t\t\treturn 'remove_this_line'\n\treturn kodi_setting\n",["checks if the frequency setting is the same as the default pi setting ."]]
["def get_dasharray(obj):\n\tif (obj.__dict__.get('_dashSeq') is not None):\n\t\treturn ','.join(map(str, obj._dashSeq))\n\telse:\n\t\tls = obj.get_linestyle()\n\t\tdasharray = LINESTYLES.get(ls, 'not\tfound')\n\t\tif (dasharray == 'not\tfound'):\n\t\t\twarnings.warn(\"line\tstyle\t'{0}'\tnot\tunderstood:\tdefaulting\tto\tsolid\tline.\".format(ls))\n\t\t\tdasharray = LINESTYLES['solid']\n\t\treturn dasharray\n",["get an svg dash array for the given matplotlib linestyle parameters obj : matplotlib object the matplotlib line or path object ."]]
["def test_call_accepts_func_kw_params_kw_types():\n\t@accepts(kw_1=int, kw_2=int, kw_3=int)\n\tdef foo(kw_1=5, kw_2=6, kw_3=7):\n\t\tpass\n\tt = time.time()\n\tfor i in range(0, 10000):\n\t\tfoo(5, 6, 7)\n\treturn (time.time() - t)\n",["calling an accepts-checked function: kw params ."]]
["def MGF1(mgfSeed, maskLen, hash):\n\tT = b('')\n\tfor counter in xrange(ceil_div(maskLen, hash.digest_size)):\n\t\tc = long_to_bytes(counter, 4)\n\t\ttry:\n\t\t\tT = (T + hash.new((mgfSeed + c)).digest())\n\t\texcept AttributeError:\n\t\t\tT = (T + Hash_new(hash, (mgfSeed + c)).digest())\n\tassert (len(T) >= maskLen)\n\treturn T[:maskLen]\n",["mask generation function ."]]
["@py.test.mark.parametrize('item_name', [item.name for item in six._urllib_request_moved_attributes])\ndef test_move_items_urllib_request(item_name):\n\tif (sys.version_info[:2] >= (2, 6)):\n\t\tassert (item_name in dir(six.moves.urllib.request))\n\tgetattr(six.moves.urllib.request, item_name)\n",["ensure that everything loads correctly ."]]
["def get_ha1_file_htdigest(filename):\n\tdef get_ha1(realm, username):\n\t\tresult = None\n\t\tf = open(filename, 'r')\n\t\tfor line in f:\n\t\t\t(u, r, ha1) = line.rstrip().split(':')\n\t\t\tif ((u == username) and (r == realm)):\n\t\t\t\tresult = ha1\n\t\t\t\tbreak\n\t\tf.close()\n\t\treturn result\n\treturn get_ha1\n",["returns a get_ha1 function which obtains a ha1 password hash from a flat file with lines of the same format as that produced by the apache htdigest utility"]]
["@pytest.mark.network\ndef test_download_if_requested(script):\n\tresult = script.pip('install', 'INITools==0.1', '-d', '.', expect_error=True)\n\tassert ((Path('scratch') \/ 'INITools-0.1.tar.gz') in result.files_created)\n\tassert ((script.site_packages \/ 'initools') not in result.files_created)\n",["it should download and not install if requested ."]]
["def skip(reason):\n\treturn skipif(True, reason=reason)\n",["unconditionally skip a test ."]]
["def chshell(name, shell):\n\tpre_info = info(name)\n\tif (not pre_info):\n\t\traise CommandExecutionError(\"User\t'{0}'\tdoes\tnot\texist\".format(name))\n\tif (shell == pre_info['shell']):\n\t\treturn True\n\tcmd = ['pw', 'usermod', '-s', shell, '-n', name]\n\t__salt__['cmd.run'](cmd, python_shell=False)\n\treturn (info(name).get('shell') == shell)\n",["change the default shell of the user cli example: ."]]
["def chop_traceback(tb, exclude_prefix=_UNITTEST_RE, exclude_suffix=_SQLA_RE):\n\tstart = 0\n\tend = (len(tb) - 1)\n\twhile ((start <= end) and exclude_prefix.search(tb[start])):\n\t\tstart += 1\n\twhile ((start <= end) and exclude_suffix.search(tb[end])):\n\t\tend -= 1\n\treturn tb[start:(end + 1)]\n",["chop extraneous lines off beginning and end of a traceback ."]]
["def dottedQuadToNum(ip):\n\timport socket, struct\n\ttry:\n\t\treturn struct.unpack('!L', socket.inet_aton(ip.strip()))[0]\n\texcept socket.error:\n\t\tif (ip.strip() == '255.255.255.255'):\n\t\t\treturn 4294967295L\n\t\telse:\n\t\t\traise ValueError(('Not\ta\tgood\tdotted-quad\tIP:\t%s' % ip))\n\treturn\n",["convert decimal dotted quad string to long integer ."]]
["def sort(filename, key, outputFile, fields=None, watermark=((1024 * 1024) * 100)):\n\tif (fields is not None):\n\t\tassert set(key).issubset(set([f[0] for f in fields]))\n\twith FileRecordStream(filename) as f:\n\t\tif fields:\n\t\t\tfieldNames = [ff[0] for ff in fields]\n\t\t\tindices = [f.getFieldNames().index(name) for name in fieldNames]\n\t\t\tassert (len(indices) == len(fields))\n\t\telse:\n\t\t\tfileds = f.getFields()\n\t\t\tfieldNames = f.getFieldNames()\n\t\t\tindices = None\n\t\tkey = [fieldNames.index(name) for name in key]\n\t\tchunk = 0\n\t\trecords = []\n\t\tfor (i, r) in enumerate(f):\n\t\t\tif indices:\n\t\t\t\ttemp = []\n\t\t\t\tfor i in indices:\n\t\t\t\t\ttemp.append(r[i])\n\t\t\t\tr = temp\n\t\t\trecords.append(r)\n\t\t\tavailable_memory = psutil.avail_phymem()\n\t\t\tif (available_memory < watermark):\n\t\t\t\t_sortChunk(records, key, chunk, fields)\n\t\t\t\trecords = []\n\t\t\t\tchunk += 1\n\t\tif (len(records) > 0):\n\t\t\t_sortChunk(records, key, chunk, fields)\n\t\t\tchunk += 1\n\t\t_mergeFiles(key, chunk, outputFile, fields)\n",["takes a list of integers and sorts them in ascending order ."]]
["def topic_list(request, slug, template_name='groups\/topics\/topic_list.html'):\n\tgroup = get_object_or_404(Group, slug=slug, is_active=True)\n\ttopic_list = GroupTopic.objects.filter(group=group, is_active=True)\n\treturn render(request, template_name, {'group': group, 'topic_list': topic_list})\n",["returns a group topic list page ."]]
["def is_unavailable_exception(e):\n\ttry:\n\t\tif ((e.errcode == (-1)) or (e.headers is None)):\n\t\t\treturn True\n\t\texc_mess = e.headers.get('X-exception')\n\texcept AttributeError:\n\t\texc_mess = str(e)\n\tif (exc_mess and ('temporarily\tunavailable' in exc_mess.lower())):\n\t\treturn True\n",["returns true if the given protocolerror is the product of a server-side exception caused by the temporarily unavailable response sometimes given by operations on non-blocking sockets ."]]
["def export_set(dataset):\n\treturn yaml.safe_dump(dataset._package(ordered=False))\n",["returns xls representation of dataset ."]]
["def list_nodes_min(call=None, **kwargs):\n\tif (call == 'action'):\n\t\traise SaltCloudSystemExit('The\tlist_nodes_min\tfunction\tmust\tbe\tcalled\twith\t-f\tor\t--function.')\n\tconn = get_conn()\n\tserver_list = conn.server_list_min()\n\tif (not server_list):\n\t\treturn {}\n\treturn server_list\n",["return a list of the vms that are on the provider ."]]
["def LoadYamlConfig(config_file_name):\n\t(loaders, exporters) = bulkloader_config.load_config(config_file_name, increment_id=IncrementId)\n\tfor cls in loaders:\n\t\tLoader.RegisterLoader(cls())\n\tfor cls in exporters:\n\t\tExporter.RegisterExporter(cls())\n",["loads a config file and registers any loader classes present ."]]
["def clean_url(url):\n\turl = url.encode('utf8')\n\turl = ''.join([(urllib.parse.quote(c) if (ord(c) >= 127) else c) for c in url.decode('utf-8')])\n\treturn url\n",["url quotes unicode data out of urls ."]]
["def compare_package(version1, version2):\n\tdef normalize(v):\n\t\treturn [int(x) for x in re.sub('(\\\\.0+)*$', '', v).split('.')]\n\treturn cmp(normalize(version1), normalize(version2))\n",["compare version packages ."]]
["def short_cycle_task(*args, **kwargs):\n\tdef decorator(f):\n\t\tf._short_cycle_task = True\n\t\tf._ticks_between_runs = kwargs.pop('ticks_between_runs', 0)\n\t\treturn f\n\tif kwargs:\n\t\treturn decorator\n\telse:\n\t\treturn decorator(args[0])\n",["decorator to indicate that a method is a periodic task ."]]
["def manhattan(rating1, rating2):\n\tdistance = 0\n\ttotal = 0\n\tfor key in rating1:\n\t\tif (key in rating2):\n\t\t\tdistance += abs((rating1[key] - rating2[key]))\n\t\t\ttotal += 1\n\tif (total > 0):\n\t\treturn (distance \/ total)\n\telse:\n\t\treturn (-1)\n",["computes the manhattan distance ."]]
["def overload_attribute(typ, attr):\n\tfrom .typing.templates import make_overload_attribute_template\n\tdef decorate(overload_func):\n\t\ttemplate = make_overload_attribute_template(typ, attr, overload_func)\n\t\tinfer_getattr(template)\n\t\treturn overload_func\n\treturn decorate\n",["a decorator marking the decorated function as typing and implementing attribute *attr* for the given numba type in nopython mode ."]]
["def escape(text):\n\ttext = text.replace('\\\\', '\\\\\\\\')\n\ttext = text.replace('\"\"\"', '\"\"\\\\\"')\n\ttext = text.replace('\t\\n', '\t\\\\n\\\\\\n')\n\treturn text\n",["escape & ."]]
["def prepare_lookup_value(key, value):\n\tif key.endswith(u'__in'):\n\t\tvalue = value.split(u',')\n\tif key.endswith(u'__isnull'):\n\t\tif (value.lower() in (u'', u'false', u'0')):\n\t\t\tvalue = False\n\t\telse:\n\t\t\tvalue = True\n\treturn value\n",["returns a lookup value prepared to be used in queryset filtering ."]]
["def onLoginAppReady():\n\tINFO_MSG(('onLoginAppReady:\tbootstrapGroupIndex=%s,\tbootstrapGlobalIndex=%s' % (os.getenv('KBE_BOOTIDX_GROUP'), os.getenv('KBE_BOOTIDX_GLOBAL'))))\n",["kbengine method ."]]
["def vary_on_cookie(func):\n\tdef inner_func(*args, **kwargs):\n\t\tresponse = func(*args, **kwargs)\n\t\tpatch_vary_headers(response, ('Cookie',))\n\t\treturn response\n\treturn inner_func\n",["a view decorator that adds \"cookie\" to the vary header of a response ."]]
["def _module_to_dict(module, omittable=(lambda k: k.startswith('_'))):\n\treturn dict([(k, repr(v)) for (k, v) in module.__dict__.items() if (not omittable(k))])\n",["converts a module namespace to a python dictionary ."]]
["def truncate_letters(s, num):\n\ts = force_unicode(s)\n\tlength = int(num)\n\tif (len(s) > length):\n\t\ts = s[:length]\n\t\tif (not s.endswith('...')):\n\t\t\ts += '...'\n\treturn s\n",["truncates a string to a number of letters ."]]
["def _get_zone(gcdns, zone_name, zone_id):\n\tif (zone_id is not None):\n\t\ttry:\n\t\t\treturn gcdns.get_zone(zone_id)\n\t\texcept ZoneDoesNotExistError:\n\t\t\treturn None\n\tavailable_zones = gcdns.iterate_zones()\n\tfound_zone = None\n\tfor zone in available_zones:\n\t\tif (zone.domain == zone_name):\n\t\t\tfound_zone = zone\n\t\t\tbreak\n\treturn found_zone\n",["gets the zone object for a given domain name ."]]
["def sql_indexes(app, style, connection):\n\toutput = []\n\tfor model in models.get_models(app, include_auto_created=True):\n\t\toutput.extend(connection.creation.sql_indexes_for_model(model, style))\n\treturn output\n",["returns a list of the create index sql statements for all models in the given app ."]]
["def rfc822_escape(header):\n\tlines = header.split('\\n')\n\tsep = ('\\n' + (8 * '\t'))\n\treturn sep.join(lines)\n",["return a version of the string escaped for inclusion in an rfc-822 header ."]]
["def getBottomByPaths(paths):\n\tbottom = 9.876543219876543e+17\n\tfor path in paths:\n\t\tfor point in path:\n\t\t\tbottom = min(bottom, point.z)\n\treturn bottom\n",["get the bottom of the paths ."]]
["def _translate_attachment_summary_view(_context, vol):\n\td = {}\n\tconductor_id = vol['id']\n\td['id'] = conductor_id\n\td['conductor_id'] = conductor_id\n\treturn d\n",["maps keys for attachment summary view ."]]
["@require_context\ndef virtual_interface_get_by_uuid(context, vif_uuid):\n\tvif_ref = _virtual_interface_query(context).filter_by(uuid=vif_uuid).first()\n\treturn vif_ref\n",["gets a virtual interface from the table ."]]
["def encode_for_xml(ustr, encoding='ascii'):\n\tif isinstance(ustr, unicode):\n\t\tpass\n\telif isinstance(ustr, str):\n\t\tustr = ustr.decode(codepage, 'replace')\n\telse:\n\t\tustr = unicode(str(ustr))\n\treturn ustr.encode(encoding, 'xmlcharrefreplace')\n",["encode unicode_data for use as xml or html ."]]
["@environmentfilter\ndef do_groupby(environment, value, attribute):\n\texpr = make_attrgetter(environment, attribute)\n\treturn [_GroupTuple(key, list(values)) for (key, values) in groupby(sorted(value, key=expr), expr)]\n",["group a sequence of objects by a common attribute ."]]
["@profiler.trace\ndef server_console_output(request, instance_id, tail_length=None):\n\treturn novaclient(request).servers.get_console_output(instance_id, length=tail_length)\n",["gets console output of an instance ."]]
["@domain_constructor(loss_target=0)\ndef quadratic1():\n\treturn {'loss': ((hp.uniform('x', (-5), 5) - 3) ** 2), 'status': base.STATUS_OK}\n",["about the simplest problem you could ask for: optimize a one-variable quadratic function ."]]
["def network_get_associated_fixed_ips(context, network_id, host=None):\n\treturn IMPL.network_get_associated_fixed_ips(context, network_id, host)\n",["get all networks ips that have been associated ."]]
["def col(loc, strg):\n\treturn ((((loc < len(strg)) and (strg[loc] == '\\n')) and 1) or (loc - strg.rfind('\\n', 0, loc)))\n",["returns current column within a string ."]]
["def write_file(filename, contents):\n\tf = open(filename, 'w')\n\ttry:\n\t\tfor line in contents:\n\t\t\tf.write((line + '\\n'))\n\tfinally:\n\t\tf.close()\n",["create a file with the specified name and write contents to it ."]]
["def __hash_new(name, string=''):\n\ttry:\n\t\treturn _hashlib.new(name, string)\n\texcept ValueError:\n\t\treturn __get_builtin_constructor(name)(string)\n",["new - return a new hashing object using the named algorithm; optionally initialized with a string ."]]
["def enable(name, start=False, **kwargs):\n\tif (not available(name)):\n\t\treturn False\n\talias = get_svc_alias()\n\tif (name in alias):\n\t\tlog.error('This\tservice\tis\taliased,\tenable\tits\talias\tinstead')\n\t\treturn False\n\tsvc_realpath = _get_svc_path(name)[0]\n\tdown_file = os.path.join(svc_realpath, 'down')\n\tif enabled(name):\n\t\tif os.path.exists(down_file):\n\t\t\ttry:\n\t\t\t\tos.unlink(down_file)\n\t\t\texcept OSError:\n\t\t\t\tlog.error('Unable\tto\tremove\tfile\t{0}'.format(down_file))\n\t\t\t\treturn False\n\t\treturn True\n\tif (not start):\n\t\tlog.trace('need\ta\ttemporary\tfile\t{0}'.format(down_file))\n\t\tif (not os.path.exists(down_file)):\n\t\t\ttry:\n\t\t\t\tsalt.utils.fopen(down_file, 'w').close()\n\t\t\texcept IOError:\n\t\t\t\tlog.error('Unable\tto\tcreate\tfile\t{0}'.format(down_file))\n\t\t\t\treturn False\n\ttry:\n\t\tos.symlink(svc_realpath, _service_path(name))\n\texcept IOError:\n\t\tlog.error('Unable\tto\tcreate\tsymlink\t{0}'.format(down_file))\n\t\tif (not start):\n\t\t\tos.unlink(down_file)\n\t\treturn False\n\tcmd = 'sv\tstatus\t{0}'.format(_service_path(name))\n\tretcode_sv = 1\n\tcount_sv = 0\n\twhile ((retcode_sv != 0) and (count_sv < 10)):\n\t\ttime.sleep(0.5)\n\t\tcount_sv += 1\n\t\tcall = __salt__['cmd.run_all'](cmd)\n\t\tretcode_sv = call['retcode']\n\tif ((not start) and os.path.exists(down_file)):\n\t\ttry:\n\t\t\tos.unlink(down_file)\n\t\texcept OSError:\n\t\t\tlog.error('Unable\tto\tremove\ttemp\tfile\t{0}'.format(down_file))\n\t\t\tretcode_sv = 1\n\tif (retcode_sv != 0):\n\t\tos.unlink(os.path.join([_service_path(name), name]))\n\t\treturn False\n\treturn True\n",["enable firewall profile ."]]
["def quota_create(context, project_id, resource, limit, user_id=None):\n\treturn IMPL.quota_create(context, project_id, resource, limit, user_id=user_id)\n",["create a quota for the given project and resource ."]]
["def smart_split(text):\n\ttext = force_unicode(text)\n\tfor bit in smart_split_re.finditer(text):\n\t\t(yield bit.group(0))\n",["generator that splits a string by spaces ."]]
["@removals.remove(message='Use\tkeystoneclient.session.request\tinstead.', version='1.7.0', removal_version='2.0.0')\ndef request(*args, **kwargs):\n\treturn client_session.request(*args, **kwargs)\n",["constructs and sends a :class:request <request> ."]]
["def apply_optimizer(optimizer, func, a, b):\n\treturn (optimizer(func, a, b, full_output=True)[1].function_calls,)\n",["return the number of function calls given an root-finding optimizer ."]]
["def get_level(request):\n\tstorage = getattr(request, '_messages', default_storage(request))\n\treturn storage.level\n",["returns the minimum level of messages to be recorded ."]]
["def set_vif_host_backend_ethernet_config(conf, tapname):\n\tconf.net_type = 'ethernet'\n\tconf.target_dev = tapname\n\tconf.script = ''\n",["populate a libvirtconfigguestinterface instance with host backend details for an externally configured host device ."]]
["def askcolor(color=None, **options):\n\tif color:\n\t\toptions = options.copy()\n\t\toptions['initialcolor'] = color\n\treturn Chooser(**options).show()\n",["ask for a color ."]]
["def do_forceescape(value):\n\tif hasattr(value, '__html__'):\n\t\tvalue = value.__html__()\n\treturn escape(text_type(value))\n",["enforce html escaping ."]]
["def processXMLElement(xmlElement):\n\tgroup.processShape(Difference, xmlElement)\n",["process the xml element ."]]
["def preferredencoding():\n\ttry:\n\t\tpref = locale.getpreferredencoding()\n\t\tu'TEST'.encode(pref)\n\texcept:\n\t\tpref = u'UTF-8'\n\treturn pref\n",["get preferred encoding ."]]
["def out_of_date(original, derived):\n\treturn ((not os.path.exists(derived)) or (os.path.exists(original) and (os.stat(derived).st_mtime < os.stat(original).st_mtime)))\n",["returns true if derivative is out-of-date wrt original ."]]
["def index_alt():\n\ts3_redirect_default(URL(f='person'))\n",["module homepage for non-admin users when no cms content found ."]]
["def window_hanning(x):\n\treturn (np.hanning(len(x)) * x)\n",["return x times the hanning window of len(x) ."]]
["def _get_hub():\n\tglobal _threadlocal\n\ttry:\n\t\treturn _threadlocal.hub\n\texcept AttributeError:\n\t\tpass\n",["return the hub for the current thread ."]]
["@register.filter(is_safe=False)\n@stringfilter\ndef upper(value):\n\treturn value.upper()\n",["converts a string into all uppercase ."]]
["def model_cr(method):\n\tmethod._api = 'model_cr'\n\treturn method\n",["decorate a record-style method where self is a recordset ."]]
["def normcase(s):\n\treturn s\n",["normalize case of pathname ."]]
["def output():\n\treturn s3_rest_controller()\n",["appends string_ to the response ."]]
["def build_inlinepatterns(md_instance, **kwargs):\n\tinlinePatterns = odict.OrderedDict()\n\tinlinePatterns[u'backtick'] = BacktickPattern(BACKTICK_RE)\n\tinlinePatterns[u'escape'] = EscapePattern(ESCAPE_RE, md_instance)\n\tinlinePatterns[u'reference'] = ReferencePattern(REFERENCE_RE, md_instance)\n\tinlinePatterns[u'link'] = LinkPattern(LINK_RE, md_instance)\n\tinlinePatterns[u'image_link'] = ImagePattern(IMAGE_LINK_RE, md_instance)\n\tinlinePatterns[u'image_reference'] = ImageReferencePattern(IMAGE_REFERENCE_RE, md_instance)\n\tinlinePatterns[u'short_reference'] = ReferencePattern(SHORT_REF_RE, md_instance)\n\tinlinePatterns[u'autolink'] = AutolinkPattern(AUTOLINK_RE, md_instance)\n\tinlinePatterns[u'automail'] = AutomailPattern(AUTOMAIL_RE, md_instance)\n\tinlinePatterns[u'linebreak'] = SubstituteTagPattern(LINE_BREAK_RE, u'br')\n\tif (md_instance.safeMode != u'escape'):\n\t\tinlinePatterns[u'html'] = HtmlPattern(HTML_RE, md_instance)\n\tinlinePatterns[u'entity'] = HtmlPattern(ENTITY_RE, md_instance)\n\tinlinePatterns[u'not_strong'] = SimpleTextPattern(NOT_STRONG_RE)\n\tinlinePatterns[u'em_strong'] = DoubleTagPattern(EM_STRONG_RE, u'strong,em')\n\tinlinePatterns[u'strong_em'] = DoubleTagPattern(STRONG_EM_RE, u'em,strong')\n\tinlinePatterns[u'strong'] = SimpleTagPattern(STRONG_RE, u'strong')\n\tinlinePatterns[u'emphasis'] = SimpleTagPattern(EMPHASIS_RE, u'em')\n\tif md_instance.smart_emphasis:\n\t\tinlinePatterns[u'emphasis2'] = SimpleTagPattern(SMART_EMPHASIS_RE, u'em')\n\telse:\n\t\tinlinePatterns[u'emphasis2'] = SimpleTagPattern(EMPHASIS_2_RE, u'em')\n\treturn inlinePatterns\n",["build the default set of inline patterns for markdown ."]]
["def _sample_rois(roidb, fg_rois_per_image, rois_per_image, num_classes):\n\tlabels = roidb['max_classes']\n\toverlaps = roidb['max_overlaps']\n\trois = roidb['boxes']\n\tfg_inds = np.where((overlaps >= cfg.TRAIN.FG_THRESH))[0]\n\tfg_rois_per_this_image = np.minimum(fg_rois_per_image, fg_inds.size)\n\tif (fg_inds.size > 0):\n\t\tfg_inds = npr.choice(fg_inds, size=fg_rois_per_this_image, replace=False)\n\tbg_inds = np.where(((overlaps < cfg.TRAIN.BG_THRESH_HI) & (overlaps >= cfg.TRAIN.BG_THRESH_LO)))[0]\n\tbg_rois_per_this_image = (rois_per_image - fg_rois_per_this_image)\n\tbg_rois_per_this_image = np.minimum(bg_rois_per_this_image, bg_inds.size)\n\tif (bg_inds.size > 0):\n\t\tbg_inds = npr.choice(bg_inds, size=bg_rois_per_this_image, replace=False)\n\tkeep_inds = np.append(fg_inds, bg_inds)\n\tlabels = labels[keep_inds]\n\tlabels[fg_rois_per_this_image:] = 0\n\toverlaps = overlaps[keep_inds]\n\trois = rois[keep_inds]\n\t(bbox_targets, bbox_inside_weights) = _get_bbox_regression_labels(roidb['bbox_targets'][keep_inds, :], num_classes)\n\treturn (labels, overlaps, rois, bbox_targets, bbox_inside_weights)\n",["generate a random sample of rois comprising foreground and background examples ."]]
["def get_port_from_device(port_id):\n\tLOG.debug(_('get_port_with_securitygroups()\tcalled:port_id=%s'), port_id)\n\tsession = db.get_session()\n\tsg_binding_port = sg_db.SecurityGroupPortBinding.port_id\n\tquery = session.query(models_v2.Port, sg_db.SecurityGroupPortBinding.security_group_id)\n\tquery = query.outerjoin(sg_db.SecurityGroupPortBinding, (models_v2.Port.id == sg_binding_port))\n\tquery = query.filter((models_v2.Port.id == port_id))\n\tport_and_sgs = query.all()\n\tif (not port_and_sgs):\n\t\treturn None\n\tport = port_and_sgs[0][0]\n\tplugin = manager.QuantumManager.get_plugin()\n\tport_dict = plugin._make_port_dict(port)\n\tport_dict[ext_sg.SECURITYGROUPS] = [sg_id for (port, sg_id) in port_and_sgs if sg_id]\n\tport_dict['security_group_rules'] = []\n\tport_dict['security_group_source_groups'] = []\n\tport_dict['fixed_ips'] = [ip['ip_address'] for ip in port['fixed_ips']]\n\treturn port_dict\n",["get port from database ."]]
["def GetFeedMapping(client, feed, placeholder_type):\n\tfeed_mapping_service = client.GetService('FeedMappingService', 'v201607')\n\tattribute_mappings = {}\n\tmore_pages = True\n\tselector = {'fields': ['FeedMappingId', 'AttributeFieldMappings'], 'predicates': [{'field': 'FeedId', 'operator': 'EQUALS', 'values': [feed['id']]}, {'field': 'PlaceholderType', 'operator': 'EQUALS', 'values': [placeholder_type]}], 'paging': {'startIndex': 0, 'numberResults': PAGE_SIZE}}\n\twhile more_pages:\n\t\tpage = feed_mapping_service.get(selector)\n\t\tif ('entries' in page):\n\t\t\tfor feed_mapping in page['entries']:\n\t\t\t\tfor attribute_mapping in feed_mapping['attributeFieldMappings']:\n\t\t\t\t\tif (attribute_mapping['feedAttributeId'] in attribute_mappings):\n\t\t\t\t\t\tattribute_mappings[attribute_mapping['feedAttributeId']].append(attribute_mapping['fieldId'])\n\t\t\t\t\telse:\n\t\t\t\t\t\tattribute_mappings[attribute_mapping['feedAttributeId']] = [attribute_mapping['fieldId']]\n\t\tselector['paging']['startIndex'] += PAGE_SIZE\n\t\tmore_pages = (selector['paging']['startIndex'] < int(page['totalNumEntries']))\n\treturn attribute_mappings\n",["gets the feed mapping for a given feed ."]]
["def build_ffi_for_binding(module_name, module_prefix, modules, libraries=[], extra_compile_args=[], extra_link_args=[]):\n\ttypes = []\n\tincludes = []\n\tfunctions = []\n\tmacros = []\n\tcustomizations = []\n\tfor name in modules:\n\t\t__import__((module_prefix + name))\n\t\tmodule = sys.modules[(module_prefix + name)]\n\t\ttypes.append(module.TYPES)\n\t\tmacros.append(module.MACROS)\n\t\tfunctions.append(module.FUNCTIONS)\n\t\tincludes.append(module.INCLUDES)\n\t\tcustomizations.append(module.CUSTOMIZATIONS)\n\tverify_source = '\\n'.join(((includes + functions) + customizations))\n\tffi = build_ffi(module_name, cdef_source='\\n'.join(((types + functions) + macros)), verify_source=verify_source, libraries=libraries, extra_compile_args=extra_compile_args, extra_link_args=extra_link_args)\n\treturn ffi\n",["modules listed in modules should have the following attributes: * includes: a string containing c includes ."]]
["def openWebPage(webPagePath):\n\tif (webPagePath.find('#') != (-1)):\n\t\tredirectionText = '<!DOCTYPE\tHTML\tPUBLIC\t\"-\/\/W3C\/\/DTD\tHTML\t4.0\tTransitional\/\/EN\">\\n<html>\\n<head>\\n'\n\t\tredirectionText += ('<meta\thttp-equiv=\"REFRESH\"\tcontent=\"0;url=%s\"><\/head>\\n<\/HTML>\\n' % webPagePath)\n\t\twebPagePath = archive.getDocumentationPath('redirect.html')\n\t\tarchive.writeFileText(webPagePath, redirectionText)\n\twebPagePath = ('\"%s\"' % webPagePath)\n\ttry:\n\t\tos.startfile(webPagePath)\n\t\treturn\n\texcept:\n\t\tpass\n\twebbrowserName = webbrowser.get().name\n\tif (webbrowserName == ''):\n\t\tprint 'Skeinforge\twas\tnot\table\tto\topen\tthe\tdocumentation\tfile\tin\ta\tweb\tbrowser.\t\tTo\tsee\tthe\tdocumentation,\topen\tthe\tfollowing\tfile\tin\ta\tweb\tbrowser:'\n\t\tprint webPagePath\n\t\treturn\n\tos.system(((webbrowserName + '\t') + webPagePath))\n",["open a web page in a browser ."]]
["def compile_rules(environment):\n\te = re.escape\n\trules = [(len(environment.comment_start_string), 'comment', e(environment.comment_start_string)), (len(environment.block_start_string), 'block', e(environment.block_start_string)), (len(environment.variable_start_string), 'variable', e(environment.variable_start_string))]\n\tif (environment.line_statement_prefix is not None):\n\t\trules.append((len(environment.line_statement_prefix), 'linestatement', ('^[\t\\\\t\\\\v]*' + e(environment.line_statement_prefix))))\n\tif (environment.line_comment_prefix is not None):\n\t\trules.append((len(environment.line_comment_prefix), 'linecomment', ('(?:^|(?<=\\\\S))[^\\\\S\\\\r\\\\n]*' + e(environment.line_comment_prefix))))\n\treturn [x[1:] for x in sorted(rules, reverse=True)]\n",["compiles all the rules from the environment into a list of rules ."]]
["def plot_img_and_hist(img, axes, bins=256):\n\t(ax_img, ax_hist) = axes\n\tax_cdf = ax_hist.twinx()\n\tax_img.imshow(img, cmap=plt.cm.gray)\n\tax_img.set_axis_off()\n\tax_hist.hist(img.ravel(), bins=bins)\n\tax_hist.ticklabel_format(axis='y', style='scientific', scilimits=(0, 0))\n\tax_hist.set_xlabel('Pixel\tintensity')\n\t(xmin, xmax) = dtype_range[img.dtype.type]\n\tax_hist.set_xlim(xmin, xmax)\n\t(img_cdf, bins) = exposure.cumulative_distribution(img, bins)\n\tax_cdf.plot(bins, img_cdf, 'r')\n\treturn (ax_img, ax_hist, ax_cdf)\n",["plot an image along with its histogram and cumulative histogram ."]]
["def traceParseAction(f):\n\tf = _trim_arity(f)\n\tdef z(*paArgs):\n\t\tthisFunc = f.func_name\n\t\t(s, l, t) = paArgs[(-3):]\n\t\tif (len(paArgs) > 3):\n\t\t\tthisFunc = ((paArgs[0].__class__.__name__ + '.') + thisFunc)\n\t\tsys.stderr.write((\">>entering\t%s(line:\t'%s',\t%d,\t%s)\\n\" % (thisFunc, line(l, s), l, t)))\n\t\ttry:\n\t\t\tret = f(*paArgs)\n\t\texcept Exception as exc:\n\t\t\tsys.stderr.write(('<<leaving\t%s\t(exception:\t%s)\\n' % (thisFunc, exc)))\n\t\t\traise\n\t\tsys.stderr.write(('<<leaving\t%s\t(ret:\t%s)\\n' % (thisFunc, ret)))\n\t\treturn ret\n\ttry:\n\t\tz.__name__ = f.__name__\n\texcept AttributeError:\n\t\tpass\n\treturn z\n",["decorator for debugging parse actions ."]]
["@docstring.dedent_interpd\ndef cohere(x, y, NFFT=256, Fs=2, detrend=detrend_none, window=window_hanning, noverlap=0, pad_to=None, sides=u'default', scale_by_freq=None):\n\tif (len(x) < (2 * NFFT)):\n\t\traise ValueError(_coh_error)\n\t(Pxx, f) = psd(x, NFFT, Fs, detrend, window, noverlap, pad_to, sides, scale_by_freq)\n\t(Pyy, f) = psd(y, NFFT, Fs, detrend, window, noverlap, pad_to, sides, scale_by_freq)\n\t(Pxy, f) = csd(x, y, NFFT, Fs, detrend, window, noverlap, pad_to, sides, scale_by_freq)\n\tCxy = ((np.abs(Pxy) ** 2) \/ (Pxx * Pxy))\n\treturn (Cxy, f)\n",["the coherence between *x* and *y* ."]]
["@treeio_login_required\n@handle_response_format\ndef location_add(request, response_format='html'):\n\tif request.POST:\n\t\tif ('cancel' not in request.POST):\n\t\t\tlocation = Location()\n\t\t\tform = LocationForm(request.user.profile, None, request.POST, instance=location)\n\t\t\tif form.is_valid():\n\t\t\t\tlocation = form.save()\n\t\t\t\tlocation.set_user_from_request(request)\n\t\t\t\treturn HttpResponseRedirect(reverse('identities_location_view', args=[location.id]))\n\t\telse:\n\t\t\treturn HttpResponseRedirect(reverse('identities_index'))\n\telse:\n\t\tform = LocationForm(request.user.profile, None)\n\tcontext = _get_default_context(request)\n\tcontext.update({'form': form})\n\treturn render_to_response('identities\/location_add', context, context_instance=RequestContext(request), response_format=response_format)\n",["new location form ."]]
["def test_rgb_to_hsl_part_17():\n\tassert (rgb_to_hsl(0, 0, 51) == (240, 100, 10))\n\tassert (rgb_to_hsl(0, 0, 102) == (240, 100, 20))\n\tassert (rgb_to_hsl(0, 0, 153) == (240, 100, 30))\n\tassert (rgb_to_hsl(0, 0, 204) == (240, 100, 40))\n\tassert (rgb_to_hsl(0, 0, 255) == (240, 100, 50))\n\tassert (rgb_to_hsl(51, 51, 255) == (240, 100, 60))\n\tassert (rgb_to_hsl(102, 102, 255) == (240, 100, 70))\n\tassert (rgb_to_hsl(153, 153, 255) == (240, 100, 80))\n\tassert (rgb_to_hsl(204, 204, 255) == (240, 100, 90))\n",["test rgb to hsl color function ."]]
["def _filter_non_json_lines(data):\n\twarnings = []\n\tlines = data.splitlines()\n\tfor (start, line) in enumerate(lines):\n\t\tline = line.strip()\n\t\tif line.startswith(u'{'):\n\t\t\tendchar = u'}'\n\t\t\tbreak\n\t\telif line.startswith(u'['):\n\t\t\tendchar = u']'\n\t\t\tbreak\n\telse:\n\t\traise ValueError('No\tstart\tof\tjson\tchar\tfound')\n\tlines = lines[start:]\n\tfor (reverse_end_offset, line) in enumerate(reversed(lines)):\n\t\tif line.strip().endswith(endchar):\n\t\t\tbreak\n\telse:\n\t\traise ValueError('No\tend\tof\tjson\tchar\tfound')\n\tif (reverse_end_offset > 0):\n\t\ttrailing_junk = lines[(len(lines) - reverse_end_offset):]\n\t\tfor line in trailing_junk:\n\t\t\tif line.strip():\n\t\t\t\twarnings.append(('Module\tinvocation\thad\tjunk\tafter\tthe\tJSON\tdata:\t%s' % '\\n'.join(trailing_junk)))\n\t\t\t\tbreak\n\tlines = lines[:(len(lines) - reverse_end_offset)]\n\treturn ('\\n'.join(lines), warnings)\n",["used to filter unrelated output around module json output ."]]
["def addAlreadyFilledArounds(alreadyFilledArounds, loop, radius):\n\tradius = abs(radius)\n\talreadyFilledLoop = []\n\tslightlyGreaterThanRadius = (1.01 * radius)\n\tmuchGreaterThanRadius = (2.5 * radius)\n\tcenters = intercircle.getCentersFromLoop(loop, slightlyGreaterThanRadius)\n\tfor center in centers:\n\t\talreadyFilledInset = intercircle.getSimplifiedInsetFromClockwiseLoop(center, radius)\n\t\tif intercircle.isLargeSameDirection(alreadyFilledInset, center, radius):\n\t\t\talreadyFilledLoop.append(alreadyFilledInset)\n\tif (len(alreadyFilledLoop) > 0):\n\t\talreadyFilledArounds.append(alreadyFilledLoop)\n",["add already filled loops around loop to alreadyfilledarounds ."]]
["def localize(value):\n\treturn force_unicode(formats.localize(value, use_l10n=True))\n",["checks if value is a localizable type and returns it formatted as a string using current locale format ."]]
["def _getwindowsize(folder_alias):\n\tfinder = _getfinder()\n\targs = {}\n\tattrs = {}\n\taeobj_0 = aetypes.ObjectSpecifier(want=aetypes.Type('cfol'), form='alis', seld=folder_alias, fr=None)\n\taeobj_1 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('cwnd'), fr=aeobj_0)\n\taeobj_2 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('posn'), fr=aeobj_1)\n\targs['----'] = aeobj_2\n\t(_reply, args, attrs) = finder.send('core', 'getd', args, attrs)\n\tif args.has_key('errn'):\n\t\traise Error, aetools.decodeerror(args)\n\tif args.has_key('----'):\n\t\treturn args['----']\n",["set the size of a finder window for folder to ."]]
["def parse_datetime(value):\n\tmatch = datetime_re.match(value)\n\tif match:\n\t\tkw = match.groupdict()\n\t\tif kw['microsecond']:\n\t\t\tkw['microsecond'] = kw['microsecond'].ljust(6, '0')\n\t\ttzinfo = kw.pop('tzinfo')\n\t\tif (tzinfo == 'Z'):\n\t\t\ttzinfo = utc\n\t\telif (tzinfo is not None):\n\t\t\toffset_mins = (int(tzinfo[(-2):]) if (len(tzinfo) > 3) else 0)\n\t\t\toffset = ((60 * int(tzinfo[1:3])) + offset_mins)\n\t\t\tif (tzinfo[0] == '-'):\n\t\t\t\toffset = (- offset)\n\t\t\ttzinfo = get_fixed_timezone(offset)\n\t\tkw = {k: int(v) for (k, v) in kw.items() if (v is not None)}\n\t\tkw['tzinfo'] = tzinfo\n\t\treturn datetime.datetime(**kw)\n",["parses a string and return a datetime ."]]
["def select(rlist, wlist, xlist, timeout=None):\n\tallevents = []\n\ttimeout = Timeout.start_new(timeout)\n\tresult = SelectResult()\n\ttry:\n\t\ttry:\n\t\t\tfor readfd in rlist:\n\t\t\t\tallevents.append(core.read_event(get_fileno(readfd), result.update, arg=readfd))\n\t\t\tfor writefd in wlist:\n\t\t\t\tallevents.append(core.write_event(get_fileno(writefd), result.update, arg=writefd))\n\t\texcept IOError as ex:\n\t\t\traise error(*ex.args)\n\t\tresult.event.wait(timeout=timeout)\n\t\treturn (result.read, result.write, [])\n\tfinally:\n\t\tfor evt in allevents:\n\t\t\tevt.cancel()\n\t\ttimeout.cancel()\n",["an implementation of :meth:select ."]]
["def get_size():\n\tplatf = platform.system()\n\tdimension = None\n\tif (platf == 'Windows'):\n\t\tdimension = _get_size_windows()\n\t\tif (dimension is None):\n\t\t\tdimension = _get_size_tput()\n\telif ((platf == 'Linux') or (platf == 'Darwin') or platf.startswith('CYGWIN')):\n\t\tdimension = _get_size_linux()\n\tif (dimension is None):\n\t\tdimension = (80, 25)\n\treturn dimension\n",["find the total dir and filesize of a path ."]]
["@csrf_protect\n@permission_required('comments.can_moderate')\ndef delete(request, comment_id, next=None):\n\tcomment = get_object_or_404(comments.get_model(), pk=comment_id, site__pk=settings.SITE_ID)\n\tif (request.method == 'POST'):\n\t\tperform_delete(request, comment)\n\t\treturn next_redirect(request, next, delete_done, c=comment.pk)\n\telse:\n\t\treturn render_to_response('comments\/delete.html', {'comment': comment, 'next': next}, template.RequestContext(request))\n",["delete an elb ."]]
["def load_handler(path, *args, **kwargs):\n\ti = path.rfind(u'.')\n\t(module, attr) = (path[:i], path[(i + 1):])\n\ttry:\n\t\tmod = importlib.import_module(module)\n\texcept ImportError as e:\n\t\traise ImproperlyConfigured((u'Error\timporting\tupload\thandler\tmodule\t%s:\t\"%s\"' % (module, e)))\n\texcept ValueError:\n\t\traise ImproperlyConfigured(u'Error\timporting\tupload\thandler\tmodule.Is\tFILE_UPLOAD_HANDLERS\ta\tcorrectly\tdefined\tlist\tor\ttuple?')\n\ttry:\n\t\tcls = getattr(mod, attr)\n\texcept AttributeError:\n\t\traise ImproperlyConfigured((u'Module\t\"%s\"\tdoes\tnot\tdefine\ta\t\"%s\"\tupload\thandler\tbackend' % (module, attr)))\n\treturn cls(*args, **kwargs)\n",["given a path to a handler ."]]
["def get_resource_manager_extra_kwargs(f, args, allow_conflicts=False):\n\thooks = getattr(f, 'resource_manager_kwargs_hooks', [])\n\textra_kwargs = {}\n\tfor hook in hooks:\n\t\thook_name = hook.__name__\n\t\thook_kwargs = hook(args)\n\t\tconflicting_keys = (set(hook_kwargs.keys()) & set(extra_kwargs.keys()))\n\t\tif (conflicting_keys and (not allow_conflicts)):\n\t\t\traise Exception((\"Hook\t'%(hook_name)s'\tis\tattempting\tto\tredefine\tattributes\t'%(conflicting_keys)s'\" % locals()))\n\t\textra_kwargs.update(hook_kwargs)\n\treturn extra_kwargs\n",["return extra_kwargs by calling resource manager kwargs hooks ."]]
["def _dynamic_max_trials(n_inliers, n_samples, min_samples, probability):\n\tinlier_ratio = (n_inliers \/ float(n_samples))\n\tnom = max(_EPSILON, (1 - probability))\n\tdenom = max(_EPSILON, (1 - (inlier_ratio ** min_samples)))\n\tif (nom == 1):\n\t\treturn 0\n\tif (denom == 1):\n\t\treturn float('inf')\n\treturn abs(float(np.ceil((np.log(nom) \/ np.log(denom)))))\n",["determine number trials such that at least one outlier-free subset is sampled for the given inlier\/outlier ratio ."]]
["def get_args(namespace):\n\targv = [sys.argv[0]]\n\tif (namespace.qt_flag is not None):\n\t\targv += [('-' + flag[0]) for flag in namespace.qt_flag]\n\tif (namespace.qt_arg is not None):\n\t\tfor (name, value) in namespace.qt_arg:\n\t\t\targv += [('-' + name), value]\n\treturn argv\n",["supports the command-line arguments listed below ."]]
["def server(host, port, func):\n\tdef handler(conn):\n\t\ttry:\n\t\t\t(yield func(conn))\n\t\tfinally:\n\t\t\tconn.close()\n\tlistener = Listener(host, port)\n\ttry:\n\t\twhile True:\n\t\t\tconn = (yield listener.accept())\n\t\t\t(yield spawn(handler(conn)))\n\texcept KeyboardInterrupt:\n\t\tpass\n\tfinally:\n\t\tlistener.close()\n",["open a tcp server in three steps 1) set evt to true to let the parent know we are ready 2) [optional] if is not false ."]]
["def config_from_file(filename, config=None):\n\tif config:\n\t\ttry:\n\t\t\twith open(filename, 'w') as fdesc:\n\t\t\t\tfdesc.write(json.dumps(config))\n\t\texcept IOError as error:\n\t\t\t_LOGGER.error('Saving\tconfig\tfile\tfailed:\t%s', error)\n\t\t\treturn False\n\t\treturn config\n\telif os.path.isfile(filename):\n\t\ttry:\n\t\t\twith open(filename, 'r') as fdesc:\n\t\t\t\treturn json.loads(fdesc.read())\n\t\texcept IOError as error:\n\t\t\t_LOGGER.error('Reading\tconfig\tfile\tfailed:\t%s', error)\n\t\t\treturn False\n\telse:\n\t\treturn {}\n",["small configuration file management function ."]]
["def url_params_from_lookup_dict(lookups):\n\tparams = {}\n\tif (lookups and hasattr(lookups, u'items')):\n\t\titems = []\n\t\tfor (k, v) in lookups.items():\n\t\t\tif callable(v):\n\t\t\t\tv = v()\n\t\t\tif isinstance(v, (tuple, list)):\n\t\t\t\tv = u','.join([str(x) for x in v])\n\t\t\telif isinstance(v, bool):\n\t\t\t\tv = (u'0', u'1')[v]\n\t\t\telse:\n\t\t\t\tv = six.text_type(v)\n\t\t\titems.append((k, v))\n\t\tparams.update(dict(items))\n\treturn params\n",["converts the type of lookups specified in a foreignkey limit_choices_to attribute to a dictionary of query parameters ."]]
["def html_body(input_string, source_path=None, destination_path=None, input_encoding='unicode', output_encoding='unicode', doctitle=True, initial_header_level=1):\n\tparts = html_parts(input_string=input_string, source_path=source_path, destination_path=destination_path, input_encoding=input_encoding, doctitle=doctitle, initial_header_level=initial_header_level)\n\tfragment = parts['html_body']\n\tif (output_encoding != 'unicode'):\n\t\tfragment = fragment.encode(output_encoding)\n\treturn fragment\n",["given an input string ."]]
["def _date_from_string(year, year_format, month='', month_format='', day='', day_format='', delim='__'):\n\tformat = delim.join((year_format, month_format, day_format))\n\tdatestr = delim.join((year, month, day))\n\ttry:\n\t\treturn datetime.datetime.strptime(datestr, format).date()\n\texcept ValueError:\n\t\traise Http404((_(\"Invalid\tdate\tstring\t'%(datestr)s'\tgiven\tformat\t'%(format)s'\") % {'datestr': datestr, 'format': format}))\n",["helper: get a datetime ."]]
["def _syscmd_file(target, default=''):\n\tif (sys.platform in ('dos', 'win32', 'win16', 'os2')):\n\t\treturn default\n\ttarget = _follow_symlinks(target)\n\ttry:\n\t\tf = os.popen(('file\t\"%s\"\t2>\t\/dev\/null' % target))\n\texcept (AttributeError, os.error):\n\t\treturn default\n\toutput = string.strip(f.read())\n\trc = f.close()\n\tif ((not output) or rc):\n\t\treturn default\n\telse:\n\t\treturn output\n",["interface to the systems file command ."]]
["def headers_to_account_info(headers, status_int=HTTP_OK):\n\t(headers, meta, sysmeta) = _prep_headers_to_info(headers, 'account')\n\taccount_info = {'status': status_int, 'container_count': headers.get('x-account-container-count'), 'total_object_count': headers.get('x-account-object-count'), 'bytes': headers.get('x-account-bytes-used'), 'meta': meta, 'sysmeta': sysmeta}\n\tif is_success(status_int):\n\t\taccount_info['account_really_exists'] = (not config_true_value(headers.get('x-backend-fake-account-listing')))\n\treturn account_info\n",["construct a cacheable dict of account info based on response headers ."]]
["def populate_xheaders(request, response, model, object_id):\n\tfrom django.conf import settings\n\tif ((request.META.get('REMOTE_ADDR') in settings.INTERNAL_IPS) or (hasattr(request, 'user') and request.user.is_authenticated() and request.user.is_staff)):\n\t\tresponse['X-Object-Type'] = ('%s.%s' % (model._meta.app_label, model._meta.object_name.lower()))\n\t\tresponse['X-Object-Id'] = str(object_id)\n",["adds the \"x-object-type\" and \"x-object-id\" headers to the given httpresponse according to the given model and object_id -- but only if the given httprequest object has an ip"]]
["def format_histograms(raw_hist, pre_hist, post_hist, bin_edges):\n\tlines = []\n\tlines.append('#\tbins\traw\tsequence\tlengths,\tlength\tof\tsequences\tthat\tpass\tquality\tfilters\tbefore\tprocessing,\tand\tlengths\tof\tsequences\tthat\tpass\tquality\tfilters\tpost\tprocessing.')\n\tlines.append('Length DCTB Raw DCTB Before DCTB After')\n\tfor (edge, raw, pre, post) in zip(bin_edges, raw_hist, pre_hist, post_hist):\n\t\tlines.append(' DCTB '.join(map(str, [edge, raw, pre, post])))\n\treturn '\\n'.join(lines)\n",["returns text-formatted histogram ."]]
["def convert_path(pathname):\n\tif (os.sep == '\/'):\n\t\treturn pathname\n\tif (not pathname):\n\t\treturn pathname\n\tif (pathname[0] == '\/'):\n\t\traise ValueError((\"path\t'%s'\tcannot\tbe\tabsolute\" % pathname))\n\tif (pathname[(-1)] == '\/'):\n\t\traise ValueError((\"path\t'%s'\tcannot\tend\twith\t'\/'\" % pathname))\n\tpaths = pathname.split('\/')\n\twhile ('.' in paths):\n\t\tpaths.remove('.')\n\tif (not paths):\n\t\treturn os.curdir\n\treturn os.path.join(*paths)\n",["return pathname as a name that will work on the native filesystem ."]]
["def text_repr(value):\n\ttry:\n\t\treturn pydoc.text.repr(value)\n\texcept KeyboardInterrupt:\n\t\traise\n\texcept:\n\t\ttry:\n\t\t\treturn repr(value)\n\t\texcept KeyboardInterrupt:\n\t\t\traise\n\t\texcept:\n\t\t\ttry:\n\t\t\t\tname = getattr(value, '__name__', None)\n\t\t\t\tif name:\n\t\t\t\t\treturn text_repr(name)\n\t\t\t\tklass = getattr(value, '__class__', None)\n\t\t\t\tif klass:\n\t\t\t\t\treturn ('%s\tinstance' % text_repr(klass))\n\t\t\texcept KeyboardInterrupt:\n\t\t\t\traise\n\t\t\texcept:\n\t\t\t\treturn 'UNRECOVERABLE\tREPR\tFAILURE'\n",["hopefully pretty robust repr equivalent ."]]
["def manhattan_distances(X, Y):\n\tif (X is Y):\n\t\tX = Y = np.asanyarray(X)\n\telse:\n\t\tX = np.asanyarray(X)\n\t\tY = np.asanyarray(Y)\n\tif (X.shape[1] != Y.shape[1]):\n\t\traise ValueError('Incompatible\tdimension\tfor\tX\tand\tY\tmatrices')\n\tXY = ssd.cdist(X, Y, 'cityblock')\n\treturn (1.0 - (XY \/ float(X.shape[1])))\n",["considering the rows of x as vectors ."]]
["def create_bootstrap_script(extra_text, python_version=''):\n\tfilename = __file__\n\tif filename.endswith('.pyc'):\n\t\tfilename = filename[:(-1)]\n\twith codecs.open(filename, 'r', encoding='utf-8') as f:\n\t\tcontent = f.read()\n\tpy_exe = ('python%s' % python_version)\n\tcontent = ((('#!\/usr\/bin\/env\t%s\\n' % py_exe) + '##\tWARNING:\tThis\tfile\tis\tgenerated\\n') + content)\n\treturn content.replace('##EXTEND##', extra_text)\n",["creates a bootstrap script ."]]
["def check_password(environ, username, password):\n\tUserModel = auth.get_user_model()\n\tdb.reset_queries()\n\ttry:\n\t\ttry:\n\t\t\tuser = UserModel._default_manager.get_by_natural_key(username)\n\t\texcept UserModel.DoesNotExist:\n\t\t\treturn None\n\t\tif (not user.is_active):\n\t\t\treturn None\n\t\treturn user.check_password(password)\n\tfinally:\n\t\tdb.close_old_connections()\n",["returns a boolean of whether the raw password matches the three part encoded digest ."]]
["def make_image_dict(image):\n\tdef _fetch_attrs(d, attrs):\n\t\treturn dict([(a, d[a]) for a in attrs if (a in d.keys())])\n\tproperties = dict(((p['name'], p['value']) for p in image['properties'] if (not p['deleted'])))\n\timage_dict = _fetch_attrs(image, glance.db.IMAGE_ATTRS)\n\timage_dict['properties'] = properties\n\t_limit_locations(image_dict)\n\treturn image_dict\n",["create a dict representation of an image which we can use to serialize the image ."]]
["def cookies(*requireds, **defaults):\n\tif (defaults.get('_unicode') is True):\n\t\tdefaults['_unicode'] = decode_cookie\n\tif ('_parsed_cookies' not in ctx):\n\t\thttp_cookie = ctx.env.get('HTTP_COOKIE', '')\n\t\tctx._parsed_cookies = parse_cookies(http_cookie)\n\ttry:\n\t\treturn storify(ctx._parsed_cookies, *requireds, **defaults)\n\texcept KeyError:\n\t\tbadrequest()\n\t\traise StopIteration()\n",["returns a storage object with all the request cookies in it ."]]
["def newer(source, target):\n\tif (not os.path.exists(source)):\n\t\traise DistutilsFileError((\"file\t'%s'\tdoes\tnot\texist\" % os.path.abspath(source)))\n\tif (not os.path.exists(target)):\n\t\treturn 1\n\tfrom stat import ST_MTIME\n\tmtime1 = os.stat(source)[ST_MTIME]\n\tmtime2 = os.stat(target)[ST_MTIME]\n\treturn (mtime1 > mtime2)\n",["return true if source exists and is more recently modified than target ."]]
["def render_value_in_context(value, context):\n\tvalue = template_localtime(value, use_tz=context.use_tz)\n\tvalue = localize(value, use_l10n=context.use_l10n)\n\tvalue = force_text(value)\n\tif ((context.autoescape and (not isinstance(value, SafeData))) or isinstance(value, EscapeData)):\n\t\treturn escape(value)\n\telse:\n\t\treturn value\n",["converts any value to a string to become part of a rendered template ."]]
["def int_to_base36(i):\n\tdigits = u'0123456789abcdefghijklmnopqrstuvwxyz'\n\tfactor = 0\n\tif (i < 0):\n\t\traise ValueError(u'Negative\tbase36\tconversion\tinput.')\n\tif (not six.PY3):\n\t\tif (not isinstance(i, six.integer_types)):\n\t\t\traise TypeError(u'Non-integer\tbase36\tconversion\tinput.')\n\t\tif (i > sys.maxint):\n\t\t\traise ValueError(u'Base36\tconversion\tinput\ttoo\tlarge.')\n\twhile True:\n\t\tfactor += 1\n\t\tif (i < (36 ** factor)):\n\t\t\tfactor -= 1\n\t\t\tbreak\n\tbase36 = []\n\twhile (factor >= 0):\n\t\tj = (36 ** factor)\n\t\tbase36.append(digits[(i \/\/ j)])\n\t\ti = (i % j)\n\t\tfactor -= 1\n\treturn u''.join(base36)\n",["converts an integer to a base36 string ."]]
["def test_prompt_should_ask_and_rm_repo_dir(mocker, tmpdir):\n\tmock_read_user = mocker.patch('cookiecutter.vcs.read_user_yes_no', return_value=True, autospec=True)\n\trepo_dir = tmpdir.mkdir('repo')\n\tvcs.prompt_and_delete_repo(str(repo_dir))\n\tassert mock_read_user.called\n\tassert (not repo_dir.exists())\n",["in prompt_and_delete_repo() ."]]
["def volume_info(path):\n\t(out, err) = utils.execute('lvs', '-o', 'vg_all,lv_all', '--separator', '|', path, run_as_root=True)\n\tinfo = [line.split('|') for line in out.splitlines()]\n\tif (len(info) != 2):\n\t\traise RuntimeError((_('Path\t%s\tmust\tbe\tLVM\tlogical\tvolume') % path))\n\treturn dict(zip(*info))\n",["get logical volume info ."]]
["def widthratio(parser, token):\n\tbits = token.contents.split()\n\tif (len(bits) != 4):\n\t\traise TemplateSyntaxError('widthratio\ttakes\tthree\targuments')\n\t(tag, this_value_expr, max_value_expr, max_width) = bits\n\treturn WidthRatioNode(parser.compile_filter(this_value_expr), parser.compile_filter(max_value_expr), parser.compile_filter(max_width))\n",["for creating bar charts and such ."]]
["def _AddClearFieldMethod(message_descriptor, cls):\n\tdef ClearField(self, field_name):\n\t\ttry:\n\t\t\tfield = message_descriptor.fields_by_name[field_name]\n\t\texcept KeyError:\n\t\t\traise ValueError(('Protocol\tmessage\thas\tno\t\"%s\"\tfield.' % field_name))\n\t\tif (field in self._fields):\n\t\t\tdel self._fields[field]\n\t\tself._Modified()\n\tcls.ClearField = ClearField\n",["helper for _addmessagemethods() ."]]
["@pytest.mark.network\ndef test_download_vcs_link(script):\n\tresult = script.pip('install', '-d', '.', 'git+git:\/\/github.com\/pypa\/pip-test-package.git', expect_stderr=True)\n\tassert ((Path('scratch') \/ 'pip-test-package-0.1.1.zip') in result.files_created)\n\tassert ((script.site_packages \/ 'piptestpackage') not in result.files_created)\n",["it should allow -d flag for vcs links ."]]
["def UnregisterNamedPath(name):\n\tkeyStr = ((BuildDefaultPythonKey() + '\\\\PythonPath\\\\') + name)\n\ttry:\n\t\twin32api.RegDeleteKey(GetRootKey(), keyStr)\n\texcept win32api.error as (code, fn, details):\n\t\timport winerror\n\t\tif (code != winerror.ERROR_FILE_NOT_FOUND):\n\t\t\traise win32api.error, (code, fn, desc)\n\t\treturn\n",["unregister a named path - ie ."]]
["def test_nm2_sample_wrong_X():\n\tnm2 = NearMiss(random_state=RND_SEED, version=VERSION_NEARMISS)\n\tnm2.fit(X, Y)\n\tassert_raises(RuntimeError, nm2.sample, np.random.random((100, 40)), np.array((([0] * 50) + ([1] * 50))))\n",["test either if an error is raised when x is different at fitting and sampling ."]]
["def get_random_string(length=12, allowed_chars=u'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'):\n\tif (not using_sysrandom):\n\t\trandom.seed(hashlib.sha256((u'%s%s%s' % (random.getstate(), time.time(), settings.SECRET_KEY)).encode(u'utf-8')).digest())\n\treturn u''.join([random.choice(allowed_chars) for i in range(length)])\n",["returns a securely generated random string ."]]
["def get_cashflow_data(year, quarter):\n\tif (ct._check_input(year, quarter) is True):\n\t\tct._write_head()\n\t\tdf = _get_cashflow_data(year, quarter, 1, pd.DataFrame())\n\t\tif (df is not None):\n\t\t\tdf['code'] = df['code'].map((lambda x: str(x).zfill(6)))\n\t\treturn df\n",["parameters year:int \u5e74\u5ea6 e ."]]
["def set_maxdays(name, days):\n\tminutes = ((days * 24) * 60)\n\t_set_account_policy(name, 'maxMinutesUntilChangePassword={0}'.format(minutes))\n\treturn (get_maxdays(name) == days)\n",["set the maximum number of days during which a password is valid ."]]
["def run_on_executor(*args, **kwargs):\n\tdef run_on_executor_decorator(fn):\n\t\texecutor = kwargs.get('executor', 'executor')\n\t\tio_loop = kwargs.get('io_loop', 'io_loop')\n\t\t@functools.wraps(fn)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tcallback = kwargs.pop('callback', None)\n\t\t\tfuture = getattr(self, executor).submit(fn, self, *args, **kwargs)\n\t\t\tif callback:\n\t\t\t\tgetattr(self, io_loop).add_future(future, (lambda future: callback(future.result())))\n\t\t\treturn future\n\t\treturn wrapper\n\tif (args and kwargs):\n\t\traise ValueError('cannot\tcombine\tpositional\tand\tkeyword\targs')\n\tif (len(args) == 1):\n\t\treturn run_on_executor_decorator(args[0])\n\telif (len(args) != 0):\n\t\traise ValueError('expected\t1\targument,\tgot\t%d', len(args))\n\treturn run_on_executor_decorator\n",["decorator to run a synchronous method asynchronously on an executor ."]]
["def action_peek_json(body):\n\ttry:\n\t\tdecoded = jsonutils.loads(body)\n\texcept ValueError:\n\t\tmsg = _('cannot\tunderstand\tJSON')\n\t\traise exception.MalformedRequestBody(reason=msg)\n\tif (len(decoded) != 1):\n\t\tmsg = _('too\tmany\tbody\tkeys')\n\t\traise exception.MalformedRequestBody(reason=msg)\n\treturn list(decoded.keys())[0]\n",["determine action to invoke ."]]
["def allow_cross_site_request(f):\n\t@functools.wraps(f)\n\tdef wrapper(request, *args, **kw):\n\t\tresponse = f(request, *args, **kw)\n\t\t\"If\tAccess-Control-Allow-Credentials\tisn't\tset,\tthe\tbrowser\twon't\\n\t\t\t\t\t\t\t\treturn\tdata\trequired\tcookies\tto\tsee.\t\tThis\tis\ta\tgood\tthing,\tlet's\tkeep\\n\t\t\t\t\t\t\t\tit\tthat\tway.\"\n\t\tresponse['Access-Control-Allow-Origin'] = '*'\n\t\tresponse['Access-Control-Allow-Methods'] = 'GET'\n\t\treturn response\n\treturn wrapper\n",["allow other sites to access this resource ."]]
["def perform_import(val, setting_name):\n\tif isinstance(val, six.string_types):\n\t\treturn import_from_string(val, setting_name)\n\telif isinstance(val, (list, tuple)):\n\t\treturn [import_from_string(item, setting_name) for item in val]\n\treturn val\n",["if the given setting is a string import notation ."]]
["def instance_update(context, instance_uuid, values, update_cells=True):\n\trv = IMPL.instance_update(context, instance_uuid, values)\n\tif update_cells:\n\t\ttry:\n\t\t\tcells_rpcapi.CellsAPI().instance_update_at_top(context, rv)\n\t\texcept Exception:\n\t\t\tLOG.exception(_('Failed\tto\tnotify\tcells\tof\tinstance\tupdate'))\n\treturn rv\n",["set the given properties on an instance and update it ."]]
["def register_filter(f):\n\tif (not issubclass(f, Filter)):\n\t\traise ValueError(\"Must\tbe\ta\tsubclass\tof\t'Filter'\")\n\tif (not f.name):\n\t\traise ValueError('Must\thave\ta\tname')\n\tif (f.name in _FILTERS):\n\t\traise KeyError(('Filter\twith\tname\t%s\talready\tregistered' % f.name))\n\t_FILTERS[f.name] = f\n",["add the given filter to the list of know filters ."]]
["@ioflo.base.deeding.deedify('SaltRaetReactorFork', ioinits={'opts': '.salt.opts', 'proc_mgr': '.salt.usr.proc_mgr'})\ndef reactor_fork(self):\n\tself.proc_mgr.value.add_process(salt.utils.reactor.Reactor, args=(self.opts.value,))\n",["add a reactor object to the process manager ."]]
["def split_into(n, type, value):\n\tparts = map((lambda x: x.strip()), value.split(';', (n - 1)))\n\tif (sum((1 for part in parts if part)) < n):\n\t\traise ValueError(('invalid\t%s\tindex\tentry\t%r' % (type, value)))\n\treturn parts\n",["split an index entry into a given number of parts at semicolons ."]]
["def define_db(conn, db, host, port, user, passwd):\n\tdal = 'db\t=\tDAL(\"mssql4:\/\/%s:%s@%s:%s\/%s\",\tpool_size=10,\tdecode_credentials=True)'\n\tprint (dal % (user.replace('@', '%40').replace(':', '%3A'), passwd.replace('@', '%40').replace(':', '%3A'), host, port, db))\n\tprint\n\tprint 'migrate\t=\tFalse'\n\tprint\n\tfor table in get_tables(conn):\n\t\tdefine_table(conn, table)\n",["output database definition ."]]
["def libvlc_new(argc, argv):\n\tf = (_Cfunctions.get('libvlc_new', None) or _Cfunction('libvlc_new', ((1,), (1,)), class_result(Instance), ctypes.c_void_p, ctypes.c_int, ListPOINTER(ctypes.c_char_p)))\n\treturn f(argc, argv)\n",["create and initialize a libvlc instance ."]]
["def allow_lazy(func, *resultclasses):\n\t@wraps(func)\n\tdef wrapper(*args, **kwargs):\n\t\tfor arg in (list(args) + list(six.itervalues(kwargs))):\n\t\t\tif isinstance(arg, Promise):\n\t\t\t\tbreak\n\t\telse:\n\t\t\treturn func(*args, **kwargs)\n\t\treturn lazy(func, *resultclasses)(*args, **kwargs)\n\treturn wrapper\n",["a decorator that allows a function to be called with one or more lazy arguments ."]]
["def DocFileSuite(*paths, **kw):\n\tsuite = unittest.TestSuite()\n\tif kw.get('module_relative', True):\n\t\tkw['package'] = _normalize_module(kw.get('package'))\n\tfor path in paths:\n\t\tsuite.addTest(DocFileTest(path, **kw))\n\treturn suite\n",["a unittest suite for one or more doctest files ."]]
["def get_sql_flush(style, tables, sequences):\n\tsql = [('%s\t%s\t%s;' % (style.SQL_KEYWORD('DELETE'), style.SQL_KEYWORD('FROM'), style.SQL_FIELD(quote_name(table)))) for table in tables]\n\treturn sql\n",["return a list of sql statements required to remove all data from all tables in the database and put the database in an empty initial state ."]]
["def padTo(n, seq, default=None):\n\tif (len(seq) > n):\n\t\traise ValueError(('%d\telements\tis\tmore\tthan\t%d.' % (len(seq), n)))\n\tblank = ([default] * n)\n\tblank[:len(seq)] = list(seq)\n\treturn blank\n",["pads a sequence out to n elements ."]]
["def _a_encode_unicode(value, mapping):\n\tassert isinstance(value, unicode), ('VALUE\thas\tinvalid\ttype:\t%s' % type(value))\n\tvalue = value.encode('UTF-8')\n\treturn (str(len(value)).encode('UTF-8'), 's', value)\n",["foo-bar --> ."]]
["def getent(refresh=False):\n\tif (('group.getent' in __context__) and (not refresh)):\n\t\treturn __context__['group.getent']\n\tret = []\n\tpythoncom.CoInitialize()\n\tnt = win32com.client.Dispatch('AdsNameSpaces')\n\tresults = nt.GetObject('', 'WinNT:\/\/.')\n\tresults.Filter = ['group']\n\tfor result in results:\n\t\tmember_list = []\n\t\tfor member in result.members():\n\t\t\tmember_list.append(member.AdsPath.replace('WinNT:\/\/', '').replace('\/', '\\\\').encode('ascii', 'backslashreplace'))\n\t\tgroup = {'gid': __salt__['file.group_to_gid'](result.name), 'members': member_list, 'name': result.name, 'passwd': 'x'}\n\t\tret.append(group)\n\t__context__['group.getent'] = ret\n\treturn ret\n",["return info on all groups cli example: ."]]
["def image_meta(system_metadata):\n\timage_meta = {}\n\tfor (md_key, md_value) in system_metadata.items():\n\t\tif md_key.startswith('image_'):\n\t\t\timage_meta[md_key[6:]] = md_value\n\treturn image_meta\n",["format image metadata for use in notifications from the instance system metadata ."]]
["def get_language():\n\tt = getattr(_active, u'value', None)\n\tif (t is not None):\n\t\ttry:\n\t\t\treturn t.to_language()\n\t\texcept AttributeError:\n\t\t\tpass\n\tfrom django.conf import settings\n\treturn settings.LANGUAGE_CODE\n",["returns the currently selected language ."]]
["def add_mimetypes():\n\timport mimetypes\n\tmimetypes.add_type('application\/vnd.ms-fontobject', '.eot')\n\tmimetypes.add_type('application\/x-font-opentype', '.otf')\n\tmimetypes.add_type('application\/x-font-ttf', '.ttf')\n\tmimetypes.add_type('application\/font-woff', '.woff')\n",["add extra mimetypes ."]]
["def addToMenu(master, menu, repository, window):\n\tmetaFilePath = archive.getSkeinforgePluginsPath('meta.py')\n\tsettings.addPluginsParentToMenu(skeinforge_meta.getPluginsDirectoryPath(), menu, metaFilePath, skeinforge_meta.getPluginFileNames())\n",["add a tool plugin menu ."]]
["def randomRange(start=0, stop=1000, seed=None):\n\tif (seed is not None):\n\t\t_ = getCurrentThreadData().random\n\t\t_.seed(seed)\n\t\trandint = _.randint\n\telse:\n\t\trandint = random.randint\n\treturn int(randint(start, stop))\n",["returns random integer value in given range ."]]
["def status(name, sig=None):\n\tif sig:\n\t\treturn bool(__salt__['status.pid'](sig))\n\tcmd = '{0}\tcheck\t{1}'.format(_cmd(), name)\n\treturn (not __salt__['cmd.retcode'](cmd))\n",["return the status for a service ."]]
["def metadef_resource_type_get(context, resource_type_name, session=None):\n\tsession = (session or get_session())\n\treturn metadef_resource_type_api.get(context, resource_type_name, session)\n",["get a resource_type ."]]
["def grains():\n\tif (not DETAILS.get('grains_cache', {})):\n\t\tr = salt.utils.http.query((DETAILS['url'] + 'info'), decode_type='json', decode=True)\n\t\tDETAILS['grains_cache'] = r['dict']\n\treturn DETAILS['grains_cache']\n",["get the grains from the proxied device ."]]
["def unregister_models(engine):\n\tmodels = (ArtifactDependency, ArtifactBlobLocation, ArtifactBlob, ArtifactProperty, ArtifactTag, Artifact)\n\tfor model in models:\n\t\tmodel.metadata.drop_all(engine)\n",["drop database tables for all models with the given engine ."]]
["def get_connection(backend=None, fail_silently=False, **kwds):\n\tpath = (backend or settings.EMAIL_BACKEND)\n\ttry:\n\t\t(mod_name, klass_name) = path.rsplit('.', 1)\n\t\tmod = import_module(mod_name)\n\texcept ImportError as e:\n\t\traise ImproperlyConfigured(('Error\timporting\temail\tbackend\tmodule\t%s:\t\"%s\"' % (mod_name, e)))\n\ttry:\n\t\tklass = getattr(mod, klass_name)\n\texcept AttributeError:\n\t\traise ImproperlyConfigured(('Module\t\"%s\"\tdoes\tnot\tdefine\ta\t\"%s\"\tclass' % (mod_name, klass_name)))\n\treturn klass(fail_silently=fail_silently, **kwds)\n",["load an email backend and return an instance of it ."]]
["@profiler.trace\ndef flavor_extra_set(request, flavor_id, metadata):\n\tflavor = novaclient(request).flavors.get(flavor_id)\n\tif (not metadata):\n\t\treturn None\n\treturn flavor.set_keys(metadata)\n",["set the flavor extra spec keys ."]]
["def gcd(a, b):\n\twhile b:\n\t\t(a, b) = (b, (a % b))\n\treturn a\n",["returns the greatest common divisor of p and q ."]]
["def _output_to_list(cmdoutput):\n\treturn [item for line in cmdoutput.splitlines() if _safe_output(line) for item in line.split()]\n",["convert rabbitmqctl output to a list of strings ."]]
["def convert_image(source, dest, out_format, run_as_root=False):\n\tcmd = ('qemu-img', 'convert', '-O', out_format, source, dest)\n\tutils.execute(run_as_root=run_as_root, *cmd)\n",["convert image to other format ."]]
["def run_migrations_offline():\n\tset_mysql_engine()\n\tkwargs = dict()\n\tif neutron_config.database.connection:\n\t\tkwargs['url'] = neutron_config.database.connection\n\telse:\n\t\tkwargs['dialect_name'] = neutron_config.database.engine\n\tkwargs['include_object'] = include_object\n\tcontext.configure(**kwargs)\n\twith context.begin_transaction():\n\t\tcontext.run_migrations()\n",["run migrations in offline mode ."]]
["def writeOutput(fileName=''):\n\tfileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName)\n\tif (fileName != ''):\n\t\tskeinforge_craft.writeChainTextWithNounMessage(fileName, 'unpause')\n",["fillet a gcode linear move file ."]]
["def is_admin_context(context):\n\tif (not context):\n\t\tLOG.warning(_LW('Use\tof\tempty\trequest\tcontext\tis\tdeprecated'), DeprecationWarning)\n\t\traise Exception('die')\n\treturn context.is_admin\n",["indicates if the request context is an administrator ."]]
["@pipeline.mutator_stage\ndef import_asis(session, task):\n\tif task.skip:\n\t\treturn\n\tlog.info(u'{}', displayable_path(task.paths))\n\ttask.set_choice(action.ASIS)\n\tapply_choice(session, task)\n",["select the action ."]]
["def token_view(request):\n\tcontext = RequestContext(request, processors=[csrf])\n\ttemplate = Template(u'{%\tcsrf_token\t%}')\n\treturn HttpResponse(template.render(context))\n",["a view that uses {% csrf_token %} ."]]
["def spawn(coro):\n\tif (not isinstance(coro, types.GeneratorType)):\n\t\traise ValueError((u'%s\tis\tnot\ta\tcoroutine' % coro))\n\treturn SpawnEvent(coro)\n",["event: add another coroutine to the scheduler ."]]
["def words(string, filter=(lambda w: w.strip(\"'\").isalnum()), punctuation=PUNCTUATION, **kwargs):\n\tstring = decode_utf8(string)\n\tstring = re.sub(\"([a-z|A-Z])'(m|s|ve|re|ll|d)\", u'\\\\1\t<QUOTE\/>\\\\2', string)\n\tstring = re.sub(\"(c|d|gl|j|l|m|n|s|t|un)'([a-z|A-Z])\", u'\\\\1<QUOTE\/>\t\\\\2', string)\n\twords = (w.strip(punctuation).replace(u'<QUOTE\/>', \"'\", 1) for w in string.split())\n\twords = (w for w in words if ((filter is None) or (filter(w) is not False)))\n\twords = [w for w in words if w]\n\treturn words\n",["an iterator over tokens in text ."]]
["def memory(since=0.0):\n\tans = get_memory()\n\tans \/= float((1024 ** 2))\n\treturn (ans - since)\n",["return memory usage in bytes ."]]
["def runtime_rewriter_middleware(application):\n\treturn functools.partial(_rewriter_middleware, _REQUEST_REWRITER_CHAIN, _RUNTIME_RESPONSE_REWRITER_CHAIN, application)\n",["wsgi middleware application that applies a chain of response rewriters ."]]
["def foobar_explosion(radius):\n\tdef get_55():\n\t\t'A\tfunction\tthat\treturns\t55.'\n\t\treturn 55\n\treturn (get_55() * radius)\n",["a nice and neat way of documenting code ."]]
["def block_device_mapping_destroy(context, bdm_id):\n\treturn IMPL.block_device_mapping_destroy(context, bdm_id)\n",["destroy the block device mapping ."]]
["def getShouldReverse(xmlElement):\n\treturn evaluate.getEvaluatedBooleanDefault(True, 'reverse', xmlElement)\n",["determine if the loop should be reversed ."]]
["def floating_ip_deallocate(context, address):\n\treturn IMPL.floating_ip_deallocate(context, address)\n",["deallocate a floating ip by address ."]]
["@context.quietfunc\n@with_device\ndef exists(path):\n\twith AdbClient() as c:\n\t\treturn bool(c.stat(path))\n",["test whether a path exists ."]]
["@hug.default_input_format('application\/made-up')\ndef made_up_formatter(data):\n\treturn data\n",["for testing ."]]
["@step(u'{word:w}\tstep\tfails')\ndef step_fails(context, word):\n\tassert False, 'EXPECT:\tFailing\tstep'\n",["step that always fails ."]]
["def new(rsa_key):\n\treturn PKCS115_SigScheme(rsa_key)\n",["create a new cast-128 cipher ."]]
["def get_dependencies():\n\tdeps = {'netaddr': HAS_NETADDR, 'python-novaclient': nova.check_nova()}\n\treturn config.check_driver_dependencies(__virtualname__, deps)\n",["warn if dependencies arent met ."]]
["def station():\n\toutput = s3_rest_controller(rheader=police_rheader)\n\treturn output\n",["restful crud controller ."]]
["def getNewDerivation(elementNode):\n\treturn evaluate.EmptyObject(elementNode)\n",["get new derivation ."]]
["def getNewDerivation(elementNode):\n\treturn evaluate.EmptyObject(elementNode)\n",["get new derivation ."]]
["def parse_decimal(string):\n\treturn get_i18n().parse_decimal(string)\n",["see :meth:i18n ."]]
["def set_nodes(nodes):\n\tglobal _FAKE_NODES\n\t_FAKE_NODES = nodes\n",["sets fakedrivers node ."]]
["def hours(h):\n\treturn (h \/ 24.0)\n",["return hours as days ."]]
["def getNewRepository():\n\treturn ExportRepository()\n",["get new repository ."]]
["def GetBatchJobDownloadUrlWhenReady(client, batch_job_id, max_poll_attempts=MAX_POLL_ATTEMPTS):\n\tbatch_job = GetBatchJob(client, batch_job_id)\n\tif (batch_job['status'] == 'CANCELED'):\n\t\traise Exception(('Batch\tJob\twith\tID\t\"%s\"\twas\tcanceled\tbefore\tcompleting.' % batch_job_id))\n\tpoll_attempt = 0\n\twhile ((poll_attempt in range(max_poll_attempts)) and (batch_job['status'] in PENDING_STATUSES)):\n\t\tsleep_interval = ((30 * (2 ** poll_attempt)) + (random.randint(0, 10000) \/ 1000))\n\t\tprint ('Batch\tJob\tnot\tready,\tsleeping\tfor\t%s\tseconds.' % sleep_interval)\n\t\ttime.sleep(sleep_interval)\n\t\tbatch_job = GetBatchJob(client, batch_job_id)\n\t\tpoll_attempt += 1\n\t\tif ('downloadUrl' in batch_job):\n\t\t\turl = batch_job['downloadUrl']['url']\n\t\t\tprint ('Batch\tJob\twith\tId\t\"%s\",\tStatus\t\"%s\",\tand\tDownloadUrl\t\"%s\"\tready.' % (batch_job['id'], batch_job['status'], url))\n\t\t\treturn url\n\tprint ('BatchJob\twith\tID\t\"%s\"\tis\tbeing\tcanceled\tbecause\tit\twas\tin\ta\tpending\tstate\tafter\tpolling\t%d\ttimes.' % (batch_job_id, max_poll_attempts))\n\tCancelBatchJob(client, batch_job)\n",["retrieves the downloadurl when the batchjob is complete ."]]
["@handle_response_format\n@treeio_login_required\ndef my_activity(request, response_format='html'):\n\tprofile = request.user.profile\n\tupdates = UpdateRecord.objects.filter(author=profile).distinct()\n\tif request.POST:\n\t\trecord = UpdateRecord()\n\t\trecord.record_type = 'share'\n\t\tform = UpdateRecordForm(request.POST, user=profile, instance=record)\n\t\tif form.is_valid():\n\t\t\trecord = form.save(commit=False)\n\t\t\trecord.body = record.body.replace('\\n', '\t<br\t\/>')\n\t\t\trecord.save()\n\t\t\trecord.set_user_from_request(request)\n\t\t\treturn HttpResponseRedirect(reverse('news_my_activity'))\n\telse:\n\t\tform = UpdateRecordForm(user=profile)\n\tif (response_format == 'rss'):\n\t\treturn ObjectFeed(title=_('My\tActivity'), link=request.path, description=_('Updates\ton\tactivity\tin\tyour\tTree.io'), objects=updates)(request)\n\tcontext = _get_default_context(request)\n\tcontext.update({'form': form, 'updates': updates, 'profile': profile})\n\treturn render_to_response('news\/my_activity', context, context_instance=RequestContext(request), response_format=response_format)\n",["default index page ."]]
["def _parse_sequence(sequence):\n\tif ((not sequence) or (sequence[0] != '<') or (sequence[(-1)] != '>')):\n\t\treturn None\n\twords = sequence[1:(-1)].split('-')\n\tmodifiers = 0\n\twhile (words and (words[0] in _modifier_names)):\n\t\tmodifiers |= (1 << _modifier_names[words[0]])\n\t\tdel words[0]\n\tif (words and (words[0] in _type_names)):\n\t\ttype = _type_names[words[0]]\n\t\tdel words[0]\n\telse:\n\t\treturn None\n\tif (_binder_classes[type] is _SimpleBinder):\n\t\tif (modifiers or words):\n\t\t\treturn None\n\t\telse:\n\t\t\tdetail = None\n\telse:\n\t\tif (type in [_type_names[s] for s in ('KeyPress', 'KeyRelease')]):\n\t\t\ttype_re = _keysym_re\n\t\telse:\n\t\t\ttype_re = _button_re\n\t\tif (not words):\n\t\t\tdetail = None\n\t\telif ((len(words) == 1) and type_re.match(words[0])):\n\t\t\tdetail = words[0]\n\t\telse:\n\t\t\treturn None\n\treturn (modifiers, type, detail)\n",["get a string which should describe an event sequence ."]]
["def _translate_volume_summary_view(context, vol, image_id=None):\n\td = {}\n\td['id'] = vol['id']\n\td['status'] = vol['status']\n\td['size'] = vol['size']\n\td['availability_zone'] = vol['availability_zone']\n\td['created_at'] = vol['created_at']\n\td['attachments'] = []\n\tif (vol['attach_status'] == 'attached'):\n\t\tattachment = _translate_attachment_detail_view(context, vol)\n\t\td['attachments'].append(attachment)\n\td['display_name'] = vol['display_name']\n\td['display_description'] = vol['display_description']\n\tif (vol['volume_type_id'] and vol.get('volume_type')):\n\t\td['volume_type'] = vol['volume_type']['name']\n\telse:\n\t\td['volume_type'] = str(vol['volume_type_id'])\n\td['snapshot_id'] = vol['snapshot_id']\n\td['source_volid'] = vol['source_volid']\n\tif image_id:\n\t\td['image_id'] = image_id\n\tLOG.audit(_('vol=%s'), vol, context=context)\n\tif vol.get('volume_metadata'):\n\t\tmetadata = vol.get('volume_metadata')\n\t\td['metadata'] = dict(((item['key'], item['value']) for item in metadata))\n\telif (vol.get('metadata') and isinstance(vol.get('metadata'), dict)):\n\t\td['metadata'] = vol['metadata']\n\telse:\n\t\td['metadata'] = {}\n\tif vol.get('volume_glance_metadata'):\n\t\td['bootable'] = 'true'\n\telse:\n\t\td['bootable'] = 'false'\n\treturn d\n",["maps keys for volumes summary view ."]]
["def _update_all_uuids_to_ids(t_images, t_image_members, t_image_properties):\n\timages = list(t_images.select().execute())\n\tnew_id = 1\n\tfor image in images:\n\t\told_id = image['id']\n\t\tt_images.update().where((t_images.c.id == old_id)).values(id=str(new_id)).execute()\n\t\tt_image_members.update().where((t_image_members.c.image_id == old_id)).values(image_id=str(new_id)).execute()\n\t\tt_image_properties.update().where((t_image_properties.c.image_id == old_id)).values(image_id=str(new_id)).execute()\n\t\tt_image_properties.update().where(and_(or_((t_image_properties.c.name == 'kernel_id'), (t_image_properties.c.name == 'ramdisk_id')), (t_image_properties.c.value == old_id))).values(value=str(new_id)).execute()\n\t\tnew_id += 1\n",["transition from varchar id to integer id ."]]
["def readPlist(pathOrFile):\n\tdidOpen = False\n\tresult = None\n\tif isinstance(pathOrFile, (bytes, unicode)):\n\t\tpathOrFile = open(pathOrFile, 'rb')\n\t\tdidOpen = True\n\ttry:\n\t\treader = PlistReader(pathOrFile)\n\t\tresult = reader.parse()\n\texcept NotBinaryPlistException as e:\n\t\ttry:\n\t\t\tpathOrFile.seek(0)\n\t\t\tresult = None\n\t\t\tif hasattr(plistlib, 'loads'):\n\t\t\t\tcontents = None\n\t\t\t\tif isinstance(pathOrFile, (bytes, unicode)):\n\t\t\t\t\twith open(pathOrFile, 'rb') as f:\n\t\t\t\t\t\tcontents = f.read()\n\t\t\t\telse:\n\t\t\t\t\tcontents = pathOrFile.read()\n\t\t\t\tresult = plistlib.loads(contents)\n\t\t\telse:\n\t\t\t\tresult = plistlib.readPlist(pathOrFile)\n\t\t\tresult = wrapDataObject(result, for_binary=True)\n\t\texcept Exception as e:\n\t\t\traise InvalidPlistException(e)\n\tfinally:\n\t\tif didOpen:\n\t\t\tpathOrFile.close()\n\treturn result\n",["raises notbinaryplistexception ."]]
["def make_argument_parser():\n\tparser = argparse.ArgumentParser(description='Launch\ta\tprediction\tfrom\ta\tpkl\tfile')\n\tparser.add_argument('model_filename', help='Specifies\tthe\tpkl\tmodel\tfile')\n\tparser.add_argument('test_filename', help='Specifies\tthe\tcsv\tfile\twith\tthe\tvalues\tto\tpredict')\n\tparser.add_argument('output_filename', help='Specifies\tthe\tpredictions\toutput\tfile')\n\tparser.add_argument('--prediction_type', '-P', default='classification', help='Prediction\ttype\t(classification\/regression)')\n\tparser.add_argument('--output_type', '-T', default='int', help='Output\tvariable\ttype\t(int\/float)')\n\tparser.add_argument('--has-headers', '-H', dest='has_headers', action='store_true', help='Indicates\tthe\tfirst\trow\tin\tthe\tinput\tfile\tis\tfeature\tlabels')\n\tparser.add_argument('--has-row-label', '-L', dest='has_row_label', action='store_true', help='Indicates\tthe\tfirst\tcolumn\tin\tthe\tinput\tfile\tis\trow\tlabels')\n\tparser.add_argument('--delimiter', '-D', default=',', help=\"Specifies\tthe\tCSV\tdelimiter\tfor\tthe\ttest\tfile.\tUsual\tvalues\tare\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcomma\t(default)\t','\tsemicolon\t';'\tcolon\t':'\ttabulation\t'\\\\t'\tand\tspace\t'\t'\")\n\treturn parser\n",["creates an argumentparser to read the options for this script from sys ."]]
["def get_models(app_labels):\n\tfrom django.db.models import get_app, get_apps, get_model\n\tfrom django.db.models import get_models as get_all_models\n\tfrom django.contrib.contenttypes.models import ContentType\n\tEXCLUDED_MODELS = (ContentType,)\n\tmodels = []\n\tif (not app_labels):\n\t\tfor app in get_apps():\n\t\t\tmodels += [m for m in get_all_models(app) if (m not in EXCLUDED_MODELS)]\n\tfor app_label in app_labels:\n\t\tif ('.' in app_label):\n\t\t\t(app_label, model_name) = app_label.split('.', 1)\n\t\t\tmodels.append(get_model(app_label, model_name))\n\t\telse:\n\t\t\tmodels += [m for m in get_all_models(get_app(app_label)) if (m not in EXCLUDED_MODELS)]\n\treturn models\n",["gets a list of models for the given app labels ."]]
["def py_encode_basestring_ascii(s, _PY3=PY3):\n\tif _PY3:\n\t\tif isinstance(s, binary_type):\n\t\t\ts = s.decode('utf-8')\n\t\tif (type(s) is not text_type):\n\t\t\ts = text_type(s)\n\telse:\n\t\tif (isinstance(s, str) and (HAS_UTF8.search(s) is not None)):\n\t\t\ts = s.decode('utf-8')\n\t\tif (type(s) not in string_types):\n\t\t\ts = text_type(s)\n\tdef replace(match):\n\t\ts = match.group(0)\n\t\ttry:\n\t\t\treturn ESCAPE_DCT[s]\n\t\texcept KeyError:\n\t\t\tn = ord(s)\n\t\t\tif (n < 65536):\n\t\t\t\treturn ('\\\\u%04x' % (n,))\n\t\t\telse:\n\t\t\t\tn -= 65536\n\t\t\t\ts1 = (55296 | ((n >> 10) & 1023))\n\t\t\t\ts2 = (56320 | (n & 1023))\n\t\t\t\treturn ('\\\\u%04x\\\\u%04x' % (s1, s2))\n\treturn (('\"' + str(ESCAPE_ASCII.sub(replace, s))) + '\"')\n",["return an ascii-only json representation of a python string ."]]
["def _MergeBuiltinsIncludes(appinfo_path, appyaml, open_fn=open):\n\tif (not appyaml.builtins):\n\t\tappyaml.builtins = [appinfo.BuiltinHandler(default='on')]\n\telif (not appinfo.BuiltinHandler.IsDefined(appyaml.builtins, 'default')):\n\t\tappyaml.builtins.append(appinfo.BuiltinHandler(default='on'))\n\t(aggregate_appinclude, include_paths) = _ResolveIncludes(appinfo_path, appinfo.AppInclude(builtins=appyaml.builtins, includes=appyaml.includes), os.path.dirname(appinfo_path), appyaml.runtime, open_fn=open_fn)\n\treturn (appinfo.AppInclude.MergeAppYamlAppInclude(appyaml, aggregate_appinclude), include_paths)\n",["merges app ."]]
["def smart_str(s, encoding='utf-8', strings_only=False, errors='strict'):\n\tif (strings_only and isinstance(s, (types.NoneType, int))):\n\t\treturn s\n\tif (not isinstance(s, basestring)):\n\t\ttry:\n\t\t\treturn str(s)\n\t\texcept UnicodeEncodeError:\n\t\t\tif isinstance(s, Exception):\n\t\t\t\treturn '\t'.join([smart_str(arg, encoding, strings_only, errors) for arg in s])\n\t\t\treturn unicode(s).encode(encoding, errors)\n\telif isinstance(s, unicode):\n\t\treturn s.encode(encoding, errors)\n\telif (s and (encoding != 'utf-8')):\n\t\treturn s.decode('utf-8', errors).encode(encoding, errors)\n\telse:\n\t\treturn s\n",["returns a bytestring version of s ."]]
["def migrate_location_credentials(migrate_engine, to_quoted):\n\tmeta = sqlalchemy.schema.MetaData()\n\tmeta.bind = migrate_engine\n\timages_table = sqlalchemy.Table('images', meta, autoload=True)\n\timages = list(images_table.select(images_table.c.location.startswith('swift')).execute())\n\tfor image in images:\n\t\ttry:\n\t\t\tfixed_uri = legacy_parse_uri(image['location'], to_quoted)\n\t\t\timages_table.update().where((images_table.c.id == image['id'])).values(location=fixed_uri).execute()\n\t\texcept exception.BadStoreUri as e:\n\t\t\treason = encodeutils.exception_to_unicode(e)\n\t\t\tmsg = (_LE('Invalid\tstore\turi\tfor\timage:\t%(image_id)s.\tDetails:\t%(reason)s') % {'image_id': image.id, 'reason': reason})\n\t\t\tLOG.exception(msg)\n\t\t\traise\n",["migrate location credentials for swift uris between the quoted and unquoted forms ."]]
["def get_scene_absolute_numbering(indexer_id, indexer, absolute_number, fallback_to_xem=True):\n\tif ((indexer_id is None) or (absolute_number is None)):\n\t\treturn absolute_number\n\tindexer_id = int(indexer_id)\n\tindexer = int(indexer)\n\tshowObj = findCertainShow(sickrage.srCore.SHOWLIST, indexer_id)\n\tif (showObj and (not showObj.is_scene)):\n\t\treturn absolute_number\n\tresult = find_scene_absolute_numbering(indexer_id, indexer, absolute_number)\n\tif result:\n\t\treturn result\n\telse:\n\t\tif fallback_to_xem:\n\t\t\txem_result = find_xem_absolute_numbering(indexer_id, indexer, absolute_number)\n\t\t\tif xem_result:\n\t\t\t\treturn xem_result\n\t\treturn absolute_number\n",["returns a tuple ."]]
["def oracle_old_passwd(password, username, uppercase=True):\n\t(IV, pad) = (('\\x00' * 8), '\\x00')\n\tif isinstance(username, unicode):\n\t\tusername = unicode.encode(username, UNICODE_ENCODING)\n\tif isinstance(password, unicode):\n\t\tpassword = unicode.encode(password, UNICODE_ENCODING)\n\tunistr = ''.join((('\\x00%s' % c) for c in (username + password).upper()))\n\tcipher = des(hexdecode('0123456789ABCDEF'), CBC, IV, pad)\n\tencrypted = cipher.encrypt(unistr)\n\tcipher = des(encrypted[(-8):], CBC, IV, pad)\n\tencrypted = cipher.encrypt(unistr)\n\tretVal = hexencode(encrypted[(-8):])\n\treturn (retVal.upper() if uppercase else retVal.lower())\n",["reference(s): url ."]]
["def get_format(format_type, lang=None, use_l10n=None):\n\tuse_l10n = (use_l10n or ((use_l10n is None) and settings.USE_L10N))\n\tif (use_l10n and (lang is None)):\n\t\tlang = get_language()\n\tcache_key = (format_type, lang)\n\ttry:\n\t\treturn _format_cache[cache_key]\n\texcept KeyError:\n\t\tpass\n\tval = None\n\tif use_l10n:\n\t\tfor module in get_format_modules(lang):\n\t\t\ttry:\n\t\t\t\tval = getattr(module, format_type)\n\t\t\t\tif (val is not None):\n\t\t\t\t\tbreak\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\tif (val is None):\n\t\tif (format_type not in FORMAT_SETTINGS):\n\t\t\treturn format_type\n\t\tval = getattr(settings, format_type)\n\telif (format_type in ISO_INPUT_FORMATS.keys()):\n\t\tval = list(val)\n\t\tfor iso_input in ISO_INPUT_FORMATS.get(format_type, ()):\n\t\t\tif (iso_input not in val):\n\t\t\t\tval.append(iso_input)\n\t_format_cache[cache_key] = val\n\treturn val\n",["for a specific format type ."]]
["def get_path_dir_files(dirName, nzbName, proc_type):\n\tpath = u''\n\tdirs = []\n\tfiles = []\n\tif (((dirName == sickbeard.TV_DOWNLOAD_DIR) and (not nzbName)) or (proc_type == u'manual')):\n\t\tfor (path, dirs, files) in ek(os.walk, dirName):\n\t\t\tbreak\n\telse:\n\t\t(path, dirs) = ek(os.path.split, dirName)\n\t\tif ((not ((nzbName is None) or nzbName.endswith(u'.nzb'))) and ek(os.path.isfile, ek(os.path.join, dirName, nzbName))):\n\t\t\tdirs = []\n\t\t\tfiles = [ek(os.path.join, dirName, nzbName)]\n\t\telse:\n\t\t\tdirs = [dirs]\n\t\t\tfiles = []\n\treturn (path, dirs, files)\n",["get files in a path ."]]
["def convDown(hidSums, filters, targets, numModulesX, paddingStart, moduleStride, filterSizeX, imSizeX, numImgColors):\n\tnumGroups = 1\n\tnumFilters = filters.shape[0]\n\tnumImages = hidSums.shape[0]\n\tnumModules = (numModulesX ** 2)\n\tassert (paddingStart >= 0)\n\tassert (targets.shape == (numImages, ((numImgColors * imSizeX) * imSizeX)))\n\t_ConvNet.convDown(hidSums.p_mat, filters.p_mat, targets.p_mat, imSizeX, (- paddingStart), moduleStride, numImgColors, numGroups)\n",["hidsums - filters - targets - ."]]
["def publish_from_doctree(document, destination_path=None, writer=None, writer_name='pseudoxml', settings=None, settings_spec=None, settings_overrides=None, config_section=None, enable_exit_status=False):\n\treader = docutils.readers.doctree.Reader(parser_name='null')\n\tpub = Publisher(reader, None, writer, source=io.DocTreeInput(document), destination_class=io.StringOutput, settings=settings)\n\tif ((not writer) and writer_name):\n\t\tpub.set_writer(writer_name)\n\tpub.process_programmatic_settings(settings_spec, settings_overrides, config_section)\n\tpub.set_destination(None, destination_path)\n\treturn pub.publish(enable_exit_status=enable_exit_status)\n",["set up & run a publisher to render from an existing document tree data structure ."]]
["def site_config_dir(appname=None, appauthor=None, version=None, multipath=False):\n\tif (system in ['win32', 'darwin']):\n\t\tpath = site_data_dir(appname, appauthor)\n\t\tif (appname and version):\n\t\t\tpath = os.path.join(path, version)\n\telse:\n\t\tpath = os.getenv('XDG_CONFIG_DIRS', '\/etc\/xdg')\n\t\tpathlist = [os.path.expanduser(x.rstrip(os.sep)) for x in path.split(os.pathsep)]\n\t\tif appname:\n\t\t\tif version:\n\t\t\t\tappname = os.path.join(appname, version)\n\t\t\tpathlist = [os.sep.join([x, appname]) for x in pathlist]\n\t\tif multipath:\n\t\t\tpath = os.pathsep.join(pathlist)\n\t\telse:\n\t\t\tpath = pathlist[0]\n\treturn path\n",["return full path to the user-shared data dir for this application ."]]
["def _get_default_tempdir():\n\tnamer = _RandomNameSequence()\n\tdirlist = _candidate_tempdir_list()\n\tflags = _text_openflags\n\tfor dir in dirlist:\n\t\tif (dir != _os.curdir):\n\t\t\tdir = _os.path.normcase(_os.path.abspath(dir))\n\t\tfor seq in xrange(100):\n\t\t\tname = namer.next()\n\t\t\tfilename = _os.path.join(dir, name)\n\t\t\ttry:\n\t\t\t\tfd = _os.open(filename, flags, 384)\n\t\t\t\ttry:\n\t\t\t\t\ttry:\n\t\t\t\t\t\twith _io.open(fd, 'wb', closefd=False) as fp:\n\t\t\t\t\t\t\tfp.write('blat')\n\t\t\t\t\tfinally:\n\t\t\t\t\t\t_os.close(fd)\n\t\t\t\tfinally:\n\t\t\t\t\t_os.unlink(filename)\n\t\t\t\treturn dir\n\t\t\texcept (OSError, IOError) as e:\n\t\t\t\tif (e.args[0] != _errno.EEXIST):\n\t\t\t\t\tbreak\n\t\t\t\tpass\n\traise IOError, (_errno.ENOENT, ('No\tusable\ttemporary\tdirectory\tfound\tin\t%s' % dirlist))\n",["calculate the default directory to use for temporary files ."]]
["def script_from_examples(s):\n\toutput = []\n\tfor piece in DocTestParser().parse(s):\n\t\tif isinstance(piece, Example):\n\t\t\toutput.append(piece.source[:(-1)])\n\t\t\twant = piece.want\n\t\t\tif want:\n\t\t\t\toutput.append('#\tExpected:')\n\t\t\t\toutput += [('##\t' + l) for l in want.split('\\n')[:(-1)]]\n\t\telse:\n\t\t\toutput += [_comment_line(l) for l in piece.split('\\n')[:(-1)]]\n\twhile (output and (output[(-1)] == '#')):\n\t\toutput.pop()\n\twhile (output and (output[0] == '#')):\n\t\toutput.pop(0)\n\treturn ('\\n'.join(output) + '\\n')\n",["extract script from text with examples ."]]
["def CreateBiddingStrategy(client):\n\tbidding_strategy_service = client.GetService('BiddingStrategyService', version='v201607')\n\tshared_bidding_strategy = {'name': ('Maximize\tClicks\t%s' % uuid.uuid4()), 'biddingScheme': {'xsi_type': 'TargetSpendBiddingScheme', 'bidCeiling': {'microAmount': '2000000'}}}\n\toperation = {'operator': 'ADD', 'operand': shared_bidding_strategy}\n\tresponse = bidding_strategy_service.mutate([operation])\n\tnew_bidding_strategy = response['value'][0]\n\tprint (\"Shared\tbidding\tstrategy\twith\tname\t'%s'\tand\tID\t'%s'\tof\ttype\t'%s'was\tcreated.\" % (new_bidding_strategy['name'], new_bidding_strategy['id'], new_bidding_strategy['biddingScheme']['BiddingScheme.Type']))\n\treturn new_bidding_strategy\n",["creates a bidding strategy object ."]]
["def remove_volume_type_access(context, volume_type_id, project_id):\n\tif (volume_type_id is None):\n\t\tmsg = _('volume_type_id\tcannot\tbe\tNone')\n\t\traise exception.InvalidVolumeType(reason=msg)\n\televated = (context if context.is_admin else context.elevated())\n\tif is_public_volume_type(elevated, volume_type_id):\n\t\tmsg = _('Type\taccess\tmodification\tis\tnot\tapplicable\tto\tpublic\tvolume\ttype.')\n\t\traise exception.InvalidVolumeType(reason=msg)\n\tdb.volume_type_access_remove(elevated, volume_type_id, project_id)\n\tnotify_about_volume_type_access_usage(context, volume_type_id, project_id, 'access.remove')\n",["remove access to volume type for project_id ."]]
["def PBKDF2(password, salt, dkLen=16, count=1000, prf=None):\n\tpassword = tobytes(password)\n\tif (prf is None):\n\t\tprf = (lambda p, s: HMAC.new(p, s, SHA1).digest())\n\tkey = b('')\n\ti = 1\n\twhile (len(key) < dkLen):\n\t\tU = previousU = prf(password, (salt + struct.pack('>I', i)))\n\t\tfor j in xrange((count - 1)):\n\t\t\tpreviousU = t = prf(password, previousU)\n\t\t\tU = strxor(U, t)\n\t\tkey += U\n\t\ti = (i + 1)\n\treturn key[:dkLen]\n",["derive one or more keys from a password ."]]
["def processSVGElementline(elementNode, svgReader):\n\tbegin = euclidean.getComplexDefaultByDictionaryKeys(complex(), elementNode.attributes, 'x1', 'y1')\n\tend = euclidean.getComplexDefaultByDictionaryKeys(complex(), elementNode.attributes, 'x2', 'y2')\n\tloopLayer = svgReader.getLoopLayer()\n\tloopLayer.loops += getTransformedOutlineByPath(elementNode, [begin, end], svgReader.yAxisPointingUpward)\n",["process xmlelement by svgreader ."]]
["def chown(path, user, group):\n\tpath = os.path.expanduser(path)\n\tuid = user_to_uid(user)\n\tgid = group_to_gid(group)\n\terr = ''\n\tif (uid == ''):\n\t\tif user:\n\t\t\terr += 'User\tdoes\tnot\texist\\n'\n\t\telse:\n\t\t\tuid = (-1)\n\tif (gid == ''):\n\t\tif group:\n\t\t\terr += 'Group\tdoes\tnot\texist\\n'\n\t\telse:\n\t\t\tgid = (-1)\n\tif (not os.path.exists(path)):\n\t\ttry:\n\t\t\treturn os.lchown(path, uid, gid)\n\t\texcept OSError:\n\t\t\tpass\n\t\terr += 'File\tnot\tfound'\n\tif err:\n\t\treturn err\n\treturn os.chown(path, uid, gid)\n",["chown a file ."]]
["def _fullyQualifiedName(obj):\n\ttry:\n\t\tname = obj.__qualname__\n\texcept AttributeError:\n\t\tname = obj.__name__\n\tif (inspect.isclass(obj) or inspect.isfunction(obj)):\n\t\tmoduleName = obj.__module__\n\t\treturn ('%s.%s' % (moduleName, name))\n\telif inspect.ismethod(obj):\n\t\ttry:\n\t\t\tcls = obj.im_class\n\t\texcept AttributeError:\n\t\t\treturn ('%s.%s' % (obj.__module__, obj.__qualname__))\n\t\telse:\n\t\t\tclassName = _fullyQualifiedName(cls)\n\t\t\treturn ('%s.%s' % (className, name))\n\treturn name\n",["return the fully qualified name of a module ."]]
["def pytest_generate_tests(metafunc):\n\ttest_files = dict(map(parse_test_files_option, metafunc.config.option.test_files))\n\tif ('case' in metafunc.fixturenames):\n\t\tbase_dir = metafunc.config.option.integration_case_dir\n\t\tthirdparty = metafunc.config.option.thirdparty\n\t\tcases = list(run.collect_dir_tests(base_dir, test_files))\n\t\tif thirdparty:\n\t\t\tcases.extend(run.collect_dir_tests(os.path.join(base_dir, 'thirdparty'), test_files, True))\n\t\tids = [('%s:%s' % (c.module_name, c.line_nr_test)) for c in cases]\n\t\tmetafunc.parametrize('case', cases, ids=ids)\n\tif ('refactor_case' in metafunc.fixturenames):\n\t\tbase_dir = metafunc.config.option.refactor_case_dir\n\t\tmetafunc.parametrize('refactor_case', refactor.collect_dir_tests(base_dir, test_files))\n\tif ('static_analysis_case' in metafunc.fixturenames):\n\t\tbase_dir = os.path.join(os.path.dirname(__file__), 'static_analysis')\n\t\tmetafunc.parametrize('static_analysis_case', collect_static_analysis_tests(base_dir, test_files))\n",["test generator ."]]
["def getVector3ByMultiplierPrefix(elementNode, multiplier, prefix, vector3):\n\tif (multiplier == 0.0):\n\t\treturn vector3\n\toldMultipliedValueVector3 = (vector3 * multiplier)\n\tvector3ByPrefix = getVector3ByPrefix(oldMultipliedValueVector3.copy(), elementNode, prefix)\n\tif (vector3ByPrefix == oldMultipliedValueVector3):\n\t\treturn vector3\n\treturn (vector3ByPrefix \/ multiplier)\n",["get vector3 from multiplier ."]]
["def emit(events, stream=None, Dumper=Dumper, canonical=None, indent=None, width=None, allow_unicode=None, line_break=None):\n\tgetvalue = None\n\tif (stream is None):\n\t\tfrom StringIO import StringIO\n\t\tstream = StringIO()\n\t\tgetvalue = stream.getvalue\n\tdumper = Dumper(stream, canonical=canonical, indent=indent, width=width, allow_unicode=allow_unicode, line_break=line_break)\n\tfor event in events:\n\t\tdumper.emit(event)\n\tif getvalue:\n\t\treturn getvalue()\n",["emit yaml parsing events into a stream ."]]
["def redirect_to_login(next, login_url=None, redirect_field_name=REDIRECT_FIELD_NAME):\n\tif (not login_url):\n\t\tlogin_url = settings.LOGIN_URL\n\treturn HttpResponseRedirect(('%s?%s=%s' % (login_url, urlquote(redirect_field_name), urlquote(next))))\n",["redirects the user to the login page ."]]
["def test_clone_should_rstrip_trailing_slash_in_repo_url(mocker, clone_dir):\n\tmocker.patch('cookiecutter.vcs.is_vcs_installed', autospec=True, return_value=True)\n\tmock_subprocess = mocker.patch('cookiecutter.vcs.subprocess.check_output', autospec=True)\n\tvcs.clone('https:\/\/github.com\/foo\/bar\/', clone_to_dir=clone_dir, no_input=True)\n\tmock_subprocess.assert_called_once_with(['git', 'clone', 'https:\/\/github.com\/foo\/bar'], cwd=clone_dir, stderr=subprocess.STDOUT)\n",["in clone() ."]]
["@register.tag\ndef ssi(parser, token):\n\tbits = token.split_contents()\n\tparsed = False\n\tif (len(bits) not in (2, 3)):\n\t\traise TemplateSyntaxError(u\"'ssi'\ttag\ttakes\tone\targument:\tthe\tpath\tto\tthe\tfile\tto\tbe\tincluded\")\n\tif (len(bits) == 3):\n\t\tif (bits[2] == u'parsed'):\n\t\t\tparsed = True\n\t\telse:\n\t\t\traise TemplateSyntaxError((u\"Second\t(optional)\targument\tto\t%s\ttag\tmust\tbe\t'parsed'\" % bits[0]))\n\tfilepath = parser.compile_filter(bits[1])\n\treturn SsiNode(filepath, parsed)\n",["outputs the contents of a given file into the page ."]]
["def is_valid_info_file(path):\n\tdigest_size = (hashlib.sha1().digestsize * 2)\n\tregexp = (CONF.image_info_filename_pattern % {'image': ('([0-9a-f]{%(digest_size)d}|[0-9a-f]{%(digest_size)d}_sm|[0-9a-f]{%(digest_size)d}_[0-9]+)' % {'digest_size': digest_size})})\n\tm = re.match(regexp, path)\n\tif m:\n\t\treturn True\n\treturn False\n",["test if a given path matches the pattern for info files ."]]
["def wait_for_occupied_port(host, port, timeout=None):\n\tif (not host):\n\t\traise ValueError(\"Host\tvalues\tof\t''\tor\tNone\tare\tnot\tallowed.\")\n\tif (timeout is None):\n\t\ttimeout = occupied_port_timeout\n\tfor trial in range(50):\n\t\ttry:\n\t\t\tcheck_port(host, port, timeout=timeout)\n\t\texcept IOError:\n\t\t\treturn\n\t\telse:\n\t\t\ttime.sleep(timeout)\n\tif (host == client_host(host)):\n\t\traise IOError(('Port\t%r\tnot\tbound\ton\t%r' % (port, host)))\n\tmsg = ('Unable\tto\tverify\tthat\tthe\tserver\tis\tbound\ton\t%r' % port)\n\twarnings.warn(msg)\n",["wait for the specified port to become active ."]]
["def run_all(plugin, args=''):\n\tdata = _execute_cmd(plugin, args, 'cmd.run_all')\n\treturn data\n",["run :py:func:cmd ."]]
["def ParseResponseEx(response, select_default=False, form_parser_class=FormParser, request_class=_request.Request, entitydefs=None, encoding=DEFAULT_ENCODING, _urljoin=urlparse.urljoin, _urlparse=urlparse.urlparse, _urlunparse=urlparse.urlunparse):\n\treturn _ParseFileEx(response, response.geturl(), select_default, False, form_parser_class, request_class, entitydefs, False, encoding, _urljoin=_urljoin, _urlparse=_urlparse, _urlunparse=_urlunparse)\n",["identical to parseresponse ."]]
["def sslwrap_simple(sock, keyfile=None, certfile=None):\n\tif hasattr(sock, '_sock'):\n\t\tsock = sock._sock\n\tssl_sock = _ssl.sslwrap(sock, 0, keyfile, certfile, CERT_NONE, PROTOCOL_SSLv23, None)\n\ttry:\n\t\tsock.getpeername()\n\texcept:\n\t\tpass\n\telse:\n\t\tssl_sock.do_handshake()\n\treturn ssl_sock\n",["a replacement for the old socket ."]]
["def notify_decorator(name, fn):\n\tdef wrapped_func(*args, **kwarg):\n\t\tbody = {}\n\t\tbody['args'] = []\n\t\tbody['kwarg'] = {}\n\t\tfor arg in args:\n\t\t\tbody['args'].append(arg)\n\t\tfor key in kwarg:\n\t\t\tbody['kwarg'][key] = kwarg[key]\n\t\tctxt = (common_context.get_context_from_function_and_args(fn, args, kwarg) or common_context.get_current() or nova.context.RequestContext())\n\t\tnotifier = rpc.get_notifier('api', publisher_id=(CONF.notifications.default_publisher_id or CONF.host))\n\t\tmethod = getattr(notifier, CONF.notifications.default_level.lower(), notifier.info)\n\t\tmethod(ctxt, name, body)\n\t\treturn fn(*args, **kwarg)\n\treturn wrapped_func\n",["decorator for notify which is used from utils ."]]
["def autodelegate(prefix=''):\n\tdef internal(self, arg):\n\t\tfunc = (prefix + arg)\n\t\tif hasattr(self, func):\n\t\t\treturn getattr(self, func)()\n\t\telse:\n\t\t\treturn notfound()\n\treturn internal\n",["returns a method that takes one argument and calls the method named prefix+arg ."]]
["def run_migrations_online():\n\tconnectable = settings.engine\n\twith connectable.connect() as connection:\n\t\tcontext.configure(connection=connection, target_metadata=target_metadata, compare_type=COMPARE_TYPE)\n\t\twith context.begin_transaction():\n\t\t\tcontext.run_migrations()\n",["run migrations in online mode ."]]
["def split_buffer(stream, splitter=None, decoder=(lambda a: a)):\n\tsplitter = (splitter or line_splitter)\n\tbuffered = six.text_type(u'')\n\tfor data in stream_as_text(stream):\n\t\tbuffered += data\n\t\twhile True:\n\t\t\tbuffer_split = splitter(buffered)\n\t\t\tif (buffer_split is None):\n\t\t\t\tbreak\n\t\t\t(item, buffered) = buffer_split\n\t\t\t(yield item)\n\tif buffered:\n\t\ttry:\n\t\t\t(yield decoder(buffered))\n\t\texcept Exception as e:\n\t\t\tlog.error((u'Compose\ttried\tdecoding\tthe\tfollowing\tdata\tchunk,\tbut\tfailed:\\n%s' % repr(buffered)))\n\t\t\traise StreamParseError(e)\n",["given a generator which yields strings and a splitter function ."]]
["@register.tag\ndef templatetag(parser, token):\n\tbits = token.contents.split()\n\tif (len(bits) != 2):\n\t\traise TemplateSyntaxError(u\"'templatetag'\tstatement\ttakes\tone\targument\")\n\ttag = bits[1]\n\tif (tag not in TemplateTagNode.mapping):\n\t\traise TemplateSyntaxError((u\"Invalid\ttemplatetag\targument:\t'%s'.\tMust\tbe\tone\tof:\t%s\" % (tag, list(TemplateTagNode.mapping))))\n\treturn TemplateTagNode(tag)\n",["outputs one of the bits used to compose template tags ."]]
["def git_commit_id(path, ref):\n\tcmd = (git_cmd_base(path) + ['show', ref])\n\ttry:\n\t\toutput = run_subprocess(cmd, stderr=None, universal_newlines=True)[0]\n\texcept CalledProcessError:\n\t\traise NameError((\"Unknown\tgit\treference\t'%s'\" % ref))\n\tcommit = output.split('\\n')[0]\n\tassert (commit[:7] == 'commit\t')\n\treturn commit[7:]\n",["return the commit id of *ref* in the git repository at *path* ."]]
["def connect_to_region(region_name, **kw_params):\n\tfor region in regions():\n\t\tif (region.name == region_name):\n\t\t\treturn region.connect(**kw_params)\n\treturn None\n",["given a valid region name ."]]
["def do_dictsort(value, case_sensitive=False, by='key'):\n\tif (by == 'key'):\n\t\tpos = 0\n\telif (by == 'value'):\n\t\tpos = 1\n\telse:\n\t\traise FilterArgumentError('You\tcan\tonly\tsort\tby\teither\t\"key\"\tor\t\"value\"')\n\tdef sort_func(item):\n\t\tvalue = item[pos]\n\t\tif (isinstance(value, basestring) and (not case_sensitive)):\n\t\t\tvalue = value.lower()\n\t\treturn value\n\treturn sorted(value.items(), key=sort_func)\n",["sort a dict and yield pairs ."]]
["def path_to_url(path):\n\tpath = os.path.normcase(os.path.abspath(path))\n\tif _drive_re.match(path):\n\t\tpath = ((path[0] + '|') + path[2:])\n\turl = urllib.quote(path)\n\turl = url.replace(os.path.sep, '\/')\n\turl = url.lstrip('\/')\n\treturn ('file:\/\/\/' + url)\n",["convert a path to a file: url ."]]
["def new_document(source_path, settings=None):\n\tfrom docutils import frontend\n\tif (settings is None):\n\t\tsettings = frontend.OptionParser().get_default_values()\n\tsource_path = decode_path(source_path)\n\treporter = new_reporter(source_path, settings)\n\tdocument = nodes.document(settings, reporter, source=source_path)\n\tdocument.note_source(source_path, (-1))\n\treturn document\n",["return a new empty document object ."]]
["def ordinal(value):\n\ttry:\n\t\tvalue = int(value)\n\texcept (TypeError, ValueError):\n\t\treturn value\n\tt = ('th', 'st', 'nd', 'rd', 'th', 'th', 'th', 'th', 'th', 'th')\n\tif ((value % 100) in (11, 12, 13)):\n\t\treturn (u'%d%s' % (value, t[0]))\n\treturn (u'%d%s' % (value, t[(value % 10)]))\n",["converts an integer to its ordinal as a string ."]]
["def processElementNodeByDerivation(derivation, elementNode):\n\tif (derivation == None):\n\t\tderivation = SolidDerivation(elementNode)\n\telementAttributesCopy = elementNode.attributes.copy()\n\tfor target in derivation.targets:\n\t\ttargetAttributesCopy = target.attributes.copy()\n\t\ttarget.attributes = elementAttributesCopy\n\t\tprocessTarget(target)\n\t\ttarget.attributes = targetAttributesCopy\n",["process the xml element by derivation ."]]
["def model_format_dict(obj):\n\tif isinstance(obj, (models.Model, models.base.ModelBase)):\n\t\topts = obj._meta\n\telif isinstance(obj, models.query.QuerySet):\n\t\topts = obj.model._meta\n\telse:\n\t\topts = obj\n\treturn {'verbose_name': force_unicode(opts.verbose_name), 'verbose_name_plural': force_unicode(opts.verbose_name_plural)}\n",["return a dict with keys verbose_name and verbose_name_plural ."]]
["def instance_info_cache_update(context, instance_uuid, values):\n\treturn IMPL.instance_info_cache_update(context, instance_uuid, values)\n",["update an instance info cache record in the table ."]]
["def quickstart(root=None, script_name='', config=None):\n\tif config:\n\t\t_global_conf_alias.update(config)\n\ttree.mount(root, script_name, config)\n\tengine.signals.subscribe()\n\tengine.start()\n\tengine.block()\n",["mount the given root ."]]
["def get_random_string(length=12, allowed_chars=u'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'):\n\tif (not using_sysrandom):\n\t\trandom.seed(hashlib.sha256((u'%s%s%s' % (random.getstate(), time.time(), settings.SECRET_KEY)).encode(u'utf-8')).digest())\n\treturn u''.join([random.choice(allowed_chars) for i in range(length)])\n",["returns a securely generated random string ."]]
["def test_rus_fit():\n\trus = RandomUnderSampler(random_state=RND_SEED)\n\trus.fit(X, Y)\n\tassert_equal(rus.min_c_, 0)\n\tassert_equal(rus.maj_c_, 1)\n\tassert_equal(rus.stats_c_[0], 3)\n\tassert_equal(rus.stats_c_[1], 7)\n",["test the fitting method ."]]
["def test_run_link(start_link, count=1000):\n\tif isinstance(start_link, basestring):\n\t\tstart_link = int(start_link, 36)\n\tlinks = Link._byID(range((start_link - count), start_link), data=True, return_dict=False)\n\tuploader = LinkUploader(g.CLOUDSEARCH_DOC_API, things=links)\n\treturn uploader.inject()\n",["inject count number of links ."]]
["def test_cleanup_after_egg_info_exception(script, data):\n\tresult = script.pip('install', '-f', data.find_links, '--no-index', 'brokenegginfo==0.1', expect_error=True)\n\tbuild = (script.venv_path \/ 'build')\n\tassert (not exists(build)), ('build\/\tdir\tstill\texists:\t%s' % result.stdout)\n\tscript.assert_no_temp()\n",["test clean up after a setup ."]]
["def getCraftedTextFromText(gcodeText, exportRepository=None):\n\tif gcodec.isProcedureDoneOrFileIsEmpty(gcodeText, 'export'):\n\t\treturn gcodeText\n\tif (exportRepository == None):\n\t\texportRepository = settings.getReadRepository(ExportRepository())\n\tif (not exportRepository.activateExport.value):\n\t\treturn gcodeText\n\treturn ExportSkein().getCraftedGcode(exportRepository, gcodeText)\n",["preface and convert an svg text ."]]
["def UnregisterPythonExe(exeAlias):\n\ttry:\n\t\twin32api.RegDeleteKey(GetRootKey(), ((GetAppPathsKey() + '\\\\') + exeAlias))\n\texcept win32api.error as exc:\n\t\timport winerror\n\t\tif (exc.winerror != winerror.ERROR_FILE_NOT_FOUND):\n\t\t\traise\n\t\treturn\n",["unregister a ."]]
["def fit_grid_point(X, y, estimator, parameters, train, test, scorer, verbose, error_score='raise', **fit_params):\n\t(score, n_samples_test, _) = _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, error_score)\n\treturn (score, parameters, n_samples_test)\n",["run fit on one set of parameters ."]]
["def str_to_unicode(s, encoding=None):\n\tif (not (type(s) == str)):\n\t\treturn s\n\tif (not encoding):\n\t\tencoding = ENCODING\n\tfor c in [encoding, 'utf-8', 'latin-1']:\n\t\ttry:\n\t\t\treturn s.decode(c)\n\t\texcept UnicodeDecodeError:\n\t\t\tpass\n\treturn s.decode(encoding, 'replace')\n",["attempts to convert a string of unknown character set to a unicode string ."]]
["def get_stable_hash(obj):\n\tif isinstance(obj, dict):\n\t\treturn get_stable_hash(list(obj.items()))\n\telif isinstance(obj, (list, tuple)):\n\t\tobj = sorted((get_stable_hash(o) for o in obj))\n\treturn md5(unicode(obj).encode('utf8')).hexdigest()\n",["return a stable hash for a python data structure ."]]
["def parse(json_string):\n\ttry:\n\t\tjson_data = json.loads(json_string)\n\texcept:\n\t\traise SchemaParseException(('Error\tparsing\tJSON:\t%s' % json_string))\n\tnames = Names()\n\treturn make_avsc_object(json_data, names)\n",["parse a document ."]]
["def startMainLoopFromConstructor(repository):\n\tdisplayedDialogFromConstructor = getDisplayedDialogFromConstructor(repository)\n\tif (displayedDialogFromConstructor == None):\n\t\tprint 'Warning,\tdisplayedDialogFromConstructor\tin\tsettings\tis\tnone,\tso\tthe\twindow\twill\tnot\tbe\tdisplayed.'\n\telse:\n\t\tdisplayedDialogFromConstructor.root.mainloop()\n",["display the repository dialog and start the main loop ."]]
["def test_sample_wrong_X():\n\tada = ADASYN(random_state=RND_SEED)\n\tada.fit(X, Y)\n\tassert_raises(RuntimeError, ada.sample, np.random.random((100, 40)), np.array((([0] * 50) + ([1] * 50))))\n",["test either if an error is raised when x is different at fitting and sampling ."]]
["def cpu_count():\n\tif (sys.platform == 'win32'):\n\t\ttry:\n\t\t\tnum = int(os.environ['NUMBER_OF_PROCESSORS'])\n\t\texcept (ValueError, KeyError):\n\t\t\tnum = 0\n\telif (sys.platform == 'darwin'):\n\t\ttry:\n\t\t\tnum = int(command_output(['\/usr\/sbin\/sysctl', '-n', 'hw.ncpu']))\n\t\texcept (ValueError, OSError, subprocess.CalledProcessError):\n\t\t\tnum = 0\n\telse:\n\t\ttry:\n\t\t\tnum = os.sysconf('SC_NPROCESSORS_ONLN')\n\t\texcept (ValueError, OSError, AttributeError):\n\t\t\tnum = 0\n\tif (num >= 1):\n\t\treturn num\n\telse:\n\t\treturn 1\n",["returns the number of processors on this machine ."]]
["def date(*args, **kwargs):\n\td = None\n\tf = None\n\tif ((len(args) == 0) and (kwargs.get('year') is not None) and kwargs.get('month') and kwargs.get('day')):\n\t\td = Date(**kwargs)\n\telif kwargs.get('week'):\n\t\tf = kwargs.pop('format', None)\n\t\td = Date(*_yyyywwd2yyyymmdd(kwargs.pop('year', ((args and args[0]) or Date.now().year)), kwargs.pop('week'), kwargs.pop('weekday', kwargs.pop('day', 1))), **kwargs)\n\telif ((len(args) == 0) or (args[0] == NOW)):\n\t\td = Date.now()\n\telif ((len(args) == 1) and isinstance(args[0], (Date, datetime))):\n\t\td = Date.fromtimestamp(int(mktime(args[0].timetuple())))\n\t\td += time(microseconds=args[0].microsecond)\n\telif ((len(args) == 1) and (isinstance(args[0], int) or (isinstance(args[0], basestring) and args[0].isdigit()))):\n\t\td = Date.fromtimestamp(int(args[0]))\n\telif ((len(args) == 1) and isinstance(args[0], basestring)):\n\t\ttry:\n\t\t\td = Date.fromtimestamp(mktime_tz(parsedate_tz(args[0])))\n\t\texcept:\n\t\t\tfor format in (((('format' in kwargs) and [kwargs['format']]) or []) + date_formats):\n\t\t\t\ttry:\n\t\t\t\t\td = Date.strptime(args[0], format)\n\t\t\t\t\tbreak\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\tif (d is None):\n\t\t\traise DateError(('unknown\tdate\tformat\tfor\t%s' % repr(args[0])))\n\telif ((len(args) == 2) and isinstance(args[0], basestring)):\n\t\td = Date.strptime(args[0], args[1])\n\telif (len(args) >= 3):\n\t\tf = kwargs.pop('format', None)\n\t\td = Date(*args[:7], **kwargs)\n\telse:\n\t\traise DateError('unknown\tdate\tformat')\n\td.format = (kwargs.get('format') or ((len(args) > 7) and args[7]) or f or Date.format)\n\treturn d\n",["formats a date according to the given format ."]]
["def splittype(url):\n\tglobal _typeprog\n\tif (_typeprog is None):\n\t\t_typeprog = re.compile('([^\/:]+):(.*)', re.DOTALL)\n\tmatch = _typeprog.match(url)\n\tif match:\n\t\t(scheme, data) = match.groups()\n\t\treturn (scheme.lower(), data)\n\treturn (None, url)\n",["splittype --> type ."]]
["def _convert_agg_to_wx_bitmap(agg, bbox):\n\tif (bbox is None):\n\t\treturn wxc.BitmapFromBuffer(int(agg.width), int(agg.height), agg.buffer_rgba())\n\telse:\n\t\treturn _WX28_clipped_agg_as_bitmap(agg, bbox)\n",["convert the region of the agg buffer bounded by bbox to a wx ."]]
["def nomethod(cls):\n\tctx.status = '405\tMethod\tNot\tAllowed'\n\theader('Content-Type', 'text\/html')\n\theader('Allow', ',\t'.join([method for method in ['GET', 'HEAD', 'POST', 'PUT', 'DELETE'] if hasattr(cls, method)]))\n\treturn output('method\tnot\tallowed')\n",["returns a 405 method not allowed error for cls ."]]
["def _match(key, value, attrs):\n\tif (key not in attrs):\n\t\treturn False\n\tif (value == '*'):\n\t\treturn True\n\tif (key != 'objectclass'):\n\t\treturn (value in attrs[key])\n\tvalues = _subs(value)\n\tfor v in values:\n\t\tif (v in attrs[key]):\n\t\t\treturn True\n\treturn False\n",["match a given key and value against an attribute list ."]]
["def delete_mig(mig):\n\tchanged = False\n\treturn_data = []\n\tactions_filter = ['NONE', 'CREATING', 'RECREATING', 'DELETING', 'ABANDONING', 'RESTARTING', 'REFRESHING']\n\tinstance_names = _get_instance_list(mig, filter_list=actions_filter)\n\tif mig.destroy():\n\t\tchanged = True\n\t\treturn_data = instance_names\n\treturn (changed, return_data)\n",["delete a managed instance group ."]]
["def parse_list_header(value):\n\tresult = []\n\tfor item in _parse_list_header(value):\n\t\tif (item[:1] == item[(-1):] == '\"'):\n\t\t\titem = unquote_header_value(item[1:(-1)])\n\t\tresult.append(item)\n\treturn result\n",["parse lists as described by rfc 2068 section 2 ."]]
["def libvlc_video_filter_list_get(p_instance):\n\tf = (_Cfunctions.get('libvlc_video_filter_list_get', None) or _Cfunction('libvlc_video_filter_list_get', ((1,),), None, ctypes.POINTER(ModuleDescription), Instance))\n\treturn f(p_instance)\n",["returns a list of video filters that are available ."]]
["@bdd.given(bdd.parsers.parse('I\tset\t{sect}\t->\t{opt}\tto\t{value}'))\ndef set_setting_given(quteproc, httpbin, sect, opt, value):\n\tif (value == '<empty>'):\n\t\tvalue = ''\n\tvalue = value.replace('(port)', str(httpbin.port))\n\tquteproc.set_setting(sect, opt, value)\n",["set a qutebrowser setting ."]]
["def get_load(jid):\n\tquery = 'SELECT\tload\tFROM\tsalt.jids\tWHERE\tjid\t=\t?;'\n\tret = {}\n\ttry:\n\t\tdata = __salt__['cassandra_cql.cql_query_with_prepare'](query, 'get_load', [jid])\n\t\tif data:\n\t\t\tload = data[0].get('load')\n\t\t\tif load:\n\t\t\t\tret = json.loads(load)\n\texcept CommandExecutionError:\n\t\tlog.critical('Could\tnot\tget\tload\tfrom\tjids\ttable.')\n\t\traise\n\texcept Exception as e:\n\t\tlog.critical('Unexpected\terror\twhile\tgetting\tload\tfrom\tjids:\t{0}'.format(str(e)))\n\t\traise\n\treturn ret\n",["return the load data that marks a specified jid ."]]
["def urldefrag(url):\n\t(url, _coerce_result) = _coerce_args(url)\n\tif ('#' in url):\n\t\t(s, n, p, a, q, frag) = urlparse(url)\n\t\tdefrag = urlunparse((s, n, p, a, q, ''))\n\telse:\n\t\tfrag = ''\n\t\tdefrag = url\n\treturn _coerce_result(DefragResult(defrag, frag))\n",["removes any existing fragment from url ."]]
["def col(loc, strg):\n\treturn ((((loc < len(strg)) and (strg[loc] == '\\n')) and 1) or (loc - strg.rfind('\\n', 0, loc)))\n",["returns current column within a string ."]]
["def cookies(*requireds, **defaults):\n\tif (defaults.get('_unicode') is True):\n\t\tdefaults['_unicode'] = decode_cookie\n\tif ('_parsed_cookies' not in ctx):\n\t\thttp_cookie = ctx.env.get('HTTP_COOKIE', '')\n\t\tctx._parsed_cookies = parse_cookies(http_cookie)\n\ttry:\n\t\treturn storify(ctx._parsed_cookies, *requireds, **defaults)\n\texcept KeyError:\n\t\tbadrequest()\n\t\traise StopIteration()\n",["returns a storage object with all the cookies in it ."]]
["def is_form_media_type(media_type):\n\t(base_media_type, params) = parse_header(media_type.encode(HTTP_HEADER_ENCODING))\n\treturn ((base_media_type == u'application\/x-www-form-urlencoded') or (base_media_type == u'multipart\/form-data'))\n",["return true if the media type is a valid form media type ."]]
["@with_setup(step_runner_environ)\ndef test_successful_behave_as_step_passes():\n\trunnable_step = Step.from_string('Given\tI\thave\ta\tstep\twhich\tcalls\tthe\t\"define\ta\tstep\"\tstep\twith\tbehave_as')\n\trunnable_step.run(True)\n\tassert runnable_step.passed\n",["when a step definition calls another step definition with behave_as ."]]
["def openSerial(port=0, rate=19200, timeout=1):\n\tsnap.openSerial(port=port, rate=rate, tout=timeout)\n",["open serial port for snap reprap communications ."]]
["def require_admin_context(f):\n\tdef wrapper(*args, **kwargs):\n\t\tif (not is_admin_context(args[0])):\n\t\t\traise exception.AdminRequired()\n\t\treturn f(*args, **kwargs)\n\treturn wrapper\n",["decorator to require admin request context ."]]
["def setNonBlocking(fd):\n\tflags = fcntl.fcntl(fd, fcntl.F_GETFL)\n\tflags = (flags | os.O_NONBLOCK)\n\tfcntl.fcntl(fd, fcntl.F_SETFL, flags)\n",["make a file descriptor non-blocking ."]]
["def restart(name, jail=None):\n\tcmd = '{0}\t{1}\tonerestart'.format(_cmd(jail), name)\n\treturn (not __salt__['cmd.retcode'](cmd, python_shell=False))\n",["restart the named service ."]]
["def vector(name=None, dtype=None):\n\tif (dtype is None):\n\t\tdtype = config.floatX\n\ttype = TensorType(dtype, (False,))\n\treturn type(name)\n",["return a symbolic vector variable ."]]
["def _findShebang(filename):\n\twith open(filename, 'rU') as f:\n\t\tif (f.read(2) == '#!'):\n\t\t\texe = f.readline(1024).strip('\\n')\n\t\t\treturn exe\n",["look for a #! line ."]]
["def addLevelName(level, levelName):\n\t_acquireLock()\n\ttry:\n\t\t_levelNames[level] = levelName\n\t\t_levelNames[levelName] = level\n\tfinally:\n\t\t_releaseLock()\n",["associate levelname with level ."]]
["def educateBackticks(text, language='en'):\n\tsmart = smartchars(language)\n\ttext = re.sub('``', smart.opquote, text)\n\ttext = re.sub(\"''\", smart.cpquote, text)\n\treturn text\n",["parameter: string ."]]
["@_get_client\ndef image_member_update(client, memb_id, values):\n\treturn client.image_member_update(memb_id=memb_id, values=values)\n",["update an imagemember object ."]]
["def openSerial(port=0, rate=19200, timeout=1):\n\tsnap.openSerial(port=port, rate=rate, tout=timeout)\n",["open serial port for snap reprap communications ."]]
["def intget(integer, default=None):\n\ttry:\n\t\treturn int(integer)\n\texcept (TypeError, ValueError):\n\t\treturn default\n",["returns integer as an int or default if it cant ."]]
["def is_resource_enabled(resource):\n\tif (sys._getframe().f_back.f_globals.get('__name__') == '__main__'):\n\t\treturn True\n\tresult = ((use_resources is not None) and ((resource in use_resources) or ('*' in use_resources)))\n\tif (not result):\n\t\t_unavail[resource] = None\n\treturn result\n",["test whether a resource is enabled ."]]
["def create_resource():\n\ttask_schema = get_task_schema()\n\tpartial_task_schema = _get_partial_task_schema()\n\tdeserializer = RequestDeserializer(task_schema)\n\tserializer = ResponseSerializer(task_schema, partial_task_schema)\n\tcontroller = TasksController()\n\treturn wsgi.Resource(controller, deserializer, serializer)\n",["images resource factory method ."]]
["def debug(msg, *args, **kwargs):\n\tif (len(root.handlers) == 0):\n\t\tbasicConfig()\n\troot.debug(*((msg,) + args), **kwargs)\n",["simplified interface to start debugging immediately ."]]
["def add_representer(data_type, representer, Dumper=Dumper):\n\tDumper.add_representer(data_type, representer)\n",["add a representer for the given type ."]]
["def makeHTMLTags(tagStr):\n\treturn _makeTags(tagStr, False)\n",["helper to construct opening and closing tag expressions for html ."]]
["def border_crossing():\n\treturn s3_rest_controller(rheader=s3db.transport_rheader)\n",["restful crud controller ."]]
["def chhomephone(name, homephone):\n\treturn _update_gecos(name, 'homephone', homephone)\n",["change the users home phone cli example: ."]]
["def test_rgb_to_hsl_part_1():\n\tpass\n",["test rgb to hsl color function ."]]
["def main():\n\terrors = 0\n\tfits_files = handle_options(sys.argv[1:])\n\tsetup_logging()\n\tfor filename in fits_files:\n\t\terrors += process_file(filename)\n\tif errors:\n\t\tlog.warning('{}\terrors'.format(errors))\n\treturn int(bool(errors))\n",["correct the font specified in the command line ."]]
["def init(mpstate):\n\treturn SerialModule(mpstate)\n",["initialise module ."]]
["def people_type():\n\treturn s3_rest_controller()\n",["rest controller ."]]
["def release():\n\treturn uname()[2]\n",["returns the systems release ."]]
["def processXMLElementByGeometry(geometryOutput, xmlElement):\n\tif (geometryOutput == None):\n\t\treturn\n\tgeometryOutput = evaluate.getVector3ListsRecursively(geometryOutput)\n\tif (('target' in xmlElement.attributeDictionary) and (not evaluate.getEvaluatedBooleanDefault(False, 'copy', xmlElement))):\n\t\ttarget = evaluate.getEvaluatedLinkValue(str(xmlElement.attributeDictionary['target']).strip(), xmlElement)\n\t\tif (target.__class__.__name__ == 'XMLElement'):\n\t\t\ttarget.removeChildrenFromIDNameParent()\n\t\t\txmlElement = target\n\t\t\tif (xmlElement.object != None):\n\t\t\t\tif (xmlElement.parent.object != None):\n\t\t\t\t\tif (xmlElement.object in xmlElement.parent.object.archivableObjects):\n\t\t\t\t\t\txmlElement.parent.object.archivableObjects.remove(xmlElement.object)\n\tfirstElement = None\n\tif (len(geometryOutput) > 0):\n\t\tfirstElement = geometryOutput[0]\n\tif (firstElement.__class__ == list):\n\t\tif (len(firstElement) > 1):\n\t\t\tpath.convertXMLElementRenameByPaths(geometryOutput, xmlElement)\n\t\telse:\n\t\t\tpath.convertXMLElementRename(firstElement, xmlElement)\n\telse:\n\t\tpath.convertXMLElementRename(geometryOutput, xmlElement)\n\tpath.processXMLElement(xmlElement)\n",["process the xml element by geometryoutput ."]]
["@sensitive_post_parameters()\n@csrf_protect\n@never_cache\ndef login(request, template_name='registration\/login.html', redirect_field_name=REDIRECT_FIELD_NAME, authentication_form=AuthenticationForm, current_app=None, extra_context=None):\n\tredirect_to = request.REQUEST.get(redirect_field_name, '')\n\tif (request.method == 'POST'):\n\t\tform = authentication_form(data=request.POST)\n\t\tif form.is_valid():\n\t\t\tif (not is_safe_url(url=redirect_to, host=request.get_host())):\n\t\t\t\tredirect_to = resolve_url(settings.LOGIN_REDIRECT_URL)\n\t\t\tauth_login(request, form.get_user())\n\t\t\tif request.session.test_cookie_worked():\n\t\t\t\trequest.session.delete_test_cookie()\n\t\t\treturn HttpResponseRedirect(redirect_to)\n\telse:\n\t\tform = authentication_form(request)\n\trequest.session.set_test_cookie()\n\tcurrent_site = get_current_site(request)\n\tcontext = {'form': form, redirect_field_name: redirect_to, 'site': current_site, 'site_name': current_site.name}\n\tif (extra_context is not None):\n\t\tcontext.update(extra_context)\n\treturn TemplateResponse(request, template_name, context, current_app=current_app)\n",["displays the login form and handles the login action ."]]
["def _parse_settings_bond_0(opts, iface, bond_def):\n\tbond = {'mode': '0'}\n\tvalid = ['list\tof\tips\t(up\tto\t16)']\n\tif ('arp_ip_target' in opts):\n\t\tif isinstance(opts['arp_ip_target'], list):\n\t\t\tif (1 <= len(opts['arp_ip_target']) <= 16):\n\t\t\t\tbond.update({'arp_ip_target': ''})\n\t\t\t\tfor ip in opts['arp_ip_target']:\n\t\t\t\t\tif (len(bond['arp_ip_target']) > 0):\n\t\t\t\t\t\tbond['arp_ip_target'] = ((bond['arp_ip_target'] + ',') + ip)\n\t\t\t\t\telse:\n\t\t\t\t\t\tbond['arp_ip_target'] = ip\n\t\t\telse:\n\t\t\t\t_raise_error_iface(iface, 'arp_ip_target', valid)\n\t\telse:\n\t\t\t_raise_error_iface(iface, 'arp_ip_target', valid)\n\telse:\n\t\t_raise_error_iface(iface, 'arp_ip_target', valid)\n\tif ('arp_interval' in opts):\n\t\ttry:\n\t\t\tint(opts['arp_interval'])\n\t\t\tbond.update({'arp_interval': opts['arp_interval']})\n\t\texcept Exception:\n\t\t\t_raise_error_iface(iface, 'arp_interval', ['integer'])\n\telse:\n\t\t_log_default_iface(iface, 'arp_interval', bond_def['arp_interval'])\n\t\tbond.update({'arp_interval': bond_def['arp_interval']})\n\treturn bond\n",["filters given options and outputs valid settings for bond0 ."]]
["def show_pricing(kwargs=None, call=None):\n\tprofile = __opts__['profiles'].get(kwargs['profile'], {})\n\tif (not profile):\n\t\treturn {'Error': 'The\trequested\tprofile\twas\tnot\tfound'}\n\tprovider = profile.get('provider', '0:0')\n\tcomps = provider.split(':')\n\tif ((len(comps) < 2) or (comps[1] != 'gce')):\n\t\treturn {'Error': 'The\trequested\tprofile\tdoes\tnot\tbelong\tto\tGCE'}\n\tcomps = profile.get('location', 'us').split('-')\n\tregion = comps[0]\n\tsize = 'CP-COMPUTEENGINE-VMIMAGE-{0}'.format(profile['size'].upper())\n\tpricefile = os.path.join(__opts__['cachedir'], 'gce-pricing.p')\n\tif (not os.path.exists(pricefile)):\n\t\tupdate_pricing()\n\twith salt.utils.fopen(pricefile, 'r') as fho:\n\t\tsizes = msgpack.load(fho)\n\tper_hour = float(sizes['gcp_price_list'][size][region])\n\tweek1_discount = float(sizes['gcp_price_list']['sustained_use_tiers']['0.25'])\n\tweek2_discount = float(sizes['gcp_price_list']['sustained_use_tiers']['0.50'])\n\tweek3_discount = float(sizes['gcp_price_list']['sustained_use_tiers']['0.75'])\n\tweek4_discount = float(sizes['gcp_price_list']['sustained_use_tiers']['1.0'])\n\tweek1 = ((per_hour * (730 \/ 4)) * week1_discount)\n\tweek2 = ((per_hour * (730 \/ 4)) * week2_discount)\n\tweek3 = ((per_hour * (730 \/ 4)) * week3_discount)\n\tweek4 = ((per_hour * (730 \/ 4)) * week4_discount)\n\traw = sizes\n\tret = {}\n\tret['per_hour'] = per_hour\n\tret['per_day'] = (ret['per_hour'] * 24)\n\tret['per_week'] = (ret['per_day'] * 7)\n\tret['per_month'] = (((week1 + week2) + week3) + week4)\n\tret['per_year'] = (ret['per_month'] * 12)\n\tif kwargs.get('raw', False):\n\t\tret['_raw'] = raw\n\treturn {profile['profile']: ret}\n",["show pricing for a particular profile ."]]
["def addRackHoles(derivation, elementNode, vector3RackProfiles):\n\tif (len(derivation.gearHolePaths) > 0):\n\t\tvector3RackProfiles += derivation.gearHolePaths\n\t\treturn\n\tif (derivation.rackHoleRadius <= 0.0):\n\t\treturn\n\taddRackHole(derivation, elementNode, vector3RackProfiles, 0.0)\n\trackHoleMargin = (derivation.rackHoleRadius + derivation.rackHoleRadius)\n\trackHoleSteps = int(math.ceil(((derivation.rackDemilength - rackHoleMargin) \/ derivation.rackHoleStep)))\n\tfor rackHoleIndex in xrange(1, rackHoleSteps):\n\t\tx = (float(rackHoleIndex) * derivation.rackHoleStep)\n\t\taddRackHole(derivation, elementNode, vector3RackProfiles, (- x))\n\t\taddRackHole(derivation, elementNode, vector3RackProfiles, x)\n",["add rack holes to vector3rackprofiles ."]]
["def test_hsl_to_rgb_part_6():\n\tassert (hsl_to_rgb(240, 100, 50) == (0, 0, 255))\n\tassert (hsl_to_rgb(252, 100, 50) == (51, 0, 255))\n\tassert (hsl_to_rgb(264, 100, 50) == (102, 0, 255))\n\tassert (hsl_to_rgb(276, 100, 50) == (153, 0, 255))\n\tassert (hsl_to_rgb(288, 100, 50) == (204, 0, 255))\n\tassert (hsl_to_rgb(300, 100, 50) == (255, 0, 255))\n\tassert (hsl_to_rgb(312, 100, 50) == (255, 0, 204))\n\tassert (hsl_to_rgb(324, 100, 50) == (255, 0, 153))\n\tassert (hsl_to_rgb(336, 100, 50) == (255, 0, 102))\n\tassert (hsl_to_rgb(348, 100, 50) == (255, 0, 51))\n\tassert (hsl_to_rgb(360, 100, 50) == (255, 0, 0))\n",["test hsl to rgb color function ."]]
["def configure_mappers():\n\tif (not Mapper._new_mappers):\n\t\treturn\n\t_CONFIGURE_MUTEX.acquire()\n\ttry:\n\t\tglobal _already_compiling\n\t\tif _already_compiling:\n\t\t\treturn\n\t\t_already_compiling = True\n\t\ttry:\n\t\t\tif (not Mapper._new_mappers):\n\t\t\t\treturn\n\t\t\tMapper.dispatch._for_class(Mapper).before_configured()\n\t\t\tfor mapper in list(_mapper_registry):\n\t\t\t\tif getattr(mapper, '_configure_failed', False):\n\t\t\t\t\te = sa_exc.InvalidRequestError((\"One\tor\tmore\tmappers\tfailed\tto\tinitialize\t-\tcan't\tproceed\twith\tinitialization\tof\tother\tmappers.\tTriggering\tmapper:\t'%s'.\tOriginal\texception\twas:\t%s\" % (mapper, mapper._configure_failed)))\n\t\t\t\t\te._configure_failed = mapper._configure_failed\n\t\t\t\t\traise e\n\t\t\t\tif (not mapper.configured):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tmapper._post_configure_properties()\n\t\t\t\t\t\tmapper._expire_memoizations()\n\t\t\t\t\t\tmapper.dispatch.mapper_configured(mapper, mapper.class_)\n\t\t\t\t\texcept Exception:\n\t\t\t\t\t\texc = sys.exc_info()[1]\n\t\t\t\t\t\tif (not hasattr(exc, '_configure_failed')):\n\t\t\t\t\t\t\tmapper._configure_failed = exc\n\t\t\t\t\t\traise\n\t\t\tMapper._new_mappers = False\n\t\tfinally:\n\t\t\t_already_compiling = False\n\tfinally:\n\t\t_CONFIGURE_MUTEX.release()\n\tMapper.dispatch._for_class(Mapper).after_configured()\n",["initialize the inter-mapper relationships of all mappers that have been constructed thus far ."]]
["def capture_stdio(logger, **kwargs):\n\tsys.excepthook = (lambda *exc_info: logger.critical(_('UNCAUGHT\tEXCEPTION'), exc_info=exc_info))\n\tstdio_files = [sys.stdin, sys.stdout, sys.stderr]\n\tconsole_fds = [h.stream.fileno() for (_junk, h) in getattr(get_logger, 'console_handler4logger', {}).items()]\n\tstdio_files = [f for f in stdio_files if (f.fileno() not in console_fds)]\n\twith open(os.devnull, 'r+b') as nullfile:\n\t\tfor f in stdio_files:\n\t\t\ttry:\n\t\t\t\tf.flush()\n\t\t\texcept IOError:\n\t\t\t\tpass\n\t\t\ttry:\n\t\t\t\tos.dup2(nullfile.fileno(), f.fileno())\n\t\t\texcept OSError:\n\t\t\t\tpass\n\tif kwargs.pop('capture_stdout', True):\n\t\tsys.stdout = LoggerFileObject(logger)\n\tif kwargs.pop('capture_stderr', True):\n\t\tsys.stderr = LoggerFileObject(logger, 'STDERR')\n",["log unhandled exceptions ."]]
["def dirichlet_logpdf_vec(x, alpha):\n\tshape = x.shape\n\tif (len(shape) == 1):\n\t\ttry:\n\t\t\treturn stats.dirichlet.logpdf(x, alpha)\n\t\texcept:\n\t\t\tx[(-1)] = (1.0 - np.sum(x[:(-1)]))\n\t\t\treturn stats.dirichlet.logpdf(x, alpha)\n\telif (len(shape) == 2):\n\t\tsize = shape[0]\n\t\tif (len(alpha.shape) == 1):\n\t\t\treturn np.array([dirichlet_logpdf_vec(x[i, :], alpha) for i in range(size)])\n\t\telse:\n\t\t\treturn np.array([dirichlet_logpdf_vec(x[i, :], alpha[i, :]) for i in range(size)])\n\telif (len(shape) == 3):\n\t\tsize = shape[0]\n\t\treturn np.array([dirichlet_logpdf_vec(x[i, :, :], alpha) for i in range(size)])\n\telse:\n\t\traise NotImplementedError()\n",["vectorized version of stats ."]]
["def normpath(path):\n\tif (path == ''):\n\t\treturn '.'\n\tinitial_slashes = path.startswith('\/')\n\tif (initial_slashes and path.startswith('\/\/') and (not path.startswith('\/\/\/'))):\n\t\tinitial_slashes = 2\n\tcomps = path.split('\/')\n\tnew_comps = []\n\tfor comp in comps:\n\t\tif (comp in ('', '.')):\n\t\t\tcontinue\n\t\tif ((comp != '..') or ((not initial_slashes) and (not new_comps)) or (new_comps and (new_comps[(-1)] == '..'))):\n\t\t\tnew_comps.append(comp)\n\t\telif new_comps:\n\t\t\tnew_comps.pop()\n\tcomps = new_comps\n\tpath = '\/'.join(comps)\n\tif initial_slashes:\n\t\tpath = (('\/' * initial_slashes) + path)\n\treturn (path or '.')\n",["normalize path ."]]
["def arm_and_takeoff(aTargetAltitude):\n\twhile (not vehicle.is_armable):\n\t\tprint '\tWaiting\tfor\tvehicle\tto\tinitialise...'\n\t\ttime.sleep(1)\n\twhile (vehicle.mode.name != 'GUIDED'):\n\t\tvehicle.mode = VehicleMode('GUIDED')\n\t\ttime.sleep(0.1)\n\twhile (not vehicle.armed):\n\t\tvehicle.armed = True\n\t\tprint '\tWaiting\tfor\tarming...'\n\t\ttime.sleep(1)\n\tprint '\tTaking\toff!'\n\tvehicle.simple_takeoff(aTargetAltitude)\n\twhile True:\n\t\trequiredAlt = (aTargetAltitude * 0.95)\n\t\tif (vehicle.location.global_relative_frame.alt >= requiredAlt):\n\t\t\tprint ('\tReached\ttarget\taltitude\tof\t~%f' % aTargetAltitude)\n\t\t\tbreak\n\t\tprint ('\tAltitude:\t%f\t<\t%f' % (vehicle.location.global_relative_frame.alt, requiredAlt))\n\t\ttime.sleep(1)\n",["arms vehicle and fly to atargetaltitude ."]]
["def serve_file(path, content_type=None, disposition=None, name=None, debug=False):\n\tresponse = cherrypy.serving.response\n\tif (not os.path.isabs(path)):\n\t\tmsg = (\"'%s'\tis\tnot\tan\tabsolute\tpath.\" % path)\n\t\tif debug:\n\t\t\tcherrypy.log(msg, 'TOOLS.STATICFILE')\n\t\traise ValueError(msg)\n\ttry:\n\t\tst = os.stat(path)\n\texcept (OSError, TypeError, ValueError):\n\t\tif debug:\n\t\t\tcherrypy.log(('os.stat(%r)\tfailed' % path), 'TOOLS.STATIC')\n\t\traise cherrypy.NotFound()\n\tif stat.S_ISDIR(st.st_mode):\n\t\tif debug:\n\t\t\tcherrypy.log(('%r\tis\ta\tdirectory' % path), 'TOOLS.STATIC')\n\t\traise cherrypy.NotFound()\n\tresponse.headers['Last-Modified'] = httputil.HTTPDate(st.st_mtime)\n\tcptools.validate_since()\n\tif (content_type is None):\n\t\text = ''\n\t\ti = path.rfind('.')\n\t\tif (i != (-1)):\n\t\t\text = path[i:].lower()\n\t\tcontent_type = mimetypes.types_map.get(ext, None)\n\tif (content_type is not None):\n\t\tresponse.headers['Content-Type'] = content_type\n\tif debug:\n\t\tcherrypy.log(('Content-Type:\t%r' % content_type), 'TOOLS.STATIC')\n\tcd = None\n\tif (disposition is not None):\n\t\tif (name is None):\n\t\t\tname = os.path.basename(path)\n\t\tcd = ('%s;\tfilename=\"%s\"' % (disposition, name))\n\t\tresponse.headers['Content-Disposition'] = cd\n\tif debug:\n\t\tcherrypy.log(('Content-Disposition:\t%r' % cd), 'TOOLS.STATIC')\n\tcontent_length = st.st_size\n\tfileobj = open(path, 'rb')\n\treturn _serve_fileobj(fileobj, content_type, content_length, debug=debug)\n",["return a chunk from a file based on the data received ."]]
["@requires_application()\ndef test_reactive_draw():\n\twith TestingCanvas() as c:\n\t\trpolygon = visuals.RegularPolygon(center=[50, 50, 0.0], radius=20, sides=8, color='yellow', parent=c.scene)\n\t\trpolygon.center = [70, 40, 0.0]\n\t\tassert_image_approved(c.render(), 'visuals\/reactive_regular_polygon1.png')\n\t\trpolygon.radius = 25\n\t\tassert_image_approved(c.render(), 'visuals\/reactive_regular_polygon2.png')\n\t\trpolygon.color = 'red'\n\t\tassert_image_approved(c.render(), 'visuals\/reactive_regular_polygon3.png')\n\t\trpolygon.border_color = 'yellow'\n\t\tassert_image_approved(c.render(), 'visuals\/reactive_regular_polygon4.png')\n\t\trpolygon.sides = 6\n\t\tassert_image_approved(c.render(), 'visuals\/reactive_regular_polygon5.png')\n",["test reactive rectpolygon attributes ."]]
["def addLighteningHoles(derivation, gearHolePaths, negatives, pitchRadius, positives):\n\textrudeDerivation = extrude.ExtrudeDerivation()\n\tpositiveVertexes = matrix.getVertexes(positives)\n\tbottomPath = euclidean.getTopPath(positiveVertexes)\n\ttopPath = euclidean.getBottomPath(positiveVertexes)\n\textrudeDerivation.offsetPathDefault = [Vector3(0.0, 0.0, bottomPath), Vector3(0.0, 0.0, topPath)]\n\textrudeDerivation.setToXMLElement(derivation.copyShallow)\n\tvector3LighteningHoles = getLighteningHoles(derivation, gearHolePaths, pitchRadius)\n\textrude.addNegativesPositives(extrudeDerivation, negatives, vector3LighteningHoles, positives)\n",["add lightening holes ."]]
["def check_config_h():\n\tfrom distutils import sysconfig\n\timport string\n\tif (string.find(sys.version, 'GCC') >= 0):\n\t\treturn (CONFIG_H_OK, \"sys.version\tmentions\t'GCC'\")\n\tfn = sysconfig.get_config_h_filename()\n\ttry:\n\t\tf = open(fn)\n\t\ts = f.read()\n\t\tf.close()\n\texcept IOError as exc:\n\t\treturn (CONFIG_H_UNCERTAIN, (\"couldn't\tread\t'%s':\t%s\" % (fn, exc.strerror)))\n\telse:\n\t\tif (string.find(s, '__GNUC__') >= 0):\n\t\t\treturn (CONFIG_H_OK, (\"'%s'\tmentions\t'__GNUC__'\" % fn))\n\t\telse:\n\t\t\treturn (CONFIG_H_NOTOK, (\"'%s'\tdoes\tnot\tmention\t'__GNUC__'\" % fn))\n",["check if the current python installation appears amenable to building extensions with gcc ."]]
["def assert_fingerprint(cert, fingerprint):\n\tfingerprint = fingerprint.replace(':', '').lower()\n\tdigest_length = len(fingerprint)\n\thashfunc = HASHFUNC_MAP.get(digest_length)\n\tif (not hashfunc):\n\t\traise SSLError('Fingerprint\tof\tinvalid\tlength:\t{0}'.format(fingerprint))\n\tfingerprint_bytes = unhexlify(fingerprint.encode())\n\tcert_digest = hashfunc(cert).digest()\n\tif (cert_digest != fingerprint_bytes):\n\t\traise SSLError('Fingerprints\tdid\tnot\tmatch.\tExpected\t\"{0}\",\tgot\t\"{1}\".'.format(fingerprint, hexlify(cert_digest)))\n",["checks if given fingerprint matches the supplied certificate ."]]
["def link(target, link_to):\n\tassert isinstance(target, str)\n\tassert os.path.exists(target)\n\tassert isinstance(link_to, str)\n\tabs_path = os.path.dirname(os.path.abspath(link_to))\n\tif (not os.path.isdir(abs_path)):\n\t\tos.makedirs(abs_path)\n\tchmod(target)\n\tos.symlink(target, link_to)\n",["create a symbolic link from path to dest ."]]
["def _api_queue_delete(output, value, kwargs):\n\tif (value.lower() == 'all'):\n\t\tremoved = NzbQueue.do.remove_all(kwargs.get('search'))\n\t\treturn report(output, keyword='', data={'status': bool(removed), 'nzo_ids': removed})\n\telif value:\n\t\titems = value.split(',')\n\t\tdel_files = int_conv(kwargs.get('del_files'))\n\t\tremoved = NzbQueue.do.remove_multiple(items, del_files)\n\t\treturn report(output, keyword='', data={'status': bool(removed), 'nzo_ids': removed})\n\telse:\n\t\treturn report(output, _MSG_NO_VALUE)\n",["api: accepts output ."]]
["def permission_required(permission_name):\n\tdef test(user):\n\t\treturn user.has_perm(permission_name)\n\treturn user_passes_test(test)\n",["a replacement for django ."]]
["def addsitedir(sys_path, sitedir, known_paths=None):\n\tif (known_paths is None):\n\t\tknown_paths = _init_pathinfo(sys_path)\n\t\treset = 1\n\telse:\n\t\treset = 0\n\t(sitedir, sitedircase) = makepath(sitedir)\n\tif (not (sitedircase in known_paths)):\n\t\tsys_path.append(sitedir)\n\t\tknown_paths.add(sitedircase)\n\ttry:\n\t\tnames = os.listdir(sitedir)\n\texcept OSError:\n\t\treturn\n\tnames = [name for name in names if name.endswith('.pth')]\n\tfor name in sorted(names):\n\t\taddpackage(sys_path, sitedir, name, known_paths)\n\tif reset:\n\t\tknown_paths = None\n\treturn known_paths\n",["add sitedir argument to sys ."]]
["def set_cors_middleware_defaults():\n\tcors.set_defaults(allow_headers=['X-Auth-Token', 'X-Identity-Status', 'X-Roles', 'X-Service-Catalog', 'X-User-Id', 'X-Tenant-Id', 'X-Openstack-Request-Id'], expose_headers=['X-Auth-Token', 'X-Subject-Token', 'X-Service-Token', 'X-Openstack-Request-Id'], allow_methods=['GET', 'PUT', 'POST', 'DELETE', 'PATCH'])\n",["update default configuration options for oslo ."]]
["def fullmodname(path):\n\tcomparepath = os.path.normcase(path)\n\tlongest = ''\n\tfor dir in sys.path:\n\t\tdir = os.path.normcase(dir)\n\t\tif (comparepath.startswith(dir) and (comparepath[len(dir)] == os.sep)):\n\t\t\tif (len(dir) > len(longest)):\n\t\t\t\tlongest = dir\n\tif longest:\n\t\tbase = path[(len(longest) + 1):]\n\telse:\n\t\tbase = path\n\tbase = base.replace(os.sep, '.')\n\tif os.altsep:\n\t\tbase = base.replace(os.altsep, '.')\n\t(filename, ext) = os.path.splitext(base)\n\treturn filename\n",["return a plausible module name for the path ."]]
["def refresh_cache(f):\n\targspec = inspect.getargspec(f)\n\t@functools.wraps(f)\n\tdef wrapper(self, context, *args, **kwargs):\n\t\tres = f(self, context, *args, **kwargs)\n\t\ttry:\n\t\t\tinstance = kwargs.get('instance')\n\t\t\tif (not instance):\n\t\t\t\tinstance = args[(argspec.args.index('instance') - 2)]\n\t\texcept ValueError:\n\t\t\tmsg = _('instance\tis\ta\trequired\targument\tto\tuse\t@refresh_cache')\n\t\t\traise Exception(msg)\n\t\tupdate_instance_cache_with_nw_info(self, context, instance, nw_info=res, conductor_api=kwargs.get('conductor_api'))\n\t\treturn res\n\treturn wrapper\n",["decorator to update the instance_info_cache requires context and instance as function args ."]]
["def merge(file, names, config, coord):\n\tinputs = get_tiles(names, config, coord)\n\toutput = {'type': 'Topology', 'transform': inputs[0]['transform'], 'objects': dict(), 'arcs': list()}\n\tfor (name, input) in zip(names, inputs):\n\t\tfor (index, object) in enumerate(input['objects'].values()):\n\t\t\tif (len(input['objects']) > 1):\n\t\t\t\toutput['objects'][('%(name)s-%(index)d' % locals())] = object\n\t\t\telse:\n\t\t\t\toutput['objects'][name] = object\n\t\t\tfor geometry in object['geometries']:\n\t\t\t\tupdate_arc_indexes(geometry, output['arcs'], input['arcs'])\n\tfile.write(json.dumps(output, separators=(',', ':')).encode('utf8'))\n",["merge multiple sorted inputs into a single sorted output ."]]
["def pluralize(word, pos=NOUN, custom={}, classical=True):\n\tif (word in custom):\n\t\treturn custom[word]\n\tif word.endswith((\"'\", \"'s\")):\n\t\tw = word.rstrip(\"'s\")\n\t\tw = pluralize(w, pos, custom, classical)\n\t\tif w.endswith('s'):\n\t\t\treturn (w + \"'\")\n\t\telse:\n\t\t\treturn (w + \"'s\")\n\tw = word.replace('-', '\t').split('\t')\n\tif (len(w) > 1):\n\t\tif ((w[1] == 'general') or ((w[1] == 'General') and (w[0] not in plural_categories['general-generals']))):\n\t\t\treturn word.replace(w[0], pluralize(w[0], pos, custom, classical))\n\t\telif (w[1] in plural_prepositions):\n\t\t\treturn word.replace(w[0], pluralize(w[0], pos, custom, classical))\n\t\telse:\n\t\t\treturn word.replace(w[(-1)], pluralize(w[(-1)], pos, custom, classical))\n\tn = range(len(plural_rules))\n\tif pos.startswith(ADJECTIVE):\n\t\tn = [0, 1]\n\tfor i in n:\n\t\tfor (suffix, inflection, category, classic) in plural_rules[i]:\n\t\t\tif (category is None):\n\t\t\t\tif ((not classic) or (classic and classical)):\n\t\t\t\t\tif (suffix.search(word) is not None):\n\t\t\t\t\t\treturn suffix.sub(inflection, word)\n\t\t\tif (category is not None):\n\t\t\t\tif ((word in plural_categories[category]) and ((not classic) or (classic and classical))):\n\t\t\t\t\tif (suffix.search(word) is not None):\n\t\t\t\t\t\treturn suffix.sub(inflection, word)\n\treturn word\n",["returns a plural suffix if the value is not 1 ."]]
["def uu_encode(input, errors='strict', filename='<data>', mode=438):\n\tassert (errors == 'strict')\n\tfrom cStringIO import StringIO\n\tfrom binascii import b2a_uu\n\tinfile = StringIO(str(input))\n\toutfile = StringIO()\n\tread = infile.read\n\twrite = outfile.write\n\twrite(('begin\t%o\t%s\\n' % ((mode & 511), filename)))\n\tchunk = read(45)\n\twhile chunk:\n\t\twrite(b2a_uu(chunk))\n\t\tchunk = read(45)\n\twrite('\t\\nend\\n')\n\treturn (outfile.getvalue(), len(input))\n",["encodes the object input and returns a tuple ."]]
["def send_email_for_after_purchase_organizers(email, buyer_email, invoice_id, order_url, event_name, event_organiser):\n\tsend_email(to=email, action=TICKET_PURCHASED_ORGANIZER, subject=MAILS[TICKET_PURCHASED_ORGANIZER]['subject'].format(invoice_id=invoice_id, event_name=event_name, buyer_email=buyer_email), html=MAILS[TICKET_PURCHASED_ORGANIZER]['message'].format(order_url=order_url, buyer_email=buyer_email, event_name=event_name, event_organiser=event_organiser))\n",["send email with order invoice link after purchase ."]]
["def _ToChannelError(error):\n\terror_map = {channel_service_pb.ChannelServiceError.INVALID_CHANNEL_KEY: InvalidChannelClientIdError, channel_service_pb.ChannelServiceError.BAD_MESSAGE: InvalidMessageError}\n\tif (error.application_error in error_map):\n\t\treturn error_map[error.application_error](error.error_detail)\n\telse:\n\t\treturn error\n",["translate an application error to a channel error ."]]
["def main():\n\terrors = 0\n\tfits_files = handle_options(sys.argv[1:])\n\tsetup_logging()\n\tfor filename in fits_files:\n\t\terrors += process_file(filename)\n\tif errors:\n\t\tlog.warning('{}\terrors'.format(errors))\n\treturn int(bool(errors))\n",["implements the main method running this http lb test ."]]
["def get_desktop():\n\tif (('KDE_FULL_SESSION' in os.environ) or ('KDE_MULTIHEAD' in os.environ)):\n\t\treturn 'KDE'\n\telif (('GNOME_DESKTOP_SESSION_ID' in os.environ) or ('GNOME_KEYRING_SOCKET' in os.environ)):\n\t\treturn 'GNOME'\n\telif (sys.platform == 'darwin'):\n\t\treturn 'Mac\tOS\tX'\n\telif hasattr(os, 'startfile'):\n\t\treturn 'Windows'\n\telif _is_xfce():\n\t\treturn 'XFCE'\n\tif _is_x11():\n\t\treturn 'X11'\n\telse:\n\t\treturn None\n",["detect the current desktop environment ."]]
["def text_to_qcolor(text):\n\tcolor = QColor()\n\ttext = str(text)\n\tif (not is_text_string(text)):\n\t\treturn color\n\tif (text.startswith('#') and (len(text) == 7)):\n\t\tcorrect = '#0123456789abcdef'\n\t\tfor char in text:\n\t\t\tif (char.lower() not in correct):\n\t\t\t\treturn color\n\telif (text not in list(QColor.colorNames())):\n\t\treturn color\n\tcolor.setNamedColor(text)\n\treturn color\n",["create a qcolor from specified string avoid warning from qt when an invalid qcolor is instantiated ."]]
["def mail_managers(subject, message, fail_silently=False, connection=None):\n\tif (not settings.MANAGERS):\n\t\treturn\n\tEmailMessage((u'%s%s' % (settings.EMAIL_SUBJECT_PREFIX, subject)), message, settings.SERVER_EMAIL, [a[1] for a in settings.MANAGERS], connection=connection).send(fail_silently=fail_silently)\n",["sends a message to the managers ."]]
["def log_1_plus_exp(mat, target=None, exact=False):\n\tif (not target):\n\t\ttarget = mat\n\tif exact:\n\t\terr_code = _cudamat.apply_log_1_plus_exp_exact(mat.p_mat, target.p_mat)\n\telse:\n\t\terr_code = _cudamat.apply_log_1_plus_exp(mat.p_mat, target.p_mat)\n\tif err_code:\n\t\traise generate_exception(err_code)\n\treturn target\n",["apply log(1+exp(x)) to each element of the matrix mat ."]]
["def find_credentials(host):\n\tuser_names = [__pillar__['proxy'].get('username', 'root')]\n\tpasswords = __pillar__['proxy']['passwords']\n\tfor user in user_names:\n\t\tfor password in passwords:\n\t\t\ttry:\n\t\t\t\tret = __salt__['vsphere.system_info'](host=host, username=user, password=password)\n\t\t\texcept SaltSystemExit:\n\t\t\t\tcontinue\n\t\t\tif ret:\n\t\t\t\tDETAILS['username'] = user\n\t\t\t\tDETAILS['password'] = password\n\t\t\t\treturn (user, password)\n\traise SaltSystemExit('Cannot\tcomplete\tlogin\tdue\tto\tan\tincorrect\tuser\tname\tor\tpassword.')\n",["cycle through all the possible credentials and return the first one that works ."]]
["def get_comment_app():\n\tcomments_app = get_comment_app_name()\n\tif (comments_app not in settings.INSTALLED_APPS):\n\t\traise ImproperlyConfigured(('The\tCOMMENTS_APP\t(%r)\tmust\tbe\tin\tINSTALLED_APPS' % settings.COMMENTS_APP))\n\ttry:\n\t\tpackage = import_module(comments_app)\n\texcept ImportError:\n\t\traise ImproperlyConfigured('The\tCOMMENTS_APP\tsetting\trefers\tto\ta\tnon-existing\tpackage.')\n\treturn package\n",["get the comment app as defined in the settings ."]]
["def copy(src, dst, createpath=0, copydates=1, forcetype=None):\n\tsrc = File.pathname(src)\n\tdst = File.pathname(dst)\n\tif createpath:\n\t\tmkdirs(os.path.split(dst)[0])\n\tifp = open(src, 'rb')\n\tofp = open(dst, 'wb')\n\td = ifp.read(BUFSIZ)\n\twhile d:\n\t\tofp.write(d)\n\t\td = ifp.read(BUFSIZ)\n\tifp.close()\n\tofp.close()\n\tifp = openrf(src, '*rb')\n\tofp = openrf(dst, '*wb')\n\td = ifp.read(BUFSIZ)\n\twhile d:\n\t\tofp.write(d)\n\t\td = ifp.read(BUFSIZ)\n\tifp.close()\n\tofp.close()\n\tsrcfss = File.FSSpec(src)\n\tdstfss = File.FSSpec(dst)\n\tsf = srcfss.FSpGetFInfo()\n\tdf = dstfss.FSpGetFInfo()\n\t(df.Creator, df.Type) = (sf.Creator, sf.Type)\n\tif (forcetype is not None):\n\t\tdf.Type = forcetype\n\tdf.Flags = (sf.Flags & COPY_FLAGS)\n\tdstfss.FSpSetFInfo(df)\n\tif copydates:\n\t\tsrcfsr = File.FSRef(src)\n\t\tdstfsr = File.FSRef(dst)\n\t\t(catinfo, _, _, _) = srcfsr.FSGetCatalogInfo(Files.kFSCatInfoAllDates)\n\t\tdstfsr.FSSetCatalogInfo(Files.kFSCatInfoAllDates, catinfo)\n",["copy a plain file ."]]
["def resolveMUITimeZone(spec):\n\tpattern = re.compile('@(?P<dllname>.*),-(?P<index>\\\\d+)(?:;(?P<comment>.*))?')\n\tmatcher = pattern.match(spec)\n\tassert matcher, 'Could\tnot\tparse\tMUI\tspec'\n\ttry:\n\t\thandle = DLLCache[matcher.groupdict()['dllname']]\n\t\tresult = win32api.LoadString(handle, int(matcher.groupdict()['index']))\n\texcept win32api.error as e:\n\t\tresult = None\n\treturn result\n",["resolve a multilingual user interface resource for the time zone name ."]]
["def get_cli_body_ssh_vrrp(command, response, module):\n\tif ('xml' in response[0]):\n\t\tbody = []\n\telif ('show\trun' in command):\n\t\tbody = response\n\telse:\n\t\ttry:\n\t\t\tresponse = response[0].replace((command + '\\n\\n'), '').strip()\n\t\t\tbody = [json.loads(response)]\n\t\texcept ValueError:\n\t\t\tmodule.fail_json(msg='Command\tdoes\tnot\tsupport\tJSON\toutput', command=command)\n\treturn body\n",["get response for when transport=cli ."]]
["def getGeometryOutput(elementNode):\n\tderivation = HeightmapDerivation(elementNode)\n\theightGrid = derivation.heightGrid\n\tif (derivation.fileName != ''):\n\t\theightGrid = getHeightGrid(archive.getAbsoluteFolderPath(elementNode.getOwnerDocument().fileName, derivation.fileName))\n\treturn getGeometryOutputByHeightGrid(derivation, elementNode, heightGrid)\n",["get vector3 vertexes from attribute dictionary ."]]
["def get_pkg_msg_specs(package):\n\t_init()\n\ttypes = list_msg_types(package, False)\n\tspecs = []\n\tfailures = []\n\tfor t in types:\n\t\ttry:\n\t\t\ttypespec = load_from_file(msg_file(package, t), package)\n\t\t\tspecs.append(typespec)\n\t\texcept Exception as e:\n\t\t\tfailures.append(t)\n\t\t\tprint(('ERROR:\tunable\tto\tload\t%s,\t%s' % (t, e)))\n\treturn (specs, failures)\n",["list all messages that a package contains ."]]
["def format_acl(version=1, **kwargs):\n\tif (version == 1):\n\t\treturn format_acl_v1(groups=kwargs.get('groups'), referrers=kwargs.get('referrers'), header_name=kwargs.get('header_name'))\n\telif (version == 2):\n\t\treturn format_acl_v2(kwargs.get('acl_dict'))\n\traise ValueError(('Invalid\tACL\tversion:\t%r' % version))\n",["compatibility wrapper to help migrate acl syntax from version 1 to 2 ."]]
["def get_makefile_filename():\n\tif python_build:\n\t\treturn os.path.join(project_base, 'Makefile')\n\tlib_dir = get_python_lib(plat_specific=1, standard_lib=1)\n\treturn os.path.join(lib_dir, 'config', 'Makefile')\n",["return the path of the makefile ."]]
["def changequery(**kw):\n\tquery = input(_method='get')\n\tfor (k, v) in kw.iteritems():\n\t\tif (v is None):\n\t\t\tquery.pop(k, None)\n\t\telse:\n\t\t\tquery[k] = v\n\tout = ctx.path\n\tif query:\n\t\tout += ('?' + urllib.urlencode(query))\n\treturn out\n",["imagine youre at \/foo?a=1&b=2 ."]]
["def splitnport(host, defport=(-1)):\n\tglobal _nportprog\n\tif (_nportprog is None):\n\t\t_nportprog = re.compile('^(.*):(.*)$')\n\tmatch = _nportprog.match(host)\n\tif match:\n\t\t(host, port) = match.group(1, 2)\n\t\tif port:\n\t\t\ttry:\n\t\t\t\tnport = int(port)\n\t\t\texcept ValueError:\n\t\t\t\tnport = None\n\t\t\treturn (host, nport)\n\treturn (host, defport)\n",["split host and port ."]]
["def hash_password(password):\n\treturn hash_password_PBKDF2(password)\n",["hash a password ."]]
["def get_operation_data(year, quarter):\n\tif (ct._check_input(year, quarter) is True):\n\t\tct._write_head()\n\t\tdata = _get_operation_data(year, quarter, 1, pd.DataFrame())\n\t\tif (data is not None):\n\t\t\tdata['code'] = data['code'].map((lambda x: str(x).zfill(6)))\n\t\treturn data\n",["parameters year:int \u5e74\u5ea6 e ."]]
["def login_required(handler_method):\n\tdef check_login(self, *args, **kwargs):\n\t\tif (self.request.method != 'GET'):\n\t\t\tself.abort(400, detail='The\tlogin_required\tdecorator\tcan\tonly\tbe\tused\tfor\tGET\trequests.')\n\t\tuser = users.get_current_user()\n\t\tif (not user):\n\t\t\treturn self.redirect(users.create_login_url(self.request.url))\n\t\telse:\n\t\t\thandler_method(self, *args, **kwargs)\n\treturn check_login\n",["decorator for views that checks that the user is logged in ."]]
["def saveNZB(nzbName, nzbString):\n\ttry:\n\t\twith io.open((nzbName + u'.nzb'), u'w') as nzb_fh:\n\t\t\tnzb_fh.write(nzbString)\n\texcept EnvironmentError as e:\n\t\tsickrage.srCore.srLogger.error(u'Unable\tto\tsave\tNZB:\t{}'.format(e.message))\n",["save nzb to disk ."]]
["def _KindKeyToString(key):\n\tkey_path = key.to_path()\n\tif ((len(key_path) == 2) and (key_path[0] == '__kind__') and isinstance(key_path[1], basestring)):\n\t\treturn key_path[1]\n\traise BadRequestError('invalid\tKey\tfor\t__kind__\ttable')\n",["extract kind name from __kind__ key ."]]
["def trycmd(*args, **kwargs):\n\tdiscard_warnings = kwargs.pop('discard_warnings', False)\n\ttry:\n\t\t(out, err) = execute(*args, **kwargs)\n\t\tfailed = False\n\texcept exception.ProcessExecutionError as exn:\n\t\t(out, err) = ('', str(exn))\n\t\tLOG.debug(err)\n\t\tfailed = True\n\tif ((not failed) and discard_warnings and err):\n\t\tLOG.debug(err)\n\t\terr = ''\n\treturn (out, err)\n",["a wrapper around execute() to more easily handle warnings and errors ."]]
["def action_peek_json(body):\n\ttry:\n\t\tdecoded = jsonutils.loads(body)\n\texcept ValueError:\n\t\tmsg = _('cannot\tunderstand\tJSON')\n\t\traise exception.MalformedRequestBody(reason=msg)\n\tif (len(decoded) != 1):\n\t\tmsg = _('too\tmany\tbody\tkeys')\n\t\traise exception.MalformedRequestBody(reason=msg)\n\treturn list(decoded.keys())[0]\n",["determine action to invoke ."]]
["def get_gem_classified():\n\tdf = fd.get_stock_basics()\n\tdf.reset_index(level=0, inplace=True)\n\tdf = df[ct.FOR_CLASSIFY_B_COLS]\n\tdf = df.ix[(df.code.str[0] == '3')]\n\tdf = df.sort('code').reset_index(drop=True)\n\treturn df\n",["return dataframe code :\u80a1\u7968\u4ee3\u7801 name :\u80a1\u7968\u540d\u79f0 ."]]
["def beneficiary():\n\ts3db.configure('project_beneficiary', deletable=False, editable=False, insertable=False)\n\tlist_btn = A(T('Beneficiary\tReport'), _href=URL(c='project', f='beneficiary', args='report', vars=get_vars), _class='action-btn')\n\treturn s3_rest_controller(hide_filter=False)\n",["restful crud controller ."]]
["def get_env_var(key):\n\tif (not hasattr(os, 'environ')):\n\t\traise Exception('os.environ\tnot\timplemented!')\n\tl = [os.environ[x] for x in os.environ.keys() if (x.lower() == key.lower())]\n\tif (len(l) > 0):\n\t\treturn l[0]\n\telse:\n\t\treturn None\n",["returns the environment variable denoted by key ."]]
["@require_context\n@pick_context_manager_reader\ndef flavor_get_by_name(context, name):\n\tresult = _flavor_get_query(context).filter_by(name=name).first()\n\tif (not result):\n\t\traise exception.FlavorNotFoundByName(flavor_name=name)\n\treturn _dict_with_extra_specs(result)\n",["returns a dict describing specific flavor ."]]
["def _update_query_params(uri, params):\n\tparts = urllib.parse.urlparse(uri)\n\tquery_params = dict(urllib.parse.parse_qsl(parts.query))\n\tquery_params.update(params)\n\tnew_parts = parts._replace(query=urllib.parse.urlencode(query_params))\n\treturn urllib.parse.urlunparse(new_parts)\n",["updates a uri with new query parameters ."]]
["@register.filter(is_safe=True)\n@stringfilter\ndef slugify(value):\n\treturn _slugify(value)\n",["normalizes string ."]]
["def hooks_namespace(k, v):\n\thookpoint = k.split('.', 1)[0]\n\tif isinstance(v, text_or_bytes):\n\t\tv = cherrypy.lib.attributes(v)\n\tif (not isinstance(v, Hook)):\n\t\tv = Hook(v)\n\tcherrypy.serving.request.hooks[hookpoint].append(v)\n",["attach bare hooks declared in config ."]]
["def get_all(context, namespace_name, session, filters=None, marker=None, limit=None, sort_key='created_at', sort_dir='desc'):\n\tnamespace = namespace_api.get(context, namespace_name, session)\n\tquery = session.query(models.MetadefTag).filter_by(namespace_id=namespace['id'])\n\tmarker_tag = None\n\tif (marker is not None):\n\t\tmarker_tag = _get(context, marker, session)\n\tsort_keys = ['created_at', 'id']\n\t(sort_keys.insert(0, sort_key) if (sort_key not in sort_keys) else sort_keys)\n\tquery = paginate_query(query=query, model=models.MetadefTag, limit=limit, sort_keys=sort_keys, marker=marker_tag, sort_dir=sort_dir)\n\tmetadef_tag = query.all()\n\tmetadef_tag_list = []\n\tfor tag in metadef_tag:\n\t\tmetadef_tag_list.append(tag.to_dict())\n\treturn metadef_tag_list\n",["return a list of all available services cli example: ."]]
["def getFloatFromCharacterSplitLine(character, splitLine):\n\tlineFromCharacter = gcodec.getStringFromCharacterSplitLine(character, splitLine)\n\tif (lineFromCharacter == None):\n\t\treturn None\n\treturn float(lineFromCharacter)\n",["get the float after the first occurence of the character in the split line ."]]
["def writeOutput(fileName=''):\n\tfileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName)\n\tif (fileName != ''):\n\t\tskeinforge_craft.writeChainTextWithNounMessage(fileName, 'unpause')\n",["chamber a gcode linear move file ."]]
["def install(config, plugins):\n\ttry:\n\t\t(installer, _) = plug_sel.choose_configurator_plugins(config, plugins, 'install')\n\texcept errors.PluginSelectionError as e:\n\t\treturn e.message\n\t(domains, _) = _find_domains_or_certname(config, installer)\n\tle_client = _init_le_client(config, authenticator=None, installer=installer)\n\tassert (config.cert_path is not None)\n\tle_client.deploy_certificate(domains, config.key_path, config.cert_path, config.chain_path, config.fullchain_path)\n\tle_client.enhance_config(domains, config.chain_path)\n",["install this package as the default twisted reactor ."]]
["def _rm_mods(pre_mods, post_mods):\n\tpre = set()\n\tpost = set()\n\tfor mod in pre_mods:\n\t\tpre.add(mod['module'])\n\tfor mod in post_mods:\n\t\tpost.add(mod['module'])\n\treturn (pre - post)\n",["return a list of the new modules ."]]
["def pytest_generate_tests(metafunc):\n\ttest_files = dict(map(parse_test_files_option, metafunc.config.option.test_files))\n\tif ('case' in metafunc.fixturenames):\n\t\tbase_dir = metafunc.config.option.integration_case_dir\n\t\tthirdparty = metafunc.config.option.thirdparty\n\t\tcases = list(run.collect_dir_tests(base_dir, test_files))\n\t\tif thirdparty:\n\t\t\tcases.extend(run.collect_dir_tests(os.path.join(base_dir, 'thirdparty'), test_files, True))\n\t\tids = [('%s:%s' % (c.module_name, c.line_nr_test)) for c in cases]\n\t\tmetafunc.parametrize('case', cases, ids=ids)\n\tif ('refactor_case' in metafunc.fixturenames):\n\t\tbase_dir = metafunc.config.option.refactor_case_dir\n\t\tmetafunc.parametrize('refactor_case', refactor.collect_dir_tests(base_dir, test_files))\n\tif ('static_analysis_case' in metafunc.fixturenames):\n\t\tbase_dir = os.path.join(os.path.dirname(__file__), 'static_analysis')\n\t\tmetafunc.parametrize('static_analysis_case', collect_static_analysis_tests(base_dir, test_files))\n",["build a list of test arguments ."]]
["def firstof(parser, token):\n\tbits = token.split_contents()[1:]\n\tif (len(bits) < 1):\n\t\traise TemplateSyntaxError(\"'firstof'\tstatement\trequires\tat\tleast\tone\targument\")\n\treturn FirstOfNode([parser.compile_filter(bit) for bit in bits])\n",["outputs the first variable passed that is not false ."]]
["def getTricomplexskewX(transformWords):\n\tskewX = math.tan(math.radians(float(transformWords[0])))\n\treturn [complex(1.0, 0.0), complex(skewX, 1.0), complex()]\n",["get matrixsvg by transformwords ."]]
["def is_present(name):\n\toutput = Popen(['locale', '-a'], stdout=PIPE).communicate()[0]\n\treturn any(((fix_case(name) == fix_case(line)) for line in output.splitlines()))\n",["checks if the given locale is currently installed ."]]
["def competency_rating():\n\tmode = session.s3.hrm.mode\n\tdef prep(r):\n\t\tif (mode is not None):\n\t\t\tauth.permission.fail()\n\t\treturn True\n\ts3.prep = prep\n\toutput = s3_rest_controller()\n\treturn output\n",["competency rating for skill types controller ."]]
["def families(root=None):\n\tif (not root):\n\t\troot = Tkinter._default_root\n\treturn root.tk.splitlist(root.tk.call('font', 'families'))\n",["get font families ."]]
["def _getpass(prompt):\n\timport getpass\n\ttry:\n\t\treturn getpass.getpass(prompt)\n\texcept IOError as e:\n\t\tif (e.errno == errno.EINTR):\n\t\t\traise KeyboardInterrupt\n\t\traise\n\texcept EOFError:\n\t\traise KeyboardInterrupt\n",["helper to turn ioerrors into keyboardinterrupts ."]]
["def W3CDTF_to_datetime(formatted_string):\n\tmatch = re.match(RE_W3CDTF, formatted_string)\n\tdigits = list(map(int, match.groups()[:6]))\n\treturn datetime.datetime(*digits)\n",["convert from a timestamp string to a datetime object ."]]
["def _get_target_port(iscsi_string):\n\tif (iscsi_string and (':' in iscsi_string)):\n\t\treturn iscsi_string.split(':')[1]\n\treturn CONF.xenserver.target_port\n",["retrieve target port ."]]
["def get_ha1_dict(user_ha1_dict):\n\tdef get_ha1(realm, username):\n\t\treturn user_ha1_dict.get(user)\n\treturn get_ha1\n",["returns a get_ha1 function which obtains a ha1 password hash from a dictionary of the form: {username : ha1} ."]]
["def getPackedGeometryOutputByLoop(elementNode, sideLoop):\n\tsideLoop.rotate(elementNode)\n\treturn getGeometryOutputByManipulation(elementNode, sideLoop)\n",["get packed geometry output by side loop ."]]
["def matrix(name=None, dtype=None):\n\tif (dtype is None):\n\t\tdtype = config.floatX\n\ttype = TensorType(dtype, (False, False))\n\treturn type(name)\n",["return a symbolic matrix variable ."]]
["def getManipulatedGeometryOutput(elementNode, geometryOutput, prefix):\n\tflippedGeometryOutput = triangle_mesh.getGeometryOutputCopy(geometryOutput)\n\tflip.flipPoints(elementNode, matrix.getVertexes(flippedGeometryOutput), prefix)\n\tif flip.getShouldReverse(elementNode, prefix):\n\t\tflippedFaces = face.getFaces(flippedGeometryOutput)\n\t\tfor flippedFace in flippedFaces:\n\t\t\tflippedFace.vertexIndexes.reverse()\n\treturn {'union': {'shapes': [flippedGeometryOutput, geometryOutput]}}\n",["get equated geometryoutput ."]]
["def random_distribution(min=(-5.0), max=5.0, total_items=50):\n\tnum_items = random.randrange(5, total_items)\n\tall_info = []\n\tfor item in range(num_items):\n\t\tnew_item = random.uniform(min, max)\n\t\tall_info.append(new_item)\n\treturn all_info\n",["generate a random column of probabilities ."]]
["def get_organization(organization_id):\n\tif (not organizations_enabled()):\n\t\treturn []\n\tfrom organizations import api as organizations_api\n\treturn organizations_api.get_organization(organization_id)\n",["client api operation adapter\/wrapper ."]]
["def error(status, message):\n\theaders = {'Content-Type': 'text\/plain'}\n\tcurrent.log.error(message)\n\traise HTTP(status, body=message, web2py_error=message, **headers)\n",["if name is none then return empty dict otherwise raise an exception with __name__ from name ."]]
["def addMenuEntitiesToMenu(menu, menuEntities):\n\tfor menuEntity in menuEntities:\n\t\tmenuEntity.addToMenu(menu)\n",["add the menu entities to the menu ."]]
["def libvlc_errmsg():\n\tf = (_Cfunctions.get('libvlc_errmsg', None) or _Cfunction('libvlc_errmsg', (), None, ctypes.c_char_p))\n\treturn f()\n",["a human-readable error message for the last libvlc error in the calling thread ."]]
["def raise_msg_to_str(msg):\n\tif (not is_string_like(msg)):\n\t\tmsg = '\\n'.join(map(str, msg))\n\treturn msg\n",["msg is a return arg from a raise ."]]
["def test_ada_init():\n\tratio = 'auto'\n\tada = ADASYN(ratio=ratio, random_state=RND_SEED)\n\tassert_equal(ada.random_state, RND_SEED)\n",["test the initialisation of the object ."]]
["def _organize_states_for_post_update(base_mapper, states, uowtransaction):\n\treturn list(_connections_for_states(base_mapper, uowtransaction, states))\n",["make an initial pass across a set of states for update corresponding to post_update ."]]
["def scopes_to_string(scopes):\n\tif isinstance(scopes, six.string_types):\n\t\treturn scopes\n\telse:\n\t\treturn '\t'.join(scopes)\n",["converts scope value to a string ."]]
["@require_admin_context\ndef volume_get_all_by_host(context, host, filters=None):\n\tif (host and isinstance(host, six.string_types)):\n\t\tsession = get_session()\n\t\twith session.begin():\n\t\t\thost_attr = getattr(models.Volume, 'host')\n\t\t\tconditions = [(host_attr == host), host_attr.op('LIKE')((host + '#%'))]\n\t\t\tquery = _volume_get_query(context).filter(or_(*conditions))\n\t\t\tif filters:\n\t\t\t\tquery = _process_volume_filters(query, filters)\n\t\t\t\tif (query is None):\n\t\t\t\t\treturn []\n\t\t\treturn query.all()\n\telif (not host):\n\t\treturn []\n",["get all volumes belonging to a host ."]]
["def strip_python_stderr(stderr):\n\tstderr = re.sub('\\\\[\\\\d+\trefs\\\\]\\\\r?\\\\n?$'.encode(), ''.encode(), stderr).strip()\n\treturn stderr\n",["strip the stderr of a python process from potential debug output emitted by the interpreter ."]]
["def wrap(prefix, text, cols):\n\tpad = ('\t' * len(prefix.expandtabs()))\n\tavailable = (cols - len(pad))\n\tseq = text.split('\t')\n\tNseq = len(seq)\n\tind = 0\n\tlines = []\n\twhile (ind < Nseq):\n\t\tlastInd = ind\n\t\tind += get_split_ind(seq[ind:], available)\n\t\tlines.append(seq[lastInd:ind])\n\tret = ((prefix + '\t'.join(lines[0])) + '\\n')\n\tfor line in lines[1:]:\n\t\tret += ((pad + '\t'.join(line)) + '\\n')\n\treturn ret\n",["wrap a single paragraph of text ."]]
["def stop(name):\n\tcmd = '\/etc\/rc.d\/{0}\t-f\tstop'.format(name)\n\treturn (not __salt__['cmd.retcode'](cmd))\n",["stop the specified service cli example: ."]]
["def pretty_name(name):\n\tif (not name):\n\t\treturn ''\n\treturn name.replace('_', '\t').capitalize()\n",["converts first_name to first name ."]]
["def force_escape(value):\n\tfrom django.utils.html import escape\n\treturn mark_safe(escape(value))\n",["escapes a strings html ."]]
["def onGlobalBases(key, value):\n\tDEBUG_MSG(('onGlobalBases:\t%s' % key))\n",["kbengine method ."]]
["def is_hop_by_hop(header_name):\n\treturn (header_name.lower() in _hop_headers)\n",["return true if header_name is an http\/1 ."]]
["def getNewDerivation(elementNode):\n\treturn evaluate.EmptyObject(elementNode)\n",["get new derivation ."]]
["def _delete_users(users):\n\treturn __salt__['users.delete_users'](users, commit=False)\n",["calls users ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["check to make sure requests and xml are installed and requests ."]]
["def log(repo='.', paths=None, outstream=sys.stdout, max_entries=None, reverse=False, name_status=False):\n\twith open_repo_closing(repo) as r:\n\t\twalker = r.get_walker(max_entries=max_entries, paths=paths, reverse=reverse)\n\t\tfor entry in walker:\n\t\t\tdecode = (lambda x: commit_decode(entry.commit, x))\n\t\t\tprint_commit(entry.commit, decode, outstream)\n\t\t\tif name_status:\n\t\t\t\toutstream.writelines([(l + '\\n') for l in print_name_status(entry.changes())])\n",["logs a message at the given runlevel ."]]
["def is_server_error(status):\n\treturn ((500 <= status) and (status <= 599))\n",["check if http status code is server error ."]]
["def _p(pp, name):\n\treturn ('%s_%s' % (pp, name))\n",["make prefix-appended name ."]]
["def iteritems_compat(d):\n\treturn iter(getattr(d, _iteritems)())\n",["return an iterator over the pairs of a dictionary ."]]
["def CanonicalPathToLocalPath(path):\n\tpath = path.replace('\/\\\\', '\\\\')\n\tpath = path.replace('\/', '\\\\')\n\tm = re.match('\\\\\\\\([a-zA-Z]):(.*)$', path)\n\tif m:\n\t\tpath = ('%s:\\\\%s' % (m.group(1), m.group(2).lstrip('\\\\')))\n\treturn path\n",["osx uses a normal path ."]]
["def get_gentoo_mirrors():\n\treturn get_var('GENTOO_MIRRORS')\n",["get the value of gentoo_mirrors variable in the make ."]]
["def getNewRepository():\n\treturn ExportRepository()\n",["get new repository ."]]
["def getNewRepository():\n\treturn ExportRepository()\n",["get new repository ."]]
["def kit_item():\n\ts3.prep = (lambda r: (r.representation == 's3json'))\n\treturn s3_rest_controller()\n",["restful crud controller ."]]
["def init(mpstate):\n\treturn SerialModule(mpstate)\n",["initialise module ."]]
["def getmacbyip6(ip6, chainCC=0):\n\tif in6_ismaddr(ip6):\n\t\tmac = in6_getnsmac(inet_pton(socket.AF_INET6, ip6))\n\t\treturn mac\n\t(iff, a, nh) = conf.route6.route(ip6, dev=conf.iface6)\n\tif (iff == LOOPBACK_NAME):\n\t\treturn 'ff:ff:ff:ff:ff:ff'\n\tif (nh != '::'):\n\t\tip6 = nh\n\tmac = conf.netcache.in6_neighbor.get(ip6)\n\tif mac:\n\t\treturn mac\n\tres = neighsol(ip6, a, iff, chainCC=chainCC)\n\tif (res is not None):\n\t\tmac = res.src\n\t\tconf.netcache.in6_neighbor[ip6] = mac\n\t\treturn mac\n\treturn None\n",["returns the mac address to be used for provided ip6 peer ."]]
["def resnet_v2_50(inputs, num_classes=None, is_training=True, global_pool=True, output_stride=None, reuse=None, scope='resnet_v2_50'):\n\tblocks = [resnet_utils.Block('block1', bottleneck, (([(256, 64, 1)] * 2) + [(256, 64, 2)])), resnet_utils.Block('block2', bottleneck, (([(512, 128, 1)] * 3) + [(512, 128, 2)])), resnet_utils.Block('block3', bottleneck, (([(1024, 256, 1)] * 5) + [(1024, 256, 2)])), resnet_utils.Block('block4', bottleneck, ([(2048, 512, 1)] * 3))]\n\treturn resnet_v2(inputs, blocks, num_classes, is_training=is_training, global_pool=global_pool, output_stride=output_stride, include_root_block=True, reuse=reuse, scope=scope)\n",["resnet-50 model of [1] ."]]
["def membership():\n\tredirect(URL(c='admin', args=request.args, vars=request.vars))\n",["rest controller ."]]
["def create_login_url(dest_url=None, _auth_domain=None, federated_identity=None):\n\treq = user_service_pb.CreateLoginURLRequest()\n\tresp = user_service_pb.CreateLoginURLResponse()\n\treq.set_destination_url(dest_url)\n\tif _auth_domain:\n\t\treq.set_auth_domain(_auth_domain)\n\tif federated_identity:\n\t\treq.set_federated_identity(federated_identity)\n\ttry:\n\t\tapiproxy_stub_map.MakeSyncCall('user', 'CreateLoginURL', req, resp)\n\texcept apiproxy_errors.ApplicationError as e:\n\t\tif (e.application_error == user_service_pb.UserServiceError.REDIRECT_URL_TOO_LONG):\n\t\t\traise RedirectTooLongError\n\t\telif (e.application_error == user_service_pb.UserServiceError.NOT_ALLOWED):\n\t\t\traise NotAllowedError\n\t\telse:\n\t\t\traise e\n\treturn resp.login_url()\n",["computes the login url for redirection ."]]
["def serve_file(path, content_type=None, disposition=None, name=None, debug=False):\n\tresponse = cherrypy.serving.response\n\tif (not os.path.isabs(path)):\n\t\tmsg = (\"'%s'\tis\tnot\tan\tabsolute\tpath.\" % path)\n\t\tif debug:\n\t\t\tcherrypy.log(msg, 'TOOLS.STATICFILE')\n\t\traise ValueError(msg)\n\ttry:\n\t\tst = os.stat(path)\n\texcept (OSError, TypeError, ValueError):\n\t\tif debug:\n\t\t\tcherrypy.log(('os.stat(%r)\tfailed' % path), 'TOOLS.STATIC')\n\t\traise cherrypy.NotFound()\n\tif stat.S_ISDIR(st.st_mode):\n\t\tif debug:\n\t\t\tcherrypy.log(('%r\tis\ta\tdirectory' % path), 'TOOLS.STATIC')\n\t\traise cherrypy.NotFound()\n\tresponse.headers['Last-Modified'] = httputil.HTTPDate(st.st_mtime)\n\tcptools.validate_since()\n\tif (content_type is None):\n\t\text = ''\n\t\ti = path.rfind('.')\n\t\tif (i != (-1)):\n\t\t\text = path[i:].lower()\n\t\tcontent_type = mimetypes.types_map.get(ext, None)\n\tif (content_type is not None):\n\t\tresponse.headers['Content-Type'] = content_type\n\tif debug:\n\t\tcherrypy.log(('Content-Type:\t%r' % content_type), 'TOOLS.STATIC')\n\tcd = None\n\tif (disposition is not None):\n\t\tif (name is None):\n\t\t\tname = os.path.basename(path)\n\t\tcd = ('%s;\tfilename=\"%s\"' % (disposition, name))\n\t\tresponse.headers['Content-Disposition'] = cd\n\tif debug:\n\t\tcherrypy.log(('Content-Disposition:\t%r' % cd), 'TOOLS.STATIC')\n\tcontent_length = st.st_size\n\tfileobj = open(path, 'rb')\n\treturn _serve_fileobj(fileobj, content_type, content_length, debug=debug)\n",["return a chunk from a file based on the data received ."]]
["def serialize_remote_exception(failure_info, log_failure=True):\n\ttb = traceback.format_exception(*failure_info)\n\tfailure = failure_info[1]\n\tif log_failure:\n\t\tLOG.error(_('Returning\texception\t%s\tto\tcaller'), six.text_type(failure))\n\t\tLOG.error(tb)\n\tkwargs = {}\n\tif hasattr(failure, 'kwargs'):\n\t\tkwargs = failure.kwargs\n\tcls_name = str(failure.__class__.__name__)\n\tmod_name = str(failure.__class__.__module__)\n\tif (cls_name.endswith(_REMOTE_POSTFIX) and mod_name.endswith(_REMOTE_POSTFIX)):\n\t\tcls_name = cls_name[:(- len(_REMOTE_POSTFIX))]\n\t\tmod_name = mod_name[:(- len(_REMOTE_POSTFIX))]\n\tdata = {'class': cls_name, 'module': mod_name, 'message': six.text_type(failure), 'tb': tb, 'args': failure.args, 'kwargs': kwargs}\n\tjson_data = jsonutils.dumps(data)\n\treturn json_data\n",["prepares exception data to be sent over rpc ."]]
["def _dnsname_match(dn, hostname, max_wildcards=1):\n\tpats = []\n\tif (not dn):\n\t\treturn False\n\tparts = dn.split('.')\n\tleftmost = parts[0]\n\twildcards = leftmost.count('*')\n\tif (wildcards > max_wildcards):\n\t\traise CertificateError(('too\tmany\twildcards\tin\tcertificate\tDNS\tname:\t' + repr(dn)))\n\tif (not wildcards):\n\t\treturn (dn.lower() == hostname.lower())\n\tif (leftmost == '*'):\n\t\tpats.append('[^.]+')\n\telif (leftmost.startswith('xn--') or hostname.startswith('xn--')):\n\t\tpats.append(re.escape(leftmost))\n\telse:\n\t\tpats.append(re.escape(leftmost).replace('\\\\*', '[^.]*'))\n\tfor frag in parts[1:]:\n\t\tpats.append(re.escape(frag))\n\tpat = re.compile((('\\\\A' + '\\\\.'.join(pats)) + '\\\\Z'), re.IGNORECASE)\n\treturn pat.match(hostname)\n",["matching according to rfc 6125 ."]]
["def data_to_token_ids(data_path, target_path, vocabulary_path, tokenizer=None, normalize_digits=True, UNK_ID=3, _DIGIT_RE=re.compile('\\\\d')):\n\tif (not gfile.Exists(target_path)):\n\t\tprint ('Tokenizing\tdata\tin\t%s' % data_path)\n\t\t(vocab, _) = initialize_vocabulary(vocabulary_path)\n\t\twith gfile.GFile(data_path, mode='rb') as data_file:\n\t\t\twith gfile.GFile(target_path, mode='w') as tokens_file:\n\t\t\t\tcounter = 0\n\t\t\t\tfor line in data_file:\n\t\t\t\t\tcounter += 1\n\t\t\t\t\tif ((counter % 100000) == 0):\n\t\t\t\t\t\tprint ('\t\ttokenizing\tline\t%d' % counter)\n\t\t\t\t\ttoken_ids = sentence_to_token_ids(line, vocab, tokenizer, normalize_digits, UNK_ID=UNK_ID, _DIGIT_RE=_DIGIT_RE)\n\t\t\t\t\ttokens_file.write(('\t'.join([str(tok) for tok in token_ids]) + '\\n'))\n\telse:\n\t\tprint ('Target\tpath\t%s\texists' % target_path)\n",["tokenize data file and turn into token-ids using given vocabulary file ."]]
["def _patch_multiple(target, spec=None, create=False, spec_set=None, autospec=None, new_callable=None, **kwargs):\n\tdef g_importer():\n\t\treturn _importer(target)\n\tdef g_target():\n\t\treturn target\n\tif (type(target) in (unicode, str)):\n\t\tgetter = g_importer\n\telse:\n\t\tgetter = g_target\n\tif (not kwargs):\n\t\traise ValueError('Must\tsupply\tat\tleast\tone\tkeyword\targument\twith\tpatch.multiple')\n\titems = list(kwargs.items())\n\t(attribute, new) = items[0]\n\tpatcher = _patch(getter, attribute, new, spec, create, spec_set, autospec, new_callable, {})\n\tpatcher.attribute_name = attribute\n\tfor (attribute, new) in items[1:]:\n\t\tthis_patcher = _patch(getter, attribute, new, spec, create, spec_set, autospec, new_callable, {})\n\t\tthis_patcher.attribute_name = attribute\n\t\tpatcher.additional_patchers.append(this_patcher)\n\treturn patcher\n",["perform multiple patches in a single call ."]]
["def dummyModelParams(perm):\n\terrScore = 50\n\tif (perm['modelParams']['sensorParams']['encoders']['address'] is not None):\n\t\terrScore -= 20\n\tif (perm['modelParams']['sensorParams']['encoders']['gym'] is not None):\n\t\terrScore -= 10\n\tif (perm['modelParams']['sensorParams']['encoders']['timestamp_dayOfWeek'] is not None):\n\t\terrScore += 30\n\tif (perm['modelParams']['sensorParams']['encoders']['timestamp_timeOfDay'] is not None):\n\t\terrScore += 40\n\tdummyModelParams = dict(metricValue=errScore, iterations=int(os.environ.get('NTA_TEST_numIterations', '1')), waitTime=None, sysExitModelRange=os.environ.get('NTA_TEST_sysExitModelRange', None), errModelRange=os.environ.get('NTA_TEST_errModelRange', None), jobFailErr=bool(os.environ.get('NTA_TEST_jobFailErr', False)))\n\treturn dummyModelParams\n",["this function can be used for hypersearch algorithm development ."]]
["def model_to_dict(instance, fields=None, exclude=None):\n\tfrom django.db.models.fields.related import ManyToManyField\n\topts = instance._meta\n\tdata = {}\n\tfor f in (opts.fields + opts.many_to_many):\n\t\tif (not f.editable):\n\t\t\tcontinue\n\t\tif (fields and (not (f.name in fields))):\n\t\t\tcontinue\n\t\tif (exclude and (f.name in exclude)):\n\t\t\tcontinue\n\t\tif isinstance(f, ManyToManyField):\n\t\t\tif (instance.pk is None):\n\t\t\t\tdata[f.name] = []\n\t\t\telse:\n\t\t\t\tdata[f.name] = list(f.value_from_object(instance).values_list(u'pk', flat=True))\n\t\telse:\n\t\t\tdata[f.name] = f.value_from_object(instance)\n\treturn data\n",["returns a dict containing the data in instance suitable for passing as a forms initial keyword argument ."]]
["def select(rlist, wlist, xlist, timeout=None):\n\tallevents = []\n\ttimeout = Timeout.start_new(timeout)\n\tresult = SelectResult()\n\ttry:\n\t\ttry:\n\t\t\tfor readfd in rlist:\n\t\t\t\tallevents.append(core.read_event(get_fileno(readfd), result.update, arg=readfd))\n\t\t\tfor writefd in wlist:\n\t\t\t\tallevents.append(core.write_event(get_fileno(writefd), result.update, arg=writefd))\n\t\texcept IOError as ex:\n\t\t\traise error(*ex.args)\n\t\tresult.event.wait(timeout=timeout)\n\t\treturn (result.read, result.write, [])\n\tfinally:\n\t\tfor evt in allevents:\n\t\t\tevt.cancel()\n\t\ttimeout.cancel()\n",["an implementation of :meth:select ."]]
["def bind_port(sock, host=HOST):\n\tif ((sock.family == socket.AF_INET) and (sock.type == socket.SOCK_STREAM)):\n\t\tif hasattr(socket, 'SO_REUSEADDR'):\n\t\t\tif (sock.getsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR) == 1):\n\t\t\t\traise TestFailed('tests\tshould\tnever\tset\tthe\tSO_REUSEADDR\tsocket\toption\ton\tTCP\/IP\tsockets!')\n\t\tif hasattr(socket, 'SO_REUSEPORT'):\n\t\t\tif (sock.getsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT) == 1):\n\t\t\t\traise TestFailed('tests\tshould\tnever\tset\tthe\tSO_REUSEPORT\tsocket\toption\ton\tTCP\/IP\tsockets!')\n\t\tif hasattr(socket, 'SO_EXCLUSIVEADDRUSE'):\n\t\t\tsock.setsockopt(socket.SOL_SOCKET, socket.SO_EXCLUSIVEADDRUSE, 1)\n\tsock.bind((host, 0))\n\tport = sock.getsockname()[1]\n\treturn port\n",["bind the socket to a free port and return the port number ."]]
["def file_upload_view_verify(request):\n\tform_data = request.POST.copy()\n\tform_data.update(request.FILES)\n\tfor (key, value) in form_data.items():\n\t\tif key.endswith(u'_hash'):\n\t\t\tcontinue\n\t\tif ((key + u'_hash') not in form_data):\n\t\t\tcontinue\n\t\tsubmitted_hash = form_data[(key + u'_hash')]\n\t\tif isinstance(value, UploadedFile):\n\t\t\tnew_hash = hashlib.sha1(value.read()).hexdigest()\n\t\telse:\n\t\t\tnew_hash = hashlib.sha1(force_bytes(value)).hexdigest()\n\t\tif (new_hash != submitted_hash):\n\t\t\treturn HttpResponseServerError()\n\tlargefile = request.FILES[u'file_field2']\n\tobj = FileModel()\n\tobj.testfile.save(largefile.name, largefile)\n\treturn HttpResponse(u'')\n",["use the sha digest hash to verify the uploaded contents ."]]
["def get_metrics():\n\tglobal METRICS, LAST_METRICS\n\tif ((time.time() - METRICS['time']) > METRICS_CACHE_TTL):\n\t\tmetrics = {}\n\t\tfor status_type in PARAMS.keys():\n\t\t\tio = os.popen(PARAMS[status_type])\n\t\t\tmetrics_str = ''.join(io.readlines()).strip()\n\t\t\tmetrics_str = re.sub('\\\\w+\\\\((.*)\\\\)', '\\\\1', metrics_str)\n\t\t\ttry:\n\t\t\t\tif (status_type == 'server_status'):\n\t\t\t\t\tmetrics.update(flatten(json.loads(metrics_str)))\n\t\t\t\telse:\n\t\t\t\t\tmetrics.update(flatten(json.loads(metrics_str), pre=('%s_' % status_type)))\n\t\t\texcept ValueError:\n\t\t\t\tmetrics = {}\n\t\tLAST_METRICS = copy.deepcopy(METRICS)\n\t\tMETRICS = {'time': time.time(), 'data': metrics}\n\treturn [METRICS, LAST_METRICS]\n",["return all metrics ."]]
["def urldecode(query):\n\tif (query and (not (set(query) <= urlencoded))):\n\t\traise ValueError('Invalid\tcharacters\tin\tquery\tstring.')\n\tinvalid_hex = u'%[^0-9A-Fa-f]|%[0-9A-Fa-f][^0-9A-Fa-f]'\n\tif len(re.findall(invalid_hex, query)):\n\t\traise ValueError('Invalid\thex\tencoding\tin\tquery\tstring.')\n\tquery = (query.decode('utf-8') if isinstance(query, str) else query)\n\tparams = urlparse.parse_qsl(query, keep_blank_values=True)\n\treturn decode_params_utf8(params)\n",["decode a query string in x-www-form-urlencoded format into a sequence of two-element tuples ."]]
["def remove_packages(module, port_path, packages):\n\tremove_c = 0\n\tfor package in packages:\n\t\tif (not query_package(module, port_path, package)):\n\t\t\tcontinue\n\t\t(rc, out, err) = module.run_command(('%s\tuninstall\t%s' % (port_path, package)))\n\t\tif query_package(module, port_path, package):\n\t\t\tmodule.fail_json(msg=('failed\tto\tremove\t%s:\t%s' % (package, out)))\n\t\tremove_c += 1\n\tif (remove_c > 0):\n\t\tmodule.exit_json(changed=True, msg=('removed\t%s\tpackage(s)' % remove_c))\n\tmodule.exit_json(changed=False, msg='package(s)\talready\tabsent')\n",["uninstalls one or more packages if installed ."]]
["def make_diff_image(im1, im2):\n\tds = im1.shape\n\tes = im2.shape\n\tdiff = np.empty((max(ds[0], es[0]), max(ds[1], es[1]), 4), dtype=int)\n\tdiff[..., :3] = 128\n\tdiff[..., 3] = 255\n\tdiff[:ds[0], :ds[1], :min(ds[2], 3)] += im1[..., :3]\n\tdiff[:es[0], :es[1], :min(es[2], 3)] -= im2[..., :3]\n\tdiff = np.clip(diff, 0, 255).astype(np.ubyte)\n\treturn diff\n",["return image array showing the differences between im1 and im2 ."]]
["def load_certificate(type, buffer):\n\tif isinstance(buffer, _text_type):\n\t\tbuffer = buffer.encode('ascii')\n\tbio = _new_mem_buf(buffer)\n\tif (type == FILETYPE_PEM):\n\t\tx509 = _lib.PEM_read_bio_X509(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n\telif (type == FILETYPE_ASN1):\n\t\tx509 = _lib.d2i_X509_bio(bio, _ffi.NULL)\n\telse:\n\t\traise ValueError('type\targument\tmust\tbe\tFILETYPE_PEM\tor\tFILETYPE_ASN1')\n\tif (x509 == _ffi.NULL):\n\t\t_raise_current_error()\n\tcert = X509.__new__(X509)\n\tcert._x509 = _ffi.gc(x509, _lib.X509_free)\n\treturn cert\n",["load a certificate from a buffer ."]]
["def initialize_decorator(init):\n\tdef initialize(self, *args, **kwargs):\n\t\tcls = type(self)\n\t\tfor (k, v) in kwargs.items():\n\t\t\tif hasattr(cls, k):\n\t\t\t\tattr = getattr(cls, k)\n\t\t\t\tif isinstance(attr, InstrumentedAttribute):\n\t\t\t\t\tcolumn = attr.property.columns[0]\n\t\t\t\t\tif isinstance(column.type, String):\n\t\t\t\t\t\tif (not isinstance(v, six.text_type)):\n\t\t\t\t\t\t\tv = six.text_type(v)\n\t\t\t\t\t\tif (column.type.length and (column.type.length < len(v))):\n\t\t\t\t\t\t\traise exception.StringLengthExceeded(string=v, type=k, length=column.type.length)\n\t\tinit(self, *args, **kwargs)\n\treturn initialize\n",["ensure that the length of string field do not exceed the limit ."]]
["def GetArgs():\n\tparser = argparse.ArgumentParser(description='Process\targs\tfor\tretrieving\tall\tthe\tVirtual\tMachines')\n\tparser.add_argument('-s', '--host', required=True, action='store', help='Remote\thost\tto\tconnect\tto')\n\tparser.add_argument('-o', '--port', type=int, default=443, action='store', help='Port\tto\tconnect\ton')\n\tparser.add_argument('-u', '--user', required=True, action='store', help='User\tname\tto\tuse\twhen\tconnecting\tto\thost')\n\tparser.add_argument('-p', '--password', required=False, action='store', help='Password\tto\tuse\twhen\tconnecting\tto\thost')\n\targs = parser.parse_args()\n\treturn args\n",["supports the command-line arguments listed below ."]]
["def parse_config_h(fp, vars=None):\n\tif (vars is None):\n\t\tvars = {}\n\timport re\n\tdefine_rx = re.compile('#define\t([A-Z][A-Za-z0-9_]+)\t(.*)\\n')\n\tundef_rx = re.compile('\/[*]\t#undef\t([A-Z][A-Za-z0-9_]+)\t[*]\/\\n')\n\twhile True:\n\t\tline = fp.readline()\n\t\tif (not line):\n\t\t\tbreak\n\t\tm = define_rx.match(line)\n\t\tif m:\n\t\t\t(n, v) = m.group(1, 2)\n\t\t\ttry:\n\t\t\t\tv = int(v)\n\t\t\texcept ValueError:\n\t\t\t\tpass\n\t\t\tvars[n] = v\n\t\telse:\n\t\t\tm = undef_rx.match(line)\n\t\t\tif m:\n\t\t\t\tvars[m.group(1)] = 0\n\treturn vars\n",["parse a config ."]]
["def parse(json_string):\n\ttry:\n\t\tjson_data = json.loads(json_string)\n\texcept:\n\t\traise SchemaParseException(('Error\tparsing\tJSON:\t%s' % json_string))\n\tnames = Names()\n\treturn make_avsc_object(json_data, names)\n",["parse a css *group of selectors* ."]]
["def open_in_browser(response, _openfunc=webbrowser.open):\n\tfrom scrapy.http import HtmlResponse, TextResponse\n\tbody = response.body\n\tif isinstance(response, HtmlResponse):\n\t\tif ('<base' not in body):\n\t\t\trepl = ('<head><base\thref=\"%s\">' % response.url)\n\t\t\tbody = body.replace('<head>', to_bytes(repl))\n\t\text = '.html'\n\telif isinstance(response, TextResponse):\n\t\text = '.txt'\n\telse:\n\t\traise TypeError(('Unsupported\tresponse\ttype:\t%s' % response.__class__.__name__))\n\t(fd, fname) = tempfile.mkstemp(ext)\n\tos.write(fd, body)\n\tos.close(fd)\n\treturn _openfunc(('file:\/\/%s' % fname))\n",["open the given response in a local web browser ."]]
["def filterwarnings(action, message='', category=Warning, module='', lineno=0, append=0):\n\timport re\n\traise ((action in ('error', 'ignore', 'always', 'default', 'module', 'once')) or AssertionError), ('invalid\taction:\t%r' % (action,))\n\traise (isinstance(message, basestring) or AssertionError), 'message\tmust\tbe\ta\tstring'\n\traise (isinstance(category, (type, types.ClassType)) or AssertionError), 'category\tmust\tbe\ta\tclass'\n\traise (issubclass(category, Warning) or AssertionError), 'category\tmust\tbe\ta\tWarning\tsubclass'\n\traise (isinstance(module, basestring) or AssertionError), 'module\tmust\tbe\ta\tstring'\n\tif (not (isinstance(lineno, int) and (lineno >= 0))):\n\t\traise AssertionError, 'lineno\tmust\tbe\tan\tint\t>=\t0'\n\t\titem = (action, re.compile(message, re.I), category, re.compile(module), lineno)\n\t\t(append and filters.append(item))\n\telse:\n\t\tfilters.insert(0, item)\n",["insert an entry into the list of warnings filters ."]]
["def insert_data(test_case, host, port):\n\td = get_postgres_connection(host, port)\n\tdef create_database(connection):\n\t\tconnection.autocommit = True\n\t\tcursor = connection.cursor()\n\t\tcursor.execute('CREATE\tDATABASE\tflockertest;')\n\t\tcursor.close()\n\t\tconnection.close()\n\td.addCallback(create_database)\n\td.addCallback((lambda _: get_postgres_connection(host, port, u'flockertest')))\n\tdef add_data(connection):\n\t\tcursor = connection.cursor()\n\t\tcursor.execute('CREATE\tTABLE\ttesttable\t(testcolumn\tint);')\n\t\tcursor.execute('INSERT\tINTO\ttesttable\t(testcolumn)\tVALUES\t(123);')\n\t\tconnection.commit()\n\t\tconnection.close()\n\td.addCallback(add_data)\n\treturn d\n",["insert some data into the database ."]]
["def make_step_decorator(context, instance, update_instance_progress, total_offset=0):\n\tstep_info = dict(total=total_offset, current=0)\n\tdef bump_progress():\n\t\tstep_info['current'] += 1\n\t\tupdate_instance_progress(context, instance, step_info['current'], step_info['total'])\n\tdef step_decorator(f):\n\t\tstep_info['total'] += 1\n\t\t@functools.wraps(f)\n\t\tdef inner(*args, **kwargs):\n\t\t\trv = f(*args, **kwargs)\n\t\t\tbump_progress()\n\t\t\treturn rv\n\t\treturn inner\n\treturn step_decorator\n",["factory to create a decorator that records instance progress as a series of discrete steps ."]]
["def load():\n\tdata = _get_data()\n\treturn du.process_recarray(data, endog_idx=0, dtype=float)\n",["deserialize fp (a ."]]
["def read_cached_file(filename, cache_info, reload_func=None):\n\tmtime = os.path.getmtime(filename)\n\tif ((not cache_info) or (mtime != cache_info.get('mtime'))):\n\t\tLOG.debug((_('Reloading\tcached\tfile\t%s') % filename))\n\t\twith open(filename) as fap:\n\t\t\tcache_info['data'] = fap.read()\n\t\tcache_info['mtime'] = mtime\n\t\tif reload_func:\n\t\t\treload_func(cache_info['data'])\n\treturn cache_info['data']\n",["read from a file if it has been modified ."]]
["def parse_date(string):\n\treturn get_i18n().parse_date(string)\n",["parses iso 8601 dates into datetime objects the timezone is parsed from the date string ."]]
["def with_polymorphic(base, classes, selectable=False, flat=False, polymorphic_on=None, aliased=False, innerjoin=False, _use_mapper_path=False):\n\tprimary_mapper = _class_to_mapper(base)\n\t(mappers, selectable) = primary_mapper._with_polymorphic_args(classes, selectable, innerjoin=innerjoin)\n\tif (aliased or flat):\n\t\tselectable = selectable.alias(flat=flat)\n\treturn AliasedClass(base, selectable, with_polymorphic_mappers=mappers, with_polymorphic_discriminator=polymorphic_on, use_mapper_path=_use_mapper_path)\n",["produce an :class: ."]]
["def GetPlatformToken(os_module=os, sys_module=sys, platform=sys.platform):\n\tif hasattr(os_module, 'uname'):\n\t\tuname = os_module.uname()\n\t\treturn ('%s\/%s' % (uname[0], uname[2]))\n\telse:\n\t\treturn 'unknown'\n",["returns a user-agent token for the host system platform ."]]
["def random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None):\n\tif (dtype is None):\n\t\tdtype = floatx()\n\tshape = tuple(map(int, shape))\n\ttf_dtype = _convert_string_dtype(dtype)\n\tif (seed is None):\n\t\tseed = np.random.randint(1000000000.0)\n\tvalue = tf.random_normal_initializer(mean, scale, dtype=tf_dtype, seed=seed)(shape)\n\treturn variable(value, dtype=dtype, name=name)\n",["instantiates an keras variable filled with samples drawn from a normal distribution and returns it ."]]
["def copy_missing_vector(a, b, missing, inplace=False, prefix=None):\n\tif (prefix is None):\n\t\tprefix = find_best_blas_type((a, b))[0]\n\tcopy = prefix_copy_missing_vector_map[prefix]\n\tif (not inplace):\n\t\tb = np.copy(b, order='F')\n\ttry:\n\t\tif (not a.is_f_contig()):\n\t\t\traise ValueError()\n\texcept:\n\t\ta = np.asfortranarray(a)\n\tcopy(a, b, np.asfortranarray(missing))\n\treturn b\n",["reorder the elements of a time-varying vector where all non-missing values are in the first elements of the vector ."]]
["def select_proxy(url, proxies):\n\tproxies = (proxies or {})\n\turlparts = urlparse(url)\n\tif (urlparts.hostname is None):\n\t\treturn proxies.get(urlparts.scheme, proxies.get('all'))\n\tproxy_keys = [((urlparts.scheme + ':\/\/') + urlparts.hostname), urlparts.scheme, ('all:\/\/' + urlparts.hostname), 'all']\n\tproxy = None\n\tfor proxy_key in proxy_keys:\n\t\tif (proxy_key in proxies):\n\t\t\tproxy = proxies[proxy_key]\n\t\t\tbreak\n\treturn proxy\n",["select a proxy for the url ."]]
["def forward_substitution(lower_triangle, variable, constant, K):\n\tcopy_lower_triangle = copy.deepcopy(lower_triangle)\n\tnrow = len(copy_lower_triangle)\n\tresult = []\n\tfor i in range(nrow):\n\t\ta = K.zero\n\t\tfor j in range(i):\n\t\t\ta += (copy_lower_triangle[i][j] * variable[j][0])\n\t\tvariable[i][0] = ((constant[i][0] - a) \/ copy_lower_triangle[i][i])\n\treturn variable\n",["performs forward substitution given a lower triangular matrix ."]]
["def maybe_download(filename, work_directory):\n\tif (not os.path.exists(work_directory)):\n\t\tos.mkdir(work_directory)\n\tfilepath = os.path.join(work_directory, filename)\n\tif (not os.path.exists(filepath)):\n\t\t(filepath, _) = urllib.urlretrieve((SOURCE_URL + filename), filepath)\n\t\tstatinfo = os.stat(filepath)\n\t\tprint('Succesfully\tdownloaded', filename, statinfo.st_size, 'bytes.')\n\treturn filepath\n",["download the data from yanns website ."]]
["@webob.dec.wsgify\n@util.check_accept('application\/json')\ndef get_resource_provider(req):\n\tuuid = util.wsgi_path_item(req.environ, 'uuid')\n\tcontext = req.environ['placement.context']\n\tresource_provider = objects.ResourceProvider.get_by_uuid(context, uuid)\n\treq.response.body = encodeutils.to_utf8(jsonutils.dumps(_serialize_provider(req.environ, resource_provider)))\n\treq.response.content_type = 'application\/json'\n\treturn req.response\n",["get a single resource provider ."]]
["def GetResult(Handle, IOType, Channel):\n\tif (os.name == 'nt'):\n\t\tstaticLib = ctypes.windll.LoadLibrary('labjackud')\n\t\tpv = ctypes.c_double()\n\t\tec = staticLib.GetResult(Handle, IOType, Channel, ctypes.byref(pv))\n\t\tif (ec != 0):\n\t\t\traise LabJackException(ec)\n\t\treturn pv.value\n\telse:\n\t\traise LabJackException(0, 'Function\tonly\tsupported\tfor\tWindows')\n",["put one value to the labjack device eput is equivilent to an addrequest followed by a goone ."]]
["@require_POST\n@login_required\n@permitted\ndef un_pin_thread(request, course_id, thread_id):\n\tcourse_key = CourseKey.from_string(course_id)\n\tuser = cc.User.from_django_user(request.user)\n\tthread = cc.Thread.find(thread_id)\n\tthread.un_pin(user, thread_id)\n\treturn JsonResponse(prepare_content(thread.to_dict(), course_key))\n",["given a course id and thread id ."]]
["def find_drum_group_device(track_or_chain):\n\tinstrument = find_if((lambda d: (d.type == Live.Device.DeviceType.instrument)), track_or_chain.devices)\n\tif instrument:\n\t\tif instrument.can_have_drum_pads:\n\t\t\treturn instrument\n\t\telif instrument.can_have_chains:\n\t\t\treturn find_if(bool, imap(find_drum_group_device, instrument.chains))\n",["looks up recursively for a drum_group device in the track ."]]
["def messageid(uniq=None, N=idGenerator().next):\n\tdatetime = time.strftime('%Y%m%d%H%M%S', time.gmtime())\n\tpid = os.getpid()\n\trand = random.randrange(((2 ** 31L) - 1))\n\tif (uniq is None):\n\t\tuniq = ''\n\telse:\n\t\tuniq = ('.' + uniq)\n\treturn ('<%s.%s.%s%s.%s@%s>' % (datetime, pid, rand, uniq, N(), DNSNAME))\n",["return a globally unique random string in rfc 2822 message-id format <datetime ."]]
["@environmentfilter\ndef do_attr(environment, obj, name):\n\ttry:\n\t\tname = str(name)\n\texcept UnicodeError:\n\t\tpass\n\telse:\n\t\ttry:\n\t\t\tvalue = getattr(obj, name)\n\t\texcept AttributeError:\n\t\t\tpass\n\t\telse:\n\t\t\tif (environment.sandboxed and (not environment.is_safe_attribute(obj, name, value))):\n\t\t\t\treturn environment.unsafe_undefined(obj, name)\n\t\t\treturn value\n\treturn environment.undefined(obj=obj, name=name)\n",["get an attribute of an object ."]]
["def get_makefile_filename():\n\tif python_build:\n\t\treturn os.path.join(project_base, 'Makefile')\n\tlib_dir = get_python_lib(plat_specific=1, standard_lib=1)\n\treturn os.path.join(lib_dir, 'config', 'Makefile')\n",["return the path of the makefile ."]]
["def check_random_state(seed):\n\tif ((seed is None) or (seed is np.random)):\n\t\treturn np.random.mtrand._rand\n\tif isinstance(seed, (int, np.integer)):\n\t\treturn np.random.RandomState(seed)\n\tif isinstance(seed, np.random.RandomState):\n\t\treturn seed\n\traise ValueError(('%r\tcannot\tbe\tused\tto\tseed\ta\tnumpy.random.RandomState\tinstance' % seed))\n",["turn seed into a np ."]]
["def makeZip(fileList, archive):\n\ttry:\n\t\ta = zipfile.ZipFile(archive, u'w', zipfile.ZIP_DEFLATED, allowZip64=True)\n\t\tfor f in fileList:\n\t\t\ta.write(f)\n\t\ta.close()\n\t\treturn True\n\texcept Exception as e:\n\t\tsickrage.srCore.srLogger.error((u'Zip\tcreation\terror:\t%r\t' % repr(e)))\n\t\treturn False\n",["create a zip of files ."]]
["def picknthweekday(year, month, dayofweek, hour, minute, whichweek):\n\tfirst = datetime.datetime(year, month, 1, hour, minute)\n\tweekdayone = first.replace(day=(((dayofweek - first.isoweekday()) % 7) + 1))\n\twd = (weekdayone + ((whichweek - 1) * ONEWEEK))\n\tif (wd.month != month):\n\t\twd -= ONEWEEK\n\treturn wd\n",["dayofweek == 0 means sunday ."]]
["def validate_bool(args, key, default=None):\n\tvalue = validate_exists(args, key, default)\n\tif (value.lower() == 'true'):\n\t\treturn True\n\telif (value.lower() == 'false'):\n\t\treturn False\n\telse:\n\t\traise ArgumentError((_(\"Argument\t%(key)s\tmay\tnot\ttake\tvalue\t%(value)s.\tValid\tvalues\tare\t['true',\t'false'].\") % {'key': key, 'value': value}))\n",["convert b to a boolean or raise ."]]
["def processSVGElementpath(svgReader, xmlElement):\n\tif ('d' not in xmlElement.attributeDictionary):\n\t\tprint 'Warning,\tin\tprocessSVGElementpath\tin\tsvgReader\tcan\tnot\tget\ta\tvalue\tfor\td\tin:'\n\t\tprint xmlElement.attributeDictionary\n\t\treturn\n\trotatedLoopLayer = svgReader.getRotatedLoopLayer()\n\tPathReader(rotatedLoopLayer.loops, xmlElement, svgReader.yAxisPointingUpward)\n",["process elementnode by svgreader ."]]
["def bdate_range(start=None, end=None, periods=None, freq='B', tz=None, normalize=True, name=None, closed=None, **kwargs):\n\treturn DatetimeIndex(start=start, end=end, periods=periods, freq=freq, tz=tz, normalize=normalize, name=name, closed=closed, **kwargs)\n",["return a fixed frequency datetime index ."]]
["@require_POST\n@login_required\ndef unwatch_document(request, document_slug):\n\tdocument = get_object_or_404(Document, locale=request.LANGUAGE_CODE, slug=document_slug)\n\tEditDocumentEvent.stop_notifying(request.user, document)\n\treturn HttpResponseRedirect(document.get_absolute_url())\n",["stop watching a document for edits ."]]
["def wrapper(func, *args, **kwds):\n\tres = None\n\ttry:\n\t\tstdscr = curses.initscr()\n\t\tcurses.noecho()\n\t\tcurses.cbreak()\n\t\tstdscr.keypad(1)\n\t\ttry:\n\t\t\tcurses.start_color()\n\t\texcept:\n\t\t\tpass\n\t\treturn func(stdscr, *args, **kwds)\n\tfinally:\n\t\tstdscr.keypad(0)\n\t\tcurses.echo()\n\t\tcurses.nocbreak()\n\t\tcurses.endwin()\n",["wrapper function that initializes curses and calls another function ."]]
["def foldr(f, seq, default=_no_default):\n\treturn reduce(flip(f), reversed(seq), *((default,) if (default is not _no_default) else ()))\n",["reduce elems using fn to combine them from right to left ."]]
["def changequery(**kw):\n\tquery = input(_method='get')\n\tfor (k, v) in kw.iteritems():\n\t\tif (v is None):\n\t\t\tquery.pop(k, None)\n\t\telse:\n\t\t\tquery[k] = v\n\tout = ctx.path\n\tif query:\n\t\tout += ('?' + urllib.urlencode(query))\n\treturn out\n",["imagine youre at \/foo?a=1&b=2 ."]]
["def isPackageDirectory(dirname):\n\tfor ext in zip(*imp.get_suffixes())[0]:\n\t\tinitFile = ('__init__' + ext)\n\t\tif os.path.exists(os.path.join(dirname, initFile)):\n\t\t\treturn initFile\n\treturn False\n",["is the directory at path dirname a python package directory? returns the name of the __init__ file if dirname is a package directory ."]]
["def cross_from_above(x, threshold):\n\tx = np.asarray(x)\n\tind = np.nonzero(((x[:(-1)] >= threshold) & (x[1:] < threshold)))[0]\n\tif len(ind):\n\t\treturn (ind + 1)\n\telse:\n\t\treturn ind\n",["return the indices into *x* where *x* crosses some threshold from below ."]]
["def directory_browser(request, path='\/'):\n\tdirectories = DojoFileStore(path, dirsonly=True, root=request.GET.get('root', '\/')).items()\n\tcontext = directories\n\tcontent = json.dumps(context)\n\treturn HttpResponse(content, content_type='application\/json')\n",["this view provides the ajax driven directory browser callback ."]]
["def locatedExpr(expr):\n\tlocator = Empty().setParseAction((lambda s, l, t: l))\n\treturn Group(((locator('locn_start') + expr('value')) + locator.copy().leaveWhitespace()('locn_end')))\n",["helper to decorate a returned token with its starting and ending locations in the input string ."]]
["def get_object_or_404(queryset, *filter_args, **filter_kwargs):\n\ttry:\n\t\treturn _get_object_or_404(queryset, *filter_args, **filter_kwargs)\n\texcept (TypeError, ValueError):\n\t\traise Http404\n",["same as djangos standard shortcut ."]]
["@environmentfilter\ndef do_urlize(environment, value, trim_url_limit=None, nofollow=False):\n\trv = urlize(value, trim_url_limit, nofollow)\n\tif environment.autoescape:\n\t\trv = Markup(rv)\n\treturn rv\n",["converts urls in plain text into clickable links ."]]
["def run_migrations_offline():\n\tset_mysql_engine()\n\tkwargs = dict()\n\tif neutron_config.database.connection:\n\t\tkwargs['url'] = neutron_config.database.connection\n\telse:\n\t\tkwargs['dialect_name'] = neutron_config.database.engine\n\tkwargs['include_object'] = include_object\n\tcontext.configure(**kwargs)\n\twith context.begin_transaction():\n\t\tcontext.run_migrations()\n",["run migrations in offline mode ."]]
["def linebreaksbr(value, autoescape=None):\n\tif (autoescape and (not isinstance(value, SafeData))):\n\t\tfrom google.appengine._internal.django.utils.html import escape\n\t\tvalue = escape(value)\n\treturn mark_safe(value.replace('\\n', '<br\t\/>'))\n",["converts all newlines in a piece of plain text to html line breaks ."]]
["def module_to_dict(module, omittable=(lambda k: k.startswith('_'))):\n\treturn dict(((k, repr(v)) for (k, v) in module.__dict__.items() if (not omittable(k))))\n",["converts a module namespace to a python dictionary ."]]
["def _make_sync_call(service, call, request, response):\n\tresp = apiproxy_stub_map.MakeSyncCall(service, call, request, response)\n\tif (resp is not None):\n\t\treturn resp\n\treturn response\n",["the apiproxy entry point for a synchronous api call ."]]
["def boto_exception(err):\n\tif hasattr(err, 'error_message'):\n\t\terror = err.error_message\n\telif hasattr(err, 'message'):\n\t\terror = ((((err.message + '\t') + str(err)) + '\t-\t') + str(type(err)))\n\telse:\n\t\terror = ('%s:\t%s' % (Exception, err))\n\treturn error\n",["generic error message handler ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load if boto libraries exist ."]]
["@register.simple_tag\ndef simple_only_unlimited_args(*args):\n\treturn ('simple_only_unlimited_args\t-\tExpected\tresult:\t%s' % ',\t'.join([unicode(arg) for arg in args]))\n",["expected simple_only_unlimited_args __doc__ ."]]
["def get_language_bidi():\n\tfrom django.conf import settings\n\tbase_lang = get_language().split(u'-')[0]\n\treturn (base_lang in settings.LANGUAGES_BIDI)\n",["returns selected languages bidi layout ."]]
["def test_io_error_if_no_replay_file(mocker, replay_test_dir):\n\twith pytest.raises(IOError):\n\t\treplay.load(replay_test_dir, 'no_replay')\n",["test that replay ."]]
["def autocommit(using=None):\n\twarnings.warn('autocommit\tis\tdeprecated\tin\tfavor\tof\tset_autocommit.', PendingDeprecationWarning, stacklevel=2)\n\tdef entering(using):\n\t\tenter_transaction_management(managed=False, using=using)\n\tdef exiting(exc_type, using):\n\t\tleave_transaction_management(using=using)\n\treturn _transaction_func(entering, exiting, using)\n",["decorator that activates commit on save ."]]
["def getManipulatedGeometryOutput(elementNode, geometryOutput, prefix):\n\tflippedGeometryOutput = triangle_mesh.getGeometryOutputCopy(geometryOutput)\n\tflip.flipPoints(elementNode, matrix.getVertexes(flippedGeometryOutput), prefix)\n\tif flip.getShouldReverse(elementNode, prefix):\n\t\tflippedFaces = face.getFaces(flippedGeometryOutput)\n\t\tfor flippedFace in flippedFaces:\n\t\t\tflippedFace.vertexIndexes.reverse()\n\treturn {'union': {'shapes': [flippedGeometryOutput, geometryOutput]}}\n",["get equated geometryoutput ."]]
["@status('Misc\/NEWS\tupdated', modal=True)\ndef reported_news(file_paths):\n\treturn (os.path.join('Misc', 'NEWS') in file_paths)\n",["check if misc\/news has been changed ."]]
["def _XXX(k, n, s):\n\tx = (s % (string.replace(k.__module__, '.', '_'), k.__name__, n))\n\treturn x\n",["string manipulation garbage ."]]
["def disable(name, lbn, target, profile='default', tgt_type='glob', expr_form=None):\n\tif (expr_form is not None):\n\t\tsalt.utils.warn_until('Fluorine', \"the\ttarget\ttype\tshould\tbe\tpassed\tusing\tthe\t'tgt_type'\targument\tinstead\tof\t'expr_form'.\tSupport\tfor\tusing\t'expr_form'\twill\tbe\tremoved\tin\tSalt\tFluorine.\")\n\t\ttgt_type = expr_form\n\treturn _talk2modjk(name, lbn, target, 'worker_disable', profile, tgt_type)\n",["disable a launchd service ."]]
["def unexpected_fail_on_npm_install(arg):\n\tif ('npm\tinstall' in arg):\n\t\traise BuildFailure('Subprocess\treturn\tcode:\t50')\n\telse:\n\t\treturn\n",["for our tests ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load if boto is available ."]]
["def prepare_bearer_body(token, body=u''):\n\treturn add_params_to_qs(body, [(u'access_token', token)])\n",["add a bearer token_ to the request body ."]]
["def default_expire_time():\n\texpire_delta = datetime.timedelta(seconds=CONF.token.expiration)\n\treturn (timeutils.utcnow() + expire_delta)\n",["determine when a fresh token should expire ."]]
["def _bytes_feature(value):\n\treturn tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",["wrapper for inserting a bytes feature into a sequenceexample proto ."]]
["def fixed_ip_get_by_host(context, host):\n\treturn IMPL.fixed_ip_get_by_host(context, host)\n",["get fixed ips by compute host ."]]
["def prepare_bearer_uri(token, uri):\n\treturn add_params_to_uri(uri, [(u'access_token', token)])\n",["add a bearer token_ to the request uri ."]]
["def http_date(timestamp=None):\n\treturn _dump_date(timestamp, '\t')\n",["formats the time to match the rfc1123 date format as specified by http rfc2616 section 3 ."]]
["def create_resource():\n\ttask_schema = get_task_schema()\n\tpartial_task_schema = _get_partial_task_schema()\n\tdeserializer = RequestDeserializer(task_schema)\n\tserializer = ResponseSerializer(task_schema, partial_task_schema)\n\tcontroller = TasksController()\n\treturn wsgi.Resource(controller, deserializer, serializer)\n",["stacks resource factory method ."]]
["def onGlobalDataDel(key):\n\tDEBUG_MSG(('onDelGlobalData:\t%s' % key))\n",["kbengine method ."]]
["def onInit(isReload):\n\tDEBUG_MSG(('onInit::isReload:%s' % isReload))\n",["kbengine method ."]]
["def assembleFormattedText(formatted):\n\treturn _textattributes.flatten(formatted, _FormattingState(), 'toMIRCControlCodes')\n",["assemble formatted text from structured information ."]]
["def getNewDerivation(elementNode):\n\treturn evaluate.EmptyObject(elementNode)\n",["get new derivation ."]]
["def onDBMgrShutDown():\n\tINFO_MSG('onDBMgrShutDown()')\n",["kbengine method ."]]
["def urlunquote_plus(quoted_url):\n\treturn force_text(unquote_plus(force_str(quoted_url)))\n",["a wrapper for pythons urllib ."]]
["def is_numlike(obj):\n\ttry:\n\t\t(obj + 1)\n\texcept TypeError:\n\t\treturn False\n\telse:\n\t\treturn True\n",["return true if *obj* looks like a number ."]]
["def safeseq(value):\n\treturn [mark_safe(force_unicode(obj)) for obj in value]\n",["a \"safe\" filter for sequences ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load if boto is available ."]]
["def put(dct, entry):\n\tid = int(entry['id'])\n\tif (id in dct):\n\t\tif (entry == dct[id]):\n\t\t\tpass\n\t\telse:\n\t\t\tprint entry\n\t\t\tprint dct[id]\n\t\t\tassert False\n\telse:\n\t\tdct[id] = entry\n",["sends a put request ."]]
["def cr_uid_ids_context(method):\n\tmethod._api = 'cr_uid_ids_context'\n\treturn method\n",["decorate a traditional-style method that takes cr ."]]
["def init(mpstate):\n\treturn SerialModule(mpstate)\n",["initialise module ."]]
["def test_smote_sk_estimator():\n\tcheck_estimator(SMOTETomek)\n",["test the sklearn estimator compatibility ."]]
["def catalog_item():\n\treturn s3_rest_controller('supply', 'catalog_item', csv_template=('supply', 'catalog_item'), csv_stylesheet=('supply', 'catalog_item.xsl'))\n",["restful crud controller ."]]
["def stats_data():\n\treturn s3_rest_controller()\n",["restful crud controller ."]]
["def init(mpstate):\n\treturn SerialModule(mpstate)\n",["initialise module ."]]
["def incident_type():\n\treturn s3_rest_controller()\n",["restful crud controller ."]]
["def type_():\n\treturn Rebulk().rules(TypeProcessor)\n",["builder for rebulk object ."]]
["def status(name, sig=None):\n\tif sig:\n\t\treturn bool(__salt__['status.pid'](sig))\n\tcmd = '{0}\tcheck\t{1}'.format(_cmd(), name)\n\treturn (not __salt__['cmd.retcode'](cmd))\n",["restful crud controller ."]]
["def plugin():\n\treturn SwapQuotes\n",["make plugin available ."]]
["def getBevelPath(begin, center, close, end, radius):\n\tbeginComplex = begin.dropAxis()\n\tcenterComplex = center.dropAxis()\n\tendComplex = end.dropAxis()\n\tbeginComplexSegmentLength = abs((centerComplex - beginComplex))\n\tendComplexSegmentLength = abs((centerComplex - endComplex))\n\tminimumRadius = lineation.getMinimumRadius(beginComplexSegmentLength, endComplexSegmentLength, radius)\n\tif (minimumRadius <= close):\n\t\treturn [center]\n\tbeginBevel = (center + ((minimumRadius \/ beginComplexSegmentLength) * (begin - center)))\n\tendBevel = (center + ((minimumRadius \/ endComplexSegmentLength) * (end - center)))\n\tif (radius > 0.0):\n\t\treturn [beginBevel, endBevel]\n\tmidpointComplex = (0.5 * (beginBevel.dropAxis() + endBevel.dropAxis()))\n\tspikeComplex = ((centerComplex + centerComplex) - midpointComplex)\n\treturn [beginBevel, Vector3(spikeComplex.real, spikeComplex.imag, center.z), endBevel]\n",["get bevel path ."]]
["def set_permissions(username, permissions, uid=None):\n\tprivileges = {'login': '0x0000001', 'drac': '0x0000002', 'user_management': '0x0000004', 'clear_logs': '0x0000008', 'server_control_commands': '0x0000010', 'console_redirection': '0x0000020', 'virtual_media': '0x0000040', 'test_alerts': '0x0000080', 'debug_commands': '0x0000100'}\n\tpermission = 0\n\tif (uid is None):\n\t\tuser = list_users()\n\t\tuid = user[username]['index']\n\tfor i in permissions.split(','):\n\t\tperm = i.strip()\n\t\tif (perm in privileges):\n\t\t\tpermission += int(privileges[perm], 16)\n\treturn __execute_cmd('config\t-g\tcfgUserAdmin\t-o\t\t\t\t\t\t\t\t\t\t\t\t\tcfgUserAdminPrivilege\t-i\t{0}\t0x{1:08X}'.format(uid, permission))\n",["configure users permissions cli example: ."]]
["def _handle_ns(packageName, path_item):\n\timporter = get_importer(path_item)\n\tif (importer is None):\n\t\treturn None\n\tloader = importer.find_module(packageName)\n\tif (loader is None):\n\t\treturn None\n\tmodule = sys.modules.get(packageName)\n\tif (module is None):\n\t\tmodule = sys.modules[packageName] = types.ModuleType(packageName)\n\t\tmodule.__path__ = []\n\t\t_set_parent_ns(packageName)\n\telif (not hasattr(module, '__path__')):\n\t\traise TypeError('Not\ta\tpackage:', packageName)\n\thandler = _find_adapter(_namespace_handlers, importer)\n\tsubpath = handler(importer, path_item, packageName, module)\n\tif (subpath is not None):\n\t\tpath = module.__path__\n\t\tpath.append(subpath)\n\t\tloader.load_module(packageName)\n\t\tsys_path = [((p and _normalize_cached(p)) or p) for p in sys.path]\n\t\tdef sort_key(p):\n\t\t\tparts = p.split(os.sep)\n\t\t\tparts = parts[:(- (packageName.count('.') + 1))]\n\t\t\treturn sys_path.index(_normalize_cached(os.sep.join(parts)))\n\t\tpath.sort(key=sort_key)\n\t\tmodule.__path__[:] = [_normalize_cached(p) for p in path]\n\treturn subpath\n",["ensure that named package includes a subpath of path_item ."]]
["def break_around_binary_operator(logical_line, tokens):\n\tdef is_binary_operator(token_type, text):\n\t\treturn (((token_type == tokenize.OP) or (text in ['and', 'or'])) and (text not in '()[]{},:.;@=%'))\n\tline_break = False\n\tunary_context = True\n\tfor (token_type, text, start, end, line) in tokens:\n\t\tif (token_type == tokenize.COMMENT):\n\t\t\tcontinue\n\t\tif ((('\\n' in text) or ('\\r' in text)) and (token_type != tokenize.STRING)):\n\t\t\tline_break = True\n\t\telse:\n\t\t\tif (is_binary_operator(token_type, text) and line_break and (not unary_context)):\n\t\t\t\t(yield (start, 'W503\tline\tbreak\tbefore\tbinary\toperator'))\n\t\t\tunary_context = (text in '([{,;')\n\t\t\tline_break = False\n",["avoid breaks before binary operators ."]]
["def send_bulk_mail(sender_email, recipient_emails, subject, plaintext_body, html_body):\n\tif (not feconf.CAN_SEND_EMAILS):\n\t\traise Exception('This\tapp\tcannot\tsend\temails.')\n\tif (not mail.is_email_valid(sender_email)):\n\t\traise ValueError(('Malformed\tsender\temail\taddress:\t%s' % sender_email))\n\tfor recipient_email in recipient_emails:\n\t\tif (not mail.is_email_valid(recipient_email)):\n\t\t\traise ValueError(('Malformed\trecipient\temail\taddress:\t%s' % recipient_email))\n\tfor recipient_email in recipient_emails:\n\t\tmail.send_mail(sender_email, recipient_email, subject, plaintext_body, html=html_body)\n",["sends an email using mailgun api ."]]
["def addToProfileMenu(profileSelection, profileType, repository):\n\tpluginFileNames = skeinforge_profile.getPluginFileNames()\n\tcraftTypeName = skeinforge_profile.getCraftTypeName()\n\tpluginModule = skeinforge_profile.getCraftTypePluginModule()\n\tprofilePluginSettings = settings.getReadRepository(pluginModule.getNewRepository())\n\tfor pluginFileName in pluginFileNames:\n\t\tskeinforge_profile.ProfileTypeMenuRadio().getFromMenuButtonDisplay(profileType, pluginFileName, repository, (craftTypeName == pluginFileName))\n\tfor profileName in profilePluginSettings.profileList.value:\n\t\tskeinforge_profile.ProfileSelectionMenuRadio().getFromMenuButtonDisplay(profileSelection, profileName, repository, (profileName == profilePluginSettings.profileListbox.value))\n",["add a profile menu ."]]
["def blank_lines(logical_line, blank_lines, indent_level, line_number, previous_logical, previous_indent_level):\n\tif ((line_number < 3) and (not previous_logical)):\n\t\treturn\n\tif previous_logical.startswith('@'):\n\t\tif blank_lines:\n\t\t\t(yield (0, 'E304\tblank\tlines\tfound\tafter\tfunction\tdecorator'))\n\telif ((blank_lines > 2) or (indent_level and (blank_lines == 2))):\n\t\t(yield (0, ('E303\ttoo\tmany\tblank\tlines\t(%d)' % blank_lines)))\n\telif logical_line.startswith(('def\t', 'class\t', '@')):\n\t\tif indent_level:\n\t\t\tif (not (blank_lines or (previous_indent_level < indent_level) or DOCSTRING_REGEX.match(previous_logical))):\n\t\t\t\t(yield (0, 'E301\texpected\t1\tblank\tline,\tfound\t0'))\n\t\telif (blank_lines != 2):\n\t\t\t(yield (0, ('E302\texpected\t2\tblank\tlines,\tfound\t%d' % blank_lines)))\n",["separate top-level function and class definitions with two blank lines ."]]
["def put_website(Bucket, ErrorDocument=None, IndexDocument=None, RedirectAllRequestsTo=None, RoutingRules=None, region=None, key=None, keyid=None, profile=None):\n\ttry:\n\t\tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\t\tWebsiteConfiguration = {}\n\t\tfor key in ('ErrorDocument', 'IndexDocument', 'RedirectAllRequestsTo', 'RoutingRules'):\n\t\t\tval = locals()[key]\n\t\t\tif (val is not None):\n\t\t\t\tif isinstance(val, six.string_types):\n\t\t\t\t\tWebsiteConfiguration[key] = json.loads(val)\n\t\t\t\telse:\n\t\t\t\t\tWebsiteConfiguration[key] = val\n\t\tconn.put_bucket_website(Bucket=Bucket, WebsiteConfiguration=WebsiteConfiguration)\n\t\treturn {'updated': True, 'name': Bucket}\n\texcept ClientError as e:\n\t\treturn {'updated': False, 'error': __utils__['boto3.get_error'](e)}\n",["given a valid config ."]]
["def learn_cache_key(request, response, cache_timeout=None, key_prefix=None, cache=None):\n\tif (key_prefix is None):\n\t\tkey_prefix = settings.CACHE_MIDDLEWARE_KEY_PREFIX\n\tif (cache_timeout is None):\n\t\tcache_timeout = settings.CACHE_MIDDLEWARE_SECONDS\n\tcache_key = _generate_cache_header_key(key_prefix, request)\n\tif (cache is None):\n\t\tcache = get_cache(settings.CACHE_MIDDLEWARE_ALIAS)\n\tif response.has_header('Vary'):\n\t\theaderlist = [('HTTP_' + header.upper().replace('-', '_')) for header in cc_delim_re.split(response['Vary'])]\n\t\tcache.set(cache_key, headerlist, cache_timeout)\n\t\treturn _generate_cache_key(request, request.method, headerlist, key_prefix)\n\telse:\n\t\tcache.set(cache_key, [], cache_timeout)\n\t\treturn _generate_cache_key(request, request.method, [], key_prefix)\n",["learns what headers to take into account for some request path from the response object ."]]
["def test_finder_installs_pre_releases_with_version_spec():\n\treq = InstallRequirement.from_line('bar>=0.0.dev0', None)\n\tlinks = ['https:\/\/foo\/bar-1.0.tar.gz', 'https:\/\/foo\/bar-2.0b1.tar.gz']\n\tfinder = PackageFinder(links, [], session=PipSession())\n\twith patch.object(finder, '_get_pages', (lambda x, y: [])):\n\t\tlink = finder.find_requirement(req, False)\n\t\tassert (link.url == 'https:\/\/foo\/bar-2.0b1.tar.gz')\n\tlinks.reverse()\n\tfinder = PackageFinder(links, [], session=PipSession())\n\twith patch.object(finder, '_get_pages', (lambda x, y: [])):\n\t\tlink = finder.find_requirement(req, False)\n\t\tassert (link.url == 'https:\/\/foo\/bar-2.0b1.tar.gz')\n",["test packagefinder only accepts stable versioned releases by default ."]]
["def get_hostname(config=None):\n\thostname = None\n\tif (config is None):\n\t\tfrom config import get_config\n\t\tconfig = get_config(parse_args=True)\n\tconfig_hostname = config.get('hostname')\n\tif (config_hostname and is_valid_hostname(config_hostname)):\n\t\treturn config_hostname\n\tgce_hostname = GCE.get_hostname(config)\n\tif (gce_hostname is not None):\n\t\tif is_valid_hostname(gce_hostname):\n\t\t\treturn gce_hostname\n\tif Platform.is_containerized():\n\t\tdocker_util = DockerUtil()\n\t\tdocker_hostname = docker_util.get_hostname(use_default_gw=False)\n\t\tif ((docker_hostname is not None) and is_valid_hostname(docker_hostname)):\n\t\t\thostname = docker_hostname\n\t\telif Platform.is_k8s():\n\t\t\tkube_util = KubeUtil()\n\t\t\t(_, kube_hostname) = kube_util.get_node_info()\n\t\t\tif ((kube_hostname is not None) and is_valid_hostname(kube_hostname)):\n\t\t\t\thostname = kube_hostname\n\tif (hostname is None):\n\t\tif (Platform.is_unix() or Platform.is_solaris()):\n\t\t\tunix_hostname = _get_hostname_unix()\n\t\t\tif (unix_hostname and is_valid_hostname(unix_hostname)):\n\t\t\t\thostname = unix_hostname\n\tif (Platform.is_ecs_instance() or ((hostname is not None) and EC2.is_default(hostname))):\n\t\tinstanceid = EC2.get_instance_id(config)\n\t\tif instanceid:\n\t\t\thostname = instanceid\n\tif (hostname is None):\n\t\ttry:\n\t\t\tsocket_hostname = socket.gethostname()\n\t\texcept socket.error:\n\t\t\tsocket_hostname = None\n\t\tif (socket_hostname and is_valid_hostname(socket_hostname)):\n\t\t\thostname = socket_hostname\n\tif (hostname is None):\n\t\tlog.critical('Unable\tto\treliably\tdetermine\thost\tname.\tYou\tcan\tdefine\tone\tin\tdatadog.conf\tor\tin\tyour\thosts\tfile')\n\t\traise Exception('Unable\tto\treliably\tdetermine\thost\tname.\tYou\tcan\tdefine\tone\tin\tdatadog.conf\tor\tin\tyour\thosts\tfile')\n\treturn hostname\n",["returns a hostname to use to contact the module ."]]
["def _ipconfig_getnode():\n\timport os, re\n\tdirs = ['', 'c:\\\\windows\\\\system32', 'c:\\\\winnt\\\\system32']\n\ttry:\n\t\timport ctypes\n\t\tbuffer = ctypes.create_string_buffer(300)\n\t\tctypes.windll.kernel32.GetSystemDirectoryA(buffer, 300)\n\t\tdirs.insert(0, buffer.value.decode('mbcs'))\n\texcept:\n\t\tpass\n\tfor dir in dirs:\n\t\ttry:\n\t\t\tpipe = os.popen((os.path.join(dir, 'ipconfig') + '\t\/all'))\n\t\texcept IOError:\n\t\t\tcontinue\n\t\tfor line in pipe:\n\t\t\tvalue = line.split(':')[(-1)].strip().lower()\n\t\t\tif re.match('([0-9a-f][0-9a-f]-){5}[0-9a-f][0-9a-f]', value):\n\t\t\t\treturn int(value.replace('-', ''), 16)\n",["get the hardware address on windows by running ipconfig ."]]
["def main():\n\terrors = 0\n\tfits_files = handle_options(sys.argv[1:])\n\tsetup_logging()\n\tfor filename in fits_files:\n\t\terrors += process_file(filename)\n\tif errors:\n\t\tlog.warning('{}\terrors'.format(errors))\n\treturn int(bool(errors))\n",["run the main command-line interface for beets ."]]
["def limited(items, request, max_limit=None):\n\tmax_limit = (max_limit or CONF.osapi_max_limit)\n\t(marker, limit, offset) = get_pagination_params(request.GET.copy(), max_limit)\n\trange_end = (offset + (limit or max_limit))\n\treturn items[offset:range_end]\n",["return a slice of items according to requested offset and limit ."]]
["def emboss_piped_AlignIO_convert(alignments, old_format, new_format):\n\tcline = SeqretCommandline(exes['seqret'], sformat=old_format, osformat=new_format, auto=True, filter=True)\n\tchild = subprocess.Popen(str(cline), stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True, shell=(sys.platform != 'win32'))\n\ttry:\n\t\tAlignIO.write(alignments, child.stdin, old_format)\n\texcept Exception as err:\n\t\tchild.stdin.close()\n\t\tchild.stderr.close()\n\t\tchild.stdout.close()\n\t\traise\n\tchild.stdin.close()\n\tchild.stderr.close()\n\ttry:\n\t\taligns = list(AlignIO.parse(child.stdout, new_format))\n\texcept Exception as err:\n\t\tchild.stdout.close()\n\t\traise\n\tchild.stdout.close()\n\treturn aligns\n",["run seqret ."]]
["def _setwindowposition(folder_alias, (x, y)):\n\tfinder = _getfinder()\n\targs = {}\n\tattrs = {}\n\taeobj_0 = aetypes.ObjectSpecifier(want=aetypes.Type('cfol'), form='alis', seld=folder_alias, fr=None)\n\taeobj_1 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('cwnd'), fr=aeobj_0)\n\taeobj_2 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('posn'), fr=aeobj_1)\n\targs['----'] = aeobj_2\n\targs['data'] = [x, y]\n\t(_reply, args, attrs) = finder.send('core', 'setd', args, attrs)\n\tif ('errn' in args):\n\t\traise Error, aetools.decodeerror(args)\n\tif ('----' in args):\n\t\treturn args['----']\n",["set the size of a finder window for folder to ."]]
["def parse_query_parts(parts, model_cls):\n\tprefixes = {':': dbcore.query.RegexpQuery}\n\tprefixes.update(plugins.queries())\n\tif ('path' in model_cls._fields):\n\t\tpath_parts = []\n\t\tnon_path_parts = []\n\t\tfor s in parts:\n\t\t\tif (s.find(os.sep, 0, s.find(':')) != (-1)):\n\t\t\t\tpath_parts.append(s)\n\t\t\telse:\n\t\t\t\tnon_path_parts.append(s)\n\telse:\n\t\tpath_parts = ()\n\t\tnon_path_parts = parts\n\t(query, sort) = dbcore.parse_sorted_query(model_cls, non_path_parts, prefixes)\n\tif path_parts:\n\t\tquery.subqueries += [PathQuery('path', s) for s in path_parts]\n\treturn (query, sort)\n",["given a beets query string as a list of components ."]]
["def b58decode(v, length):\n\tlong_value = 0L\n\tfor (i, c) in enumerate(v[::(-1)]):\n\t\tlong_value += (__b58chars.find(c) * (__b58base ** i))\n\tresult = ''\n\twhile (long_value >= 256):\n\t\t(div, mod) = divmod(long_value, 256)\n\t\tresult = (chr(mod) + result)\n\t\tlong_value = div\n\tresult = (chr(long_value) + result)\n\tnPad = 0\n\tfor c in v:\n\t\tif (c == __b58chars[0]):\n\t\t\tnPad += 1\n\t\telse:\n\t\t\tbreak\n\tresult = ((chr(0) * nPad) + result)\n\tif ((length is not None) and (len(result) != length)):\n\t\treturn None\n\treturn result\n",["decode v into a string of len bytes ."]]
["def get_images_table(meta):\n\t(get_images_table,) = from_migration_import('008_add_image_members_table', ['get_images_table'])\n\timages = get_images_table(meta)\n\treturn images\n",["returns the table object for the images table that corresponds to the images table definition of this version ."]]
["def make_runserver(app_factory, hostname='localhost', port=5000, use_reloader=False, use_debugger=False, use_evalex=True, threaded=False, processes=1, static_files=None, extra_files=None, ssl_context=None):\n\t_deprecated()\n\tdef action(hostname=('h', hostname), port=('p', port), reloader=use_reloader, debugger=use_debugger, evalex=use_evalex, threaded=threaded, processes=processes):\n\t\t'Start\ta\tnew\tdevelopment\tserver.'\n\t\tfrom werkzeug.serving import run_simple\n\t\tapp = app_factory()\n\t\trun_simple(hostname, port, app, use_reloader=reloader, use_debugger=debugger, use_evalex=evalex, extra_files=extra_files, reloader_interval=1, threaded=threaded, processes=processes, static_files=static_files, ssl_context=ssl_context)\n\treturn action\n",["returns an action callback that spawns a new development server ."]]
["def config_dict(handle, conf_mappings, handler=None):\n\tselected_config = get_config(handle)\n\tselected_config.add_listener(_SyncListener(conf_mappings, handler).update)\n\treturn conf_mappings\n",["convert content of config-file into dictionary ."]]
["def get_root_path(import_name):\n\tmod = sys.modules.get(import_name)\n\tif ((mod is not None) and hasattr(mod, '__file__')):\n\t\treturn os.path.dirname(os.path.abspath(mod.__file__))\n\tloader = pkgutil.get_loader(import_name)\n\tif ((loader is None) or (import_name == '__main__')):\n\t\treturn os.getcwd()\n\tif hasattr(loader, 'get_filename'):\n\t\tfilepath = loader.get_filename(import_name)\n\telse:\n\t\t__import__(import_name)\n\t\tmod = sys.modules[import_name]\n\t\tfilepath = getattr(mod, '__file__', None)\n\t\tif (filepath is None):\n\t\t\traise RuntimeError(('No\troot\tpath\tcan\tbe\tfound\tfor\tthe\tprovided\tmodule\t\"%s\".\t\tThis\tcan\thappen\tbecause\tthe\tmodule\tcame\tfrom\tan\timport\thook\tthat\tdoes\tnot\tprovide\tfile\tname\tinformation\tor\tbecause\tit\\'s\ta\tnamespace\tpackage.\t\tIn\tthis\tcase\tthe\troot\tpath\tneeds\tto\tbe\texplicitly\tprovided.' % import_name))\n\treturn os.path.dirname(os.path.abspath(filepath))\n",["returns the path to a package or cwd if that cannot be found ."]]
["def was_modified_since(header=None, mtime=0, size=0):\n\ttry:\n\t\tif (header is None):\n\t\t\traise ValueError\n\t\tmatches = re.match('^([^;]+)(;\tlength=([0-9]+))?$', header, re.IGNORECASE)\n\t\theader_mtime = parse_http_date(matches.group(1))\n\t\theader_len = matches.group(3)\n\t\tif (header_len and (int(header_len) != size)):\n\t\t\traise ValueError\n\t\tif (mtime > header_mtime):\n\t\t\traise ValueError\n\texcept (AttributeError, ValueError, OverflowError):\n\t\treturn True\n\treturn False\n",["was something modified since the user last downloaded it? header this is the value of the if-modified-since header ."]]
["def copyfileobj(fsrc, fdst, length=(64 * 1024)):\n\twhile True:\n\t\tbuf = fsrc.read(length)\n\t\tif (not buf):\n\t\t\tbreak\n\t\tfdst.write(buf)\n",["copy length bytes from fileobj src to fileobj dst ."]]
["def get_crawl_args(message):\n\tmsg = message.copy()\n\targs = [_to_native_str(msg['_spider'])]\n\tdel msg['_project'], msg['_spider']\n\tsettings = msg.pop('settings', {})\n\tfor (k, v) in native_stringify_dict(msg, keys_only=False).items():\n\t\targs += ['-a']\n\t\targs += [('%s=%s' % (k, v))]\n\tfor (k, v) in native_stringify_dict(settings, keys_only=False).items():\n\t\targs += ['-s']\n\t\targs += [('%s=%s' % (k, v))]\n\treturn args\n",["return the command-line arguments to use for the scrapy crawl process that will be started for this message ."]]
["def test_url_completion_delete_bookmark(qtmodeltester, config_stub, web_history, quickmarks, bookmarks, qtbot):\n\tconfig_stub.data['completion'] = {'timestamp-format': '%Y-%m-%d', 'web-history-max-items': 2}\n\tmodel = urlmodel.UrlCompletionModel()\n\tqtmodeltester.data_display_may_return_none = True\n\tqtmodeltester.check(model)\n\tview = _mock_view_index(model, 1, 0, qtbot)\n\tmodel.delete_cur_item(view)\n\tassert ('https:\/\/github.com' not in bookmarks.marks)\n\tassert ('https:\/\/python.org' in bookmarks.marks)\n\tassert ('http:\/\/qutebrowser.org' in bookmarks.marks)\n",["test deleting a bookmark from the url completion model ."]]
["def positional(max_positional_args):\n\tdef positional_decorator(wrapped):\n\t\t@functools.wraps(wrapped)\n\t\tdef positional_wrapper(*args, **kwargs):\n\t\t\tif (len(args) > max_positional_args):\n\t\t\t\tplural_s = ''\n\t\t\t\tif (max_positional_args != 1):\n\t\t\t\t\tplural_s = 's'\n\t\t\t\tmessage = ('%s()\ttakes\tat\tmost\t%d\tpositional\targument%s\t(%d\tgiven)' % (wrapped.__name__, max_positional_args, plural_s, len(args)))\n\t\t\t\tif (positional_parameters_enforcement == POSITIONAL_EXCEPTION):\n\t\t\t\t\traise TypeError(message)\n\t\t\t\telif (positional_parameters_enforcement == POSITIONAL_WARNING):\n\t\t\t\t\tlogger.warning(message)\n\t\t\t\telse:\n\t\t\t\t\tpass\n\t\t\treturn wrapped(*args, **kwargs)\n\t\treturn positional_wrapper\n\tif isinstance(max_positional_args, six.integer_types):\n\t\treturn positional_decorator\n\telse:\n\t\t(args, _, _, defaults) = inspect.getargspec(max_positional_args)\n\t\treturn positional((len(args) - len(defaults)))(max_positional_args)\n",["a decorator to declare that only the first n arguments may be positional ."]]
["def parse_accept_lang_header(lang_string):\n\tresult = []\n\tpieces = accept_language_re.split(lang_string)\n\tif pieces[(-1)]:\n\t\treturn []\n\tfor i in range(0, (len(pieces) - 1), 3):\n\t\t(first, lang, priority) = pieces[i:(i + 3)]\n\t\tif first:\n\t\t\treturn []\n\t\tpriority = ((priority and float(priority)) or 1.0)\n\t\tresult.append((lang, priority))\n\tresult.sort((lambda x, y: (- cmp(x[1], y[1]))))\n\treturn result\n",["parses the lang_string ."]]
["def send_message(to, text, sender=None):\n\tif sender:\n\t\tmsg = OutboxMessage.objects.create(sender=sender, message=text)\n\t\tmsg.to.add(*to)\n\tfor user in to:\n\t\tim = InboxMessage.objects.create(sender=sender, to=user, message=text)\n\t\tif Setting.get_for_user(user, 'email_private_messages'):\n\t\t\temail_private_message(inbox_message_id=im.id)\n\tmessage_sent.send(sender=InboxMessage, to=to, text=text, msg_sender=sender)\n",["send a message to a hipchat room ."]]
["def _get_system_version():\n\tglobal _SYSTEM_VERSION\n\tif (_SYSTEM_VERSION is None):\n\t\t_SYSTEM_VERSION = ''\n\t\ttry:\n\t\t\tf = open('\/System\/Library\/CoreServices\/SystemVersion.plist')\n\t\texcept OSError:\n\t\t\tpass\n\t\telse:\n\t\t\ttry:\n\t\t\t\tm = re.search('<key>ProductUserVisibleVersion<\/key>\\\\s*<string>(.*?)<\/string>', f.read())\n\t\t\tfinally:\n\t\t\t\tf.close()\n\t\t\tif (m is not None):\n\t\t\t\t_SYSTEM_VERSION = '.'.join(m.group(1).split('.')[:2])\n\treturn _SYSTEM_VERSION\n",["return the os x system version as a string ."]]
["def getTetragridTimesOther(firstTetragrid, otherTetragrid):\n\ttetragridTimesOther = []\n\tfor row in xrange(4):\n\t\tmatrixRow = firstTetragrid[row]\n\t\ttetragridTimesOtherRow = []\n\t\ttetragridTimesOther.append(tetragridTimesOtherRow)\n\t\tfor column in xrange(4):\n\t\t\tdotProduct = 0\n\t\t\tfor elementIndex in xrange(4):\n\t\t\t\tdotProduct += (matrixRow[elementIndex] * otherTetragrid[elementIndex][column])\n\t\t\ttetragridTimesOtherRow.append(dotProduct)\n\treturn tetragridTimesOther\n",["get this matrix multiplied by the other matrix ."]]
["def Deserializer(stream_or_string, **options):\n\tif isinstance(stream_or_string, basestring):\n\t\tstream = StringIO(stream_or_string)\n\telse:\n\t\tstream = stream_or_string\n\tfor obj in PythonDeserializer(yaml.load(stream), **options):\n\t\t(yield obj)\n",["deserialize a stream or string of yaml data ."]]
["def asynchronous(function):\n\targ_names = inspect.getargspec(function).args\n\tMessageData = collections.namedtuple(function.__name__, arg_names[1:])\n\t@functools.wraps(function)\n\tdef call_or_send(processor, *args, **kwargs):\n\t\tif ((len(args) == 1) and (not kwargs) and isinstance(args[0], MessageData)):\n\t\t\ttry:\n\t\t\t\treturn function(processor, **args[0]._asdict())\n\t\t\texcept Exception as exc:\n\t\t\t\tLOG.exception('[%s]\tException\tin\t\"%s\":\t%s', processor.name, function.__name__, exc)\n\t\t\t\traise\n\t\telse:\n\t\t\tdata = inspect.getcallargs(function, processor, *args, **kwargs)\n\t\t\tdata.pop(arg_names[0])\n\t\t\treturn processor.queue.send(function.__name__, MessageData(**data))\n\tcall_or_send.MessageData = MessageData\n\treturn call_or_send\n",["wrap request handler methods with this if they are asynchronous ."]]
["def main():\n\terrors = 0\n\tfits_files = handle_options(sys.argv[1:])\n\tsetup_logging()\n\tfor filename in fits_files:\n\t\terrors += process_file(filename)\n\tif errors:\n\t\tlog.warning('{}\terrors'.format(errors))\n\treturn int(bool(errors))\n",["the main runner function ."]]
["def load():\n\tdata = _get_data()\n\treturn du.process_recarray(data, endog_idx=0, dtype=float)\n",["deserialize fp (a ."]]
["def addClosedXMLTag(attributes, depth, localName, output, text=''):\n\tdepthStart = (' DCTB ' * depth)\n\tattributesString = getAttributesString(attributes)\n\tif (len(text) > 0):\n\t\toutput.write(('%s<%s%s\t>%s<\/%s>\\n' % (depthStart, localName, attributesString, text, localName)))\n\telse:\n\t\toutput.write(('%s<%s%s\t\/>\\n' % (depthStart, localName, attributesString)))\n",["add the closed xml tag ."]]
["def get_kinds(start=None, end=None):\n\tq = Kind.query()\n\tif ((start is not None) and (start != '')):\n\t\tq = q.filter((Kind.key >= Kind.key_for_kind(start)))\n\tif (end is not None):\n\t\tif (end == ''):\n\t\t\treturn []\n\t\tq = q.filter((Kind.key < Kind.key_for_kind(end)))\n\treturn [x.kind_name for x in q]\n",["return all kinds in the specified range ."]]
["def test_write_twoline_no_bookend():\n\tout = StringIO()\n\tascii.write(dat, out, Writer=ascii.FixedWidthTwoLine, bookend=True, delimiter='|')\n\tassert_equal_splitlines(out.getvalue(), '|Col1|\t\t\t\t\tCol2|Col3|Col4|\\n|----|---------|----|----|\\n|\t1.2|\t\t\"hello\"|\t\t\t1|\t\t\ta|\\n|\t2.4|\\'s\tworlds|\t\t\t2|\t\t\t2|\\n')\n",["write a table as a fixed width table with no bookend ."]]
["def _NewIndexFromIndexSpecPb(index_spec_pb):\n\tsource = _SOURCE_PB_TO_SOURCES_MAP.get(index_spec_pb.source())\n\tindex = None\n\tif index_spec_pb.has_namespace():\n\t\tindex = Index(name=index_spec_pb.name(), namespace=index_spec_pb.namespace(), source=source)\n\telse:\n\t\tindex = Index(name=index_spec_pb.name(), source=source)\n\treturn index\n",["creates an index from a search_service_pb ."]]
["def list_subscriptions(document_class, sub_id_start='', topic=None, max_results=DEFAULT_LIST_SUBSCRIPTIONS_MAX_RESULTS, expires_before=None):\n\tfrom google.appengine.ext import db\n\tif issubclass(document_class, db.Model):\n\t\ttopic = _get_document_topic(document_class, topic)\n\telif issubclass(document_class, datastore.Entity):\n\t\tif (not topic):\n\t\t\traise TopicNotSpecified()\n\telse:\n\t\traise DocumentTypeError()\n\treturn prospective_search_admin.list_subscriptions(topic, max_results, None, sub_id_start, expires_before)\n",["list subscriptions on a topic ."]]
["def split_named_range(range_string):\n\tfor range_string in SPLIT_NAMED_RANGE_RE.split(range_string)[1::2]:\n\t\tmatch = NAMED_RANGE_RE.match(range_string)\n\t\tif (match is None):\n\t\t\traise NamedRangeException(('Invalid\tnamed\trange\tstring:\t\"%s\"' % range_string))\n\t\telse:\n\t\t\tmatch = match.groupdict()\n\t\t\tsheet_name = (match['quoted'] or match['notquoted'])\n\t\t\txlrange = match['range']\n\t\t\tsheet_name = sheet_name.replace(\"''\", \"'\")\n\t\t\t(yield (sheet_name, xlrange))\n",["separate a named range into its component parts ."]]
["def test_sobel_h_horizontal():\n\t(i, j) = np.mgrid[(-5):6, (-5):6]\n\timage = (i >= 0).astype(float)\n\tresult = filters.sobel_h(image)\n\ti[(np.abs(j) == 5)] = 10000\n\tassert np.all((result[(i == 0)] == 1))\n\tassert np.all((result[(np.abs(i) > 1)] == 0))\n",["horizontal sobel on an edge should be a horizontal line ."]]
["def apropos(key):\n\tdef callback(path, modname, desc):\n\t\tif (modname[(-9):] == '.__init__'):\n\t\t\tmodname = (modname[:(-9)] + '\t(package)')\n\t\tprint modname, (desc and ('-\t' + desc))\n\tdef onerror(modname):\n\t\tpass\n\twith warnings.catch_warnings():\n\t\twarnings.filterwarnings('ignore')\n\t\tModuleScanner().run(callback, key, onerror=onerror)\n",["print all the one-line module summaries that contain a substring ."]]
["def get_random_string(length=12, allowed_chars=u'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'):\n\tif (not using_sysrandom):\n\t\trandom.seed(hashlib.sha256((u'%s%s%s' % (random.getstate(), time.time(), settings.SECRET_KEY)).encode(u'utf-8')).digest())\n\treturn u''.join([random.choice(allowed_chars) for i in range(length)])\n",["returns a securely generated random string ."]]
["def _getAccessibleAttribute(attributeName, elementNode):\n\tif (attributeName in globalGetAccessibleAttributeSet):\n\t\treturn getattr(Document(elementNode), attributeName, None)\n\treturn None\n",["get the accessible attribute ."]]
["def cov_hc3(results):\n\th = np.diag(np.dot(results.model.exog, np.dot(results.normalized_cov_params, results.model.exog.T)))\n\thet_scale = ((results.resid \/ (1 - h)) ** 2)\n\tcov_hc3_ = _HCCM(results, het_scale)\n\treturn cov_hc3_\n",["see statsmodels ."]]
["@register.filter('slice', is_safe=True)\ndef slice_filter(value, arg):\n\ttry:\n\t\tbits = []\n\t\tfor x in arg.split(':'):\n\t\t\tif (len(x) == 0):\n\t\t\t\tbits.append(None)\n\t\t\telse:\n\t\t\t\tbits.append(int(x))\n\t\treturn value[slice(*bits)]\n\texcept (ValueError, TypeError):\n\t\treturn value\n",["returns a slice of the list ."]]
["def webob_factory(url):\n\tbase_url = url\n\tdef web_request(url, method=None, body=None):\n\t\treq = webob.Request.blank(('%s%s' % (base_url, url)))\n\t\tif method:\n\t\t\treq.content_type = 'application\/json'\n\t\t\treq.method = method\n\t\tif body:\n\t\t\treq.body = jsonutils.dumps(body)\n\t\treturn req\n\treturn web_request\n",["factory for removing duplicate webob code from tests ."]]
["def newer_pairwise(sources, targets):\n\tif (len(sources) != len(targets)):\n\t\traise ValueError, \"'sources'\tand\t'targets'\tmust\tbe\tsame\tlength\"\n\tn_sources = []\n\tn_targets = []\n\tfor i in range(len(sources)):\n\t\tif newer(sources[i], targets[i]):\n\t\t\tn_sources.append(sources[i])\n\t\t\tn_targets.append(targets[i])\n\treturn (n_sources, n_targets)\n",["walk two filename lists in parallel ."]]
["def reparam(string_, dictionary):\n\tdictionary = dictionary.copy()\n\tresult = []\n\tfor (live, chunk) in _interpolate(string_):\n\t\tif live:\n\t\t\tv = eval(chunk, dictionary)\n\t\t\tresult.append(sqlquote(v))\n\t\telse:\n\t\t\tresult.append(chunk)\n\treturn SQLQuery.join(result, '')\n",["takes a string and a dictionary and interpolates the string using values from the dictionary ."]]
["def test_allknn_sample_wrong_X():\n\tallknn = AllKNN(random_state=RND_SEED)\n\tallknn.fit(X, Y)\n\tassert_raises(RuntimeError, allknn.sample, np.random.random((100, 40)), np.array((([0] * 50) + ([1] * 50))))\n",["test either if an error is raised when x is different at fitting and sampling ."]]
["def get_hasher(algorithm=u'default'):\n\tif hasattr(algorithm, u'algorithm'):\n\t\treturn algorithm\n\telif (algorithm == u'default'):\n\t\tif (PREFERRED_HASHER is None):\n\t\t\tload_hashers()\n\t\treturn PREFERRED_HASHER\n\telse:\n\t\tif (HASHERS is None):\n\t\t\tload_hashers()\n\t\tif (algorithm not in HASHERS):\n\t\t\traise ValueError((u\"Unknown\tpassword\thashing\talgorithm\t'%s'.\tDid\tyou\tspecify\tit\tin\tthe\tPASSWORD_HASHERS\tsetting?\" % algorithm))\n\t\treturn HASHERS[algorithm]\n",["returns an instance of a loaded password hasher ."]]
["def getcaps():\n\tcaps = {}\n\tfor mailcap in listmailcapfiles():\n\t\ttry:\n\t\t\tfp = open(mailcap, 'r')\n\t\texcept IOError:\n\t\t\tcontinue\n\t\twith fp:\n\t\t\tmorecaps = readmailcapfile(fp)\n\t\tfor (key, value) in morecaps.iteritems():\n\t\t\tif (not (key in caps)):\n\t\t\t\tcaps[key] = value\n\t\t\telse:\n\t\t\t\tcaps[key] = (caps[key] + value)\n\treturn caps\n",["return a dictionary containing the mailcap database ."]]
["def nsmallest(arr, n, keep='first'):\n\tif (keep == 'last'):\n\t\tarr = arr[::(-1)]\n\tnarr = len(arr)\n\tn = min(n, narr)\n\tsdtype = str(arr.dtype)\n\tarr = arr.view(_dtype_map.get(sdtype, sdtype))\n\tkth_val = algos.kth_smallest(arr.copy(), (n - 1))\n\treturn _finalize_nsmallest(arr, kth_val, n, keep, narr)\n",["find the n smallest elements in a dataset ."]]
["def tag(accessing_obj, accessed_obj, *args, **kwargs):\n\tif hasattr(accessing_obj, 'obj'):\n\t\taccessing_obj = accessing_obj.obj\n\ttagkey = (args[0] if args else None)\n\tcategory = (args[1] if (len(args) > 1) else None)\n\treturn accessing_obj.tags.get(tagkey, category=category)\n",["returns a list of -tuples from the given string ."]]
["def acl_group_add_hosts(id, hosts):\n\tgroup = models.AclGroup.smart_get(id)\n\tgroup.check_for_acl_violation_acl_group()\n\thosts = models.Host.smart_get_bulk(hosts)\n\tgroup.hosts.add(*hosts)\n\tgroup.on_host_membership_change()\n",["add hosts to an acl group ."]]
["def _image_entropy(img):\n\thist = img.histogram()\n\thist_size = sum(hist)\n\thist = [(float(h) \/ hist_size) for h in hist]\n\treturn (- sum(((p * math.log(p, 2)) for p in hist if (p != 0))))\n",["calculate the entropy of an image ."]]
["@xfail_py3\n@xfail(wxPython_fail, reason='Unsupported\twxPython\tversion')\n@importorskip('wx.lib.pubsub.core')\ndef test_wx_lib_pubsub_protocol_arg1(pyi_builder):\n\tpyi_builder.test_script('pyi_hooks\/wx_lib_pubsub_setuparg1.py')\n",["functional test specific to version 3 of the pypubsub api ."]]
["def DocFileSuite(*paths, **kw):\n\tsuite = unittest.TestSuite()\n\tif kw.get('module_relative', True):\n\t\tkw['package'] = _normalize_module(kw.get('package'))\n\tfor path in paths:\n\t\tsuite.addTest(DocFileTest(path, **kw))\n\treturn suite\n",["a unittest suite for one or more doctest files ."]]
["def get_object_or_404(queryset, *filter_args, **filter_kwargs):\n\ttry:\n\t\treturn _get_object_or_404(queryset, *filter_args, **filter_kwargs)\n\texcept (TypeError, ValueError):\n\t\traise Http404\n",["uses get() to return an object ."]]
["def cooperative_iter(iter):\n\ttry:\n\t\tfor chunk in iter:\n\t\t\tsleep(0)\n\t\t\t(yield chunk)\n\texcept Exception as err:\n\t\twith excutils.save_and_reraise_exception():\n\t\t\tmsg = (_LE('Error:\tcooperative_iter\texception\t%s') % err)\n\t\t\tLOG.error(msg)\n",["return an iterator which schedules after each iteration ."]]
["def _get_md5(name, path, run_func):\n\toutput = run_func(name, 'md5sum\t{0}'.format(pipes.quote(path)), ignore_retcode=True)['stdout']\n\ttry:\n\t\treturn output.split()[0]\n\texcept IndexError:\n\t\treturn None\n",["get the md5 checksum of a file from a container ."]]
["def firstof(parser, token):\n\tbits = token.split_contents()[1:]\n\tif (len(bits) < 1):\n\t\traise TemplateSyntaxError(\"'firstof'\tstatement\trequires\tat\tleast\tone\targument\")\n\treturn FirstOfNode([parser.compile_filter(bit) for bit in bits])\n",["outputs the first variable passed that is not false ."]]
["def find_autosummary_in_files(filenames):\n\tdocumented = []\n\tfor filename in filenames:\n\t\tf = open(filename, 'r')\n\t\tlines = f.read().splitlines()\n\t\tdocumented.extend(find_autosummary_in_lines(lines, filename=filename))\n\t\tf.close()\n\treturn documented\n",["find out what items are documented in source\/* ."]]
["def directory(name, user=None, group=None, recurse=None, max_depth=None, dir_mode=None, file_mode=None, makedirs=False, clean=False, require=None, exclude_pat=None, follow_symlinks=False, force=False, backupname=None, allow_symlink=True, children_only=False, win_owner=None, win_perms=None, win_deny_perms=None, win_inheritance=True, **kwargs):\n\tname = os.path.expanduser(name)\n\tret = {'name': name, 'changes': {}, 'pchanges': {}, 'result': True, 'comment': ''}\n\tif (not name):\n\t\treturn _error(ret, 'Must\tprovide\tname\tto\tfile.directory')\n\tif ((name[(-1)] == '\/') and (name != '\/')):\n\t\tname = name[:(-1)]\n\tif ((max_depth is not None) and clean):\n\t\treturn _error(ret, 'Cannot\tspecify\tboth\tmax_depth\tand\tclean')\n\tuser = _test_owner(kwargs, user=user)\n\tif salt.utils.is_windows():\n\t\tif (win_owner is None):\n\t\t\twin_owner = (user if user else None)\n\t\tif (group is not None):\n\t\t\tlog.warning('The\tgroup\targument\tfor\t{0}\thas\tbeen\tignored\tas\tthis\tis\ta\tWindows\tsystem.\tPlease\tuse\tthe\t`win_*`\tparameters\tto\tset\tpermissions\tin\tWindows.'.format(name))\n\t\tgroup = user\n\tif (('mode' in kwargs) and (not dir_mode)):\n\t\tdir_mode = kwargs.get('mode', [])\n\tif (not file_mode):\n\t\tfile_mode = dir_mode\n\tdir_mode = salt.utils.normalize_mode(dir_mode)\n\tfile_mode = salt.utils.normalize_mode(file_mode)\n\tif salt.utils.is_windows():\n\t\ttry:\n\t\t\tsalt.utils.win_dacl.get_sid(win_owner)\n\t\texcept CommandExecutionError as exc:\n\t\t\treturn _error(ret, exc)\n\telse:\n\t\tu_check = _check_user(user, group)\n\t\tif u_check:\n\t\t\treturn _error(ret, u_check)\n\tif (not os.path.isabs(name)):\n\t\treturn _error(ret, 'Specified\tfile\t{0}\tis\tnot\tan\tabsolute\tpath'.format(name))\n\tif (os.path.isfile(name) or ((not allow_symlink) and os.path.islink(name))):\n\t\tif (backupname is not None):\n\t\t\tif os.path.lexists(backupname):\n\t\t\t\tif (not force):\n\t\t\t\t\treturn _error(ret, 'File\texists\twhere\tthe\tbackup\ttarget\t{0}\tshould\tgo'.format(backupname))\n\t\t\t\telse:\n\t\t\t\t\t__salt__['file.remove'](backupname)\n\t\t\tos.rename(name, backupname)\n\t\telif force:\n\t\t\tif os.path.isfile(name):\n\t\t\t\tos.remove(name)\n\t\t\t\tret['changes']['forced'] = 'File\twas\tforcibly\treplaced'\n\t\t\telif __salt__['file.is_link'](name):\n\t\t\t\t__salt__['file.remove'](name)\n\t\t\t\tret['changes']['forced'] = 'Symlink\twas\tforcibly\treplaced'\n\t\t\telse:\n\t\t\t\t__salt__['file.remove'](name)\n\t\telif os.path.isfile(name):\n\t\t\treturn _error(ret, 'Specified\tlocation\t{0}\texists\tand\tis\ta\tfile'.format(name))\n\t\telif os.path.islink(name):\n\t\t\treturn _error(ret, 'Specified\tlocation\t{0}\texists\tand\tis\ta\tsymlink'.format(name))\n\tif salt.utils.is_windows():\n\t\t(presult, pcomment, ret['pchanges']) = _check_directory_win(name, win_owner, win_perms, win_deny_perms, win_inheritance)\n\telse:\n\t\t(presult, pcomment, ret['pchanges']) = _check_directory(name, user, group, (recurse or []), dir_mode, clean, require, exclude_pat, max_depth, follow_symlinks)\n\tif __opts__['test']:\n\t\tret['result'] = presult\n\t\tret['comment'] = pcomment\n\t\treturn ret\n\tif (not os.path.isdir(name)):\n\t\tif (not os.path.isdir(os.path.dirname(name))):\n\t\t\tif makedirs:\n\t\t\t\tif salt.utils.is_windows():\n\t\t\t\t\t(drive, path) = os.path.splitdrive(name)\n\t\t\t\t\tif (not os.path.isdir(drive)):\n\t\t\t\t\t\treturn _error(ret, 'Drive\t{0}\tis\tnot\tmapped'.format(drive))\n\t\t\t\t\t__salt__['file.makedirs'](name, win_owner, win_perms, win_deny_perms, win_inheritance)\n\t\t\t\telse:\n\t\t\t\t\t__salt__['file.makedirs'](name, user=user, group=group, mode=dir_mode)\n\t\t\telse:\n\t\t\t\treturn _error(ret, 'No\tdirectory\tto\tcreate\t{0}\tin'.format(name))\n\t\tif salt.utils.is_windows():\n\t\t\t__salt__['file.mkdir'](name, win_owner, win_perms, win_deny_perms, win_inheritance)\n\t\telse:\n\t\t\t__salt__['file.mkdir'](name, user=user, group=group, mode=dir_mode)\n\t\tret['changes'][name] = 'New\tDir'\n\tif (not os.path.isdir(name)):\n\t\treturn _error(ret, 'Failed\tto\tcreate\tdirectory\t{0}'.format(name))\n\tif (not children_only):\n\t\tif salt.utils.is_windows():\n\t\t\tret = __salt__['file.check_perms'](name, ret, win_owner, win_perms, win_deny_perms, win_inheritance)\n\t\telse:\n\t\t\t(ret, perms) = __salt__['file.check_perms'](name, ret, user, group, dir_mode, follow_symlinks)\n\terrors = []\n\tif (recurse or clean):\n\t\twalk_l = list(_depth_limited_walk(name, max_depth))\n\t\twalk_d = {}\n\t\tfor i in walk_l:\n\t\t\twalk_d[i[0]] = (i[1], i[2])\n\trecurse_set = None\n\tif recurse:\n\t\ttry:\n\t\t\trecurse_set = _get_recurse_set(recurse)\n\t\texcept (TypeError, ValueError) as exc:\n\t\t\tret['result'] = False\n\t\t\tret['comment'] = '{0}'.format(exc)\n\tif recurse_set:\n\t\tif ('user' in recurse_set):\n\t\t\tif user:\n\t\t\t\tuid = __salt__['file.user_to_uid'](user)\n\t\t\t\tif isinstance(uid, six.string_types):\n\t\t\t\t\tret['result'] = False\n\t\t\t\t\tret['comment'] = 'Failed\tto\tenforce\townership\tfor\tuser\t{0}\t(user\tdoes\tnot\texist)'.format(user)\n\t\t\telse:\n\t\t\t\tret['result'] = False\n\t\t\t\tret['comment'] = 'user\tnot\tspecified,\tbut\tconfigured\tas\ta\ttarget\tfor\trecursive\townership\tmanagement'\n\t\telse:\n\t\t\tuser = None\n\t\tif ('group' in recurse_set):\n\t\t\tif group:\n\t\t\t\tgid = __salt__['file.group_to_gid'](group)\n\t\t\t\tif isinstance(gid, six.string_types):\n\t\t\t\t\tret['result'] = False\n\t\t\t\t\tret['comment'] = 'Failed\tto\tenforce\tgroup\townership\tfor\tgroup\t{0}'.format(group)\n\t\t\telse:\n\t\t\t\tret['result'] = False\n\t\t\t\tret['comment'] = 'group\tnot\tspecified,\tbut\tconfigured\tas\ta\ttarget\tfor\trecursive\townership\tmanagement'\n\t\telse:\n\t\t\tgroup = None\n\t\tif ('mode' not in recurse_set):\n\t\t\tfile_mode = None\n\t\t\tdir_mode = None\n\t\tcheck_files = ('ignore_files' not in recurse_set)\n\t\tcheck_dirs = ('ignore_dirs' not in recurse_set)\n\t\tfor (root, dirs, files) in walk_l:\n\t\t\tif check_files:\n\t\t\t\tfor fn_ in files:\n\t\t\t\t\tfull = os.path.join(root, fn_)\n\t\t\t\t\ttry:\n\t\t\t\t\t\tif salt.utils.is_windows():\n\t\t\t\t\t\t\tret = __salt__['file.check_perms'](full, ret, win_owner, win_perms, win_deny_perms, win_inheritance)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t(ret, _) = __salt__['file.check_perms'](full, ret, user, group, file_mode, follow_symlinks)\n\t\t\t\t\texcept CommandExecutionError as exc:\n\t\t\t\t\t\tif (not exc.strerror.endswith('does\tnot\texist')):\n\t\t\t\t\t\t\terrors.append(exc.strerror)\n\t\t\tif check_dirs:\n\t\t\t\tfor dir_ in dirs:\n\t\t\t\t\tfull = os.path.join(root, dir_)\n\t\t\t\t\ttry:\n\t\t\t\t\t\tif salt.utils.is_windows():\n\t\t\t\t\t\t\tret = __salt__['file.check_perms'](full, ret, win_owner, win_perms, win_deny_perms, win_inheritance)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t(ret, _) = __salt__['file.check_perms'](full, ret, user, group, dir_mode, follow_symlinks)\n\t\t\t\t\texcept CommandExecutionError as exc:\n\t\t\t\t\t\tif (not exc.strerror.endswith('does\tnot\texist')):\n\t\t\t\t\t\t\terrors.append(exc.strerror)\n\tif clean:\n\t\tkeep = _gen_keep_files(name, require, walk_d)\n\t\tlog.debug('List\tof\tkept\tfiles\twhen\tuse\tfile.directory\twith\tclean:\t%s', keep)\n\t\tremoved = _clean_dir(name, list(keep), exclude_pat)\n\t\tif removed:\n\t\t\tret['changes']['removed'] = removed\n\t\t\tret['comment'] = 'Files\tcleaned\tfrom\tdirectory\t{0}'.format(name)\n\tif (not ret['comment']):\n\t\tif children_only:\n\t\t\tret['comment'] = 'Directory\t{0}\/*\tupdated'.format(name)\n\t\telse:\n\t\t\tret['comment'] = 'Directory\t{0}\tupdated'.format(name)\n\tif __opts__['test']:\n\t\tret['comment'] = 'Directory\t{0}\tnot\tupdated'.format(name)\n\telif ((not ret['changes']) and ret['result']):\n\t\torig_comment = None\n\t\tif ret['comment']:\n\t\t\torig_comment = ret['comment']\n\t\tret['comment'] = 'Directory\t{0}\tis\tin\tthe\tcorrect\tstate'.format(name)\n\t\tif orig_comment:\n\t\t\tret['comment'] = '\\n'.join([ret['comment'], orig_comment])\n\tif errors:\n\t\tret['result'] = False\n\t\tret['comment'] += '\\n\\nThe\tfollowing\terrors\twere\tencountered:\\n'\n\t\tfor error in errors:\n\t\t\tret['comment'] += '\\n-\t{0}'.format(error)\n\treturn ret\n",["an argument type (for use with the type= argument to argparse ."]]
["def _getWriters(reactor):\n\tif IReactorFDSet.providedBy(reactor):\n\t\treturn reactor.getWriters()\n\telif ('IOCP' in reactor.__class__.__name__):\n\t\treturn reactor.handles\n\telse:\n\t\traise Exception(('Cannot\tfind\twriters\ton\t%r' % (reactor,)))\n",["like l{ireactorfdset ."]]
["def guess_filename(obj):\n\tname = getattr(obj, 'name', None)\n\tif (name and isinstance(name, builtin_str) and (name[0] != '<') and (name[(-1)] != '>')):\n\t\treturn os.path.basename(name)\n",["tries to guess the filename of the given object ."]]
["def test_no_stdlib_collections2():\n\timport collections\n\tmatplotlib = import_module('matplotlib', __import__kwargs={'fromlist': ['collections']}, min_module_version='1.1.0', catch=(RuntimeError,))\n\tif matplotlib:\n\t\tassert (collections != matplotlib.collections)\n",["make sure we get the right collections when it is not part of a larger list ."]]
["def test_nm2_sample_wt_fit():\n\tratio = 'auto'\n\tnm2 = NearMiss(ratio=ratio, random_state=RND_SEED, version=VERSION_NEARMISS)\n\tassert_raises(RuntimeError, nm2.sample, X, Y)\n",["test either if an error is raised when sample is called before fitting ."]]
["def test_run_method_should_return_success_when_finds_command_name():\n\toptions_mock = Mock()\n\targs = ('freeze',)\n\thelp_cmd = HelpCommand()\n\tstatus = help_cmd.run(options_mock, args)\n\tassert (status == SUCCESS)\n",["test helpcommand ."]]
["def cos(x):\n\treturn tf.cos(x)\n",["apply cos to each element of the matrix mat ."]]
["def randomString(length, chrs):\n\tn = len(chrs)\n\treturn ''.join([chrs[randrange(n)] for _ in xrange(length)])\n",["produce a string of length random bytes ."]]
["def get_language():\n\tt = getattr(_active, u'value', None)\n\tif (t is not None):\n\t\ttry:\n\t\t\treturn t.to_language()\n\t\texcept AttributeError:\n\t\t\tpass\n\tfrom django.conf import settings\n\treturn settings.LANGUAGE_CODE\n",["returns the currently selected language ."]]
["@app.route('\/hidden-basic-auth\/<user>\/<passwd>')\ndef hidden_basic_auth(user='user', passwd='passwd'):\n\tif (not check_basic_auth(user, passwd)):\n\t\treturn status_code(404)\n\treturn jsonify(authenticated=True, user=user)\n",["prompts the user for authorization using http basic auth ."]]
["@register.filter(is_safe=False)\ndef dictsort(value, arg):\n\ttry:\n\t\treturn sorted(value, key=Variable(arg).resolve)\n\texcept (TypeError, VariableDoesNotExist):\n\t\treturn u''\n",["takes a list of dicts ."]]
["@release.command()\ndef publish():\n\tversion = get_version(1)\n\twith chdir(BASE):\n\t\tsubprocess.check_call(['git', 'push'])\n\t\tsubprocess.check_call(['git', 'push', '--tags'])\n\tpath = os.path.join(BASE, 'dist', 'beets-{}.tar.gz'.format(version))\n\tsubprocess.check_call(['twine', 'upload', path])\n",["easy publishing of my nice open source project ."]]
["def _template_func(setup, func):\n\tdef inner(_it, _timer, _func=func):\n\t\tsetup()\n\t\t_t0 = _timer()\n\t\tfor _i in _it:\n\t\t\t_func()\n\t\t_t1 = _timer()\n\t\treturn (_t1 - _t0)\n\treturn inner\n",["create a timer function ."]]
["def profiler(app):\n\tdef profile_internal(e, o):\n\t\t(out, result) = profile(app)(e, o)\n\t\treturn (out + [(('<pre>' + result) + '<\/pre>')])\n\treturn profile_internal\n",["outputs basic profiling information at the bottom of each response ."]]
["def test_continuous_error():\n\ty = np.linspace(0, 1, 20)\n\tcnn = CondensedNearestNeighbour(random_state=RND_SEED)\n\tassert_warns(UserWarning, cnn.fit, X, y)\n",["test either if an error is raised when the target are continuous type ."]]
["def urlencode(s):\n\treturn ''.join([('%%%02x' % ord(c)) for c in s])\n",["encode a sequence of two-element tuples or dictionary into a url query string ."]]
["def has_inherited_table(cls):\n\tfor class_ in cls.__mro__[1:]:\n\t\tif (getattr(class_, '__table__', None) is not None):\n\t\t\treturn True\n\treturn False\n",["given a class ."]]
["def iri_to_uri(iri):\n\tif (iri is None):\n\t\treturn iri\n\treturn quote(force_bytes(iri), safe=\"\/#%[]=:;$&()+,!?*@'~\")\n",["convert an internationalized resource identifier portion to a uri portion that is suitable for inclusion in a url ."]]
["def t2b(t):\n\tclean = rws(t)\n\tif ((len(clean) % 2) == 1):\n\t\traise ValueError('Even\tnumber\tof\tcharacters\texpected')\n\treturn a2b_hex(clean)\n",["convert a text string with bytes in hex form to a byte string ."]]
["def rsub(self, rhs):\n\tif isinstance(rhs, variable.Variable):\n\t\treturn Sub()(rhs, self)\n\t_check_constant_type(rhs)\n\treturn SubFromConstant(rhs)(self)\n",["element-wise subtraction ."]]
["def smart_unicode(s, strings_only=False, errors='strict'):\n\treturn django.utils.encoding.smart_unicode(s, get_site_encoding(), strings_only, errors)\n",["wrapper around djangos version ."]]
["def _test_args():\n\timport pandas as pd\n\treturn {'start': pd.Timestamp('2004', tz='utc'), 'end': pd.Timestamp('2008', tz='utc')}\n",["extra arguments to use when ziplines automated tests run this example ."]]
["@require_admin_context\ndef consistencygroup_include_in_cluster(context, cluster, partial_rename=True, **filters):\n\treturn _include_in_cluster(context, cluster, models.ConsistencyGroup, partial_rename, filters)\n",["include all consistency groups matching the filters into a cluster ."]]
["def main():\n\terrors = 0\n\tfits_files = handle_options(sys.argv[1:])\n\tsetup_logging()\n\tfor filename in fits_files:\n\t\terrors += process_file(filename)\n\tif errors:\n\t\tlog.warning('{}\terrors'.format(errors))\n\treturn int(bool(errors))\n",["runs the testsuite as command line application ."]]
["def blend_channels_screen(bottom_chan, top_chan):\n\treturn (1 - ((1 - bottom_chan[:, :]) * (1 - top_chan[:, :])))\n",["return combination of bottom and top channels ."]]
["def input_yn(prompt, require=False):\n\tsel = input_options(('y', 'n'), require, prompt, 'Enter\tY\tor\tN:')\n\treturn (sel == 'y')\n",["prompts the user for a \"yes\" or \"no\" response ."]]
["def get_data(datastore, path):\n\tclient = _get_client()\n\treturn client.get_datastore_data(datastore, path)\n",["get the data for name ."]]
["def ProfileEntryFromString(xml_string):\n\treturn atom.core.parse(ProfileEntry, xml_string)\n",["converts an xml string into a profileentry object ."]]
["def fixed_ip_get(context, id, get_network=False):\n\treturn IMPL.fixed_ip_get(context, id, get_network)\n",["get fixed ip by id or raise if it does not exist ."]]
["def set_classes(options):\n\tif options.has_key('class'):\n\t\tassert (not options.has_key('classes'))\n\t\toptions['classes'] = options['class']\n\t\tdel options['class']\n",["auxiliary function to set options[classes] and delete options[class] ."]]
["def onCellAppData(key, value):\n\tDEBUG_MSG(('onCellAppData:\t%s' % key))\n",["kbengine method ."]]
["def voidptr_output(func, argtypes, errcheck=True):\n\tfunc.argtypes = argtypes\n\tfunc.restype = c_void_p\n\tif errcheck:\n\t\tfunc.errcheck = check_pointer\n\treturn func\n",["for functions that return c_void_p ."]]
["def clear_site_cache(sender, **kwargs):\n\tinstance = kwargs['instance']\n\tusing = kwargs['using']\n\ttry:\n\t\tdel SITE_CACHE[instance.pk]\n\texcept KeyError:\n\t\tpass\n\ttry:\n\t\tdel SITE_CACHE[Site.objects.using(using).get(pk=instance.pk).domain]\n\texcept (KeyError, Site.DoesNotExist):\n\t\tpass\n",["clears the cache each time a site is saved or deleted ."]]
["def is_hop_by_hop_header(header):\n\treturn (header.lower() in _hop_by_hop_headers)\n",["check if a header is an http\/1 ."]]
["def onGlobalBasesDel(key):\n\tDEBUG_MSG(('onGlobalBasesDel:\t%s' % key))\n",["kbengine method ."]]
["def urlquote(val):\n\tif (val is None):\n\t\treturn ''\n\tif (not isinstance(val, unicode)):\n\t\tval = str(val)\n\telse:\n\t\tval = val.encode('utf-8')\n\treturn urllib.quote(val)\n",["a version of pythons urllib ."]]
["def restart(name, jail=None):\n\tcmd = '{0}\t{1}\tonerestart'.format(_cmd(jail), name)\n\treturn (not __salt__['cmd.retcode'](cmd, python_shell=False))\n",["restart the named service cli example: ."]]
["def start(name, call=None):\n\tif (call != 'action'):\n\t\traise SaltCloudSystemExit('The\tstart\taction\tmust\tbe\tcalled\twith\t-a\tor\t--action.')\n\tdata = show_instance(name, call='action')\n\tif (data.get('status') == 'active'):\n\t\treturn {'success': True, 'action': 'start', 'status': 'active', 'msg': 'Machine\tis\talready\trunning.'}\n\tret = query(droplet_id=data['id'], command='actions', args={'type': 'power_on'}, http_method='post')\n\treturn {'success': True, 'action': ret['action']['type'], 'state': ret['action']['status']}\n",["start the specified service cli example: ."]]
["def verbose_print(arg):\n\tif support.verbose:\n\t\twith _print_mutex:\n\t\t\tprint arg\n",["helper function for printing out debugging output ."]]
["def processElementNode(elementNode):\n\tpath.convertElementNode(elementNode, getGeometryOutput(None, elementNode))\n",["process the xml element ."]]
["def comment(parser, token):\n\tparser.skip_past('endcomment')\n\treturn CommentNode()\n",["ignores everything between {% comment %} and {% endcomment %} ."]]
["def urlencode(s):\n\treturn ''.join([('%%%02x' % ord(c)) for c in s])\n",["escapes a value for use in a url ."]]
["def getatime(filename):\n\treturn os.stat(filename).st_atime\n",["return the last access time of a file ."]]
["def getNewMouseTool():\n\treturn ViewpointRotate()\n",["get a new mouse tool ."]]
["def init(mpstate):\n\treturn SerialModule(mpstate)\n",["initialise module ."]]
["def init(mpstate):\n\treturn SerialModule(mpstate)\n",["initialise module ."]]
["def gdal_full_version():\n\treturn _version_info('')\n",["returns the full gdal version information ."]]
["def getNewRepository():\n\treturn ExportRepository()\n",["get new repository ."]]
["def init(mpstate):\n\treturn SerialModule(mpstate)\n",["initialise module ."]]
["def wordcount(value):\n\treturn len(value.split())\n",["returns the number of words ."]]
["def capacity_indicator():\n\treturn s3_rest_controller()\n",["restful crud controller ."]]
["def _get_image_blob(im):\n\tim_orig = im.astype(np.float32, copy=True)\n\tim_orig -= cfg.PIXEL_MEANS\n\tim_shape = im_orig.shape\n\tim_size_min = np.min(im_shape[0:2])\n\tim_size_max = np.max(im_shape[0:2])\n\tprocessed_ims = []\n\tim_scale_factors = []\n\tfor target_size in cfg.TEST.SCALES:\n\t\tim_scale = (float(target_size) \/ float(im_size_min))\n\t\tif (np.round((im_scale * im_size_max)) > cfg.TEST.MAX_SIZE):\n\t\t\tim_scale = (float(cfg.TEST.MAX_SIZE) \/ float(im_size_max))\n\t\tim = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)\n\t\tim_scale_factors.append(im_scale)\n\t\tprocessed_ims.append(im)\n\tblob = im_list_to_blob(processed_ims)\n\treturn (blob, np.array(im_scale_factors))\n",["converts an image into a network input ."]]
["def dmp_rr_prs_gcd(f, g, u, K):\n\tif (not u):\n\t\treturn dup_rr_prs_gcd(f, g, K)\n\tresult = _dmp_rr_trivial_gcd(f, g, u, K)\n\tif (result is not None):\n\t\treturn result\n\t(fc, F) = dmp_primitive(f, u, K)\n\t(gc, G) = dmp_primitive(g, u, K)\n\th = dmp_subresultants(F, G, u, K)[(-1)]\n\t(c, _, _) = dmp_rr_prs_gcd(fc, gc, (u - 1), K)\n\tif K.is_negative(dmp_ground_LC(h, u, K)):\n\t\th = dmp_neg(h, u, K)\n\t(_, h) = dmp_primitive(h, u, K)\n\th = dmp_mul_term(h, c, 0, u, K)\n\tcff = dmp_quo(f, h, u, K)\n\tcfg = dmp_quo(g, h, u, K)\n\treturn (h, cff, cfg)\n",["computes polynomial gcd using subresultants over a ring ."]]
["def _update_all_ids_to_uuids(t_images, t_image_members, t_image_properties):\n\timages = list(t_images.select().execute())\n\tfor image in images:\n\t\told_id = image['id']\n\t\tnew_id = str(uuid.uuid4())\n\t\tt_images.update().where((t_images.c.id == old_id)).values(id=new_id).execute()\n\t\tt_image_members.update().where((t_image_members.c.image_id == old_id)).values(image_id=new_id).execute()\n\t\tt_image_properties.update().where((t_image_properties.c.image_id == old_id)).values(image_id=new_id).execute()\n\t\tt_image_properties.update().where(and_(or_((t_image_properties.c.name == 'kernel_id'), (t_image_properties.c.name == 'ramdisk_id')), (t_image_properties.c.value == old_id))).values(value=new_id).execute()\n",["transition from integer id to varchar id ."]]
["def dumps(obj, *args, **kwargs):\n\tjson_options = kwargs.pop('json_options', DEFAULT_JSON_OPTIONS)\n\treturn json.dumps(_json_convert(obj, json_options), *args, **kwargs)\n",["serialize obj to a json formatted str ."]]
["def recv_item_json():\n\ttry:\n\t\titem_id = request.args[0]\n\texcept:\n\t\traise HTTP(400, current.xml.json_message(False, 400, 'No\tvalue\tprovided!'))\n\tstable = s3db.org_site\n\trtable = s3db.inv_recv\n\tittable = s3db.inv_track_item\n\trtable.date.represent = (lambda dt: dt[:10])\n\tquery = (((((ittable.req_item_id == item_id) & (rtable.id == ittable.recv_id)) & (rtable.site_id == stable.id)) & (rtable.status == s3db.inv_ship_status['RECEIVED'])) & (ittable.deleted == False))\n\trecords = db(query).select(rtable.id, rtable.date, stable.name, ittable.quantity)\n\toutput = ('[%s,%s' % (json.dumps(dict(id=str(T('Received')), quantity='#')), records.json()[1:]))\n\tresponse.headers['Content-Type'] = 'application\/json'\n\treturn output\n",["used by s3 ."]]
["@require_http_methods('GET')\n@login_required\n@expect_json\ndef xblock_outline_handler(request, usage_key_string):\n\tusage_key = usage_key_with_run(usage_key_string)\n\tif (not has_studio_read_access(request.user, usage_key.course_key)):\n\t\traise PermissionDenied()\n\tresponse_format = request.GET.get('format', 'html')\n\tif ((response_format == 'json') or ('application\/json' in request.META.get('HTTP_ACCEPT', 'application\/json'))):\n\t\tstore = modulestore()\n\t\twith store.bulk_operations(usage_key.course_key):\n\t\t\troot_xblock = store.get_item(usage_key, depth=None)\n\t\t\treturn JsonResponse(create_xblock_info(root_xblock, include_child_info=True, course_outline=True, include_children_predicate=(lambda xblock: (not (xblock.category == 'vertical')))))\n\telse:\n\t\treturn Http404\n",["the restful handler for requests for xblock information about the block and its children ."]]
["def nlargest(n, iterable, key=None):\n\tif (n == 1):\n\t\tit = iter(iterable)\n\t\thead = list(islice(it, 1))\n\t\tif (not head):\n\t\t\treturn []\n\t\tif (key is None):\n\t\t\treturn [max(chain(head, it))]\n\t\treturn [max(chain(head, it), key=key)]\n\ttry:\n\t\tsize = len(iterable)\n\texcept (TypeError, AttributeError):\n\t\tpass\n\telse:\n\t\tif (n >= size):\n\t\t\treturn sorted(iterable, key=key, reverse=True)[:n]\n\tif (key is None):\n\t\tit = izip(iterable, imap(neg, count()))\n\t\tresult = _nlargest(n, it)\n\t\treturn map(itemgetter(0), result)\n\t(in1, in2) = tee(iterable)\n\tit = izip(imap(key, in1), imap(neg, count()), in2)\n\tresult = _nlargest(n, it)\n\treturn map(itemgetter(2), result)\n",["find the n largest elements in a dataset ."]]
["def GetCustomerIDs(client):\n\tmanaged_customer_service = client.GetService('ManagedCustomerService', version='v201607')\n\toffset = 0\n\tselector = {'fields': ['CustomerId'], 'predicates': [{'field': 'CanManageClients', 'operator': 'EQUALS', 'values': [False]}], 'paging': {'startIndex': str(offset), 'numberResults': str(PAGE_SIZE)}}\n\tqueue = multiprocessing.Queue()\n\tmore_pages = True\n\twhile more_pages:\n\t\tpage = managed_customer_service.get(selector)\n\t\tif (page and ('entries' in page) and page['entries']):\n\t\t\tfor entry in page['entries']:\n\t\t\t\tqueue.put(entry['customerId'])\n\t\telse:\n\t\t\traise Exception(\"Can't\tretrieve\tany\tcustomer\tID.\")\n\t\toffset += PAGE_SIZE\n\t\tselector['paging']['startIndex'] = str(offset)\n\t\tmore_pages = (offset < int(page['totalNumEntries']))\n\treturn queue\n",["retrieves all customerids in the account hierarchy ."]]
["def retry_over_time(fun, catch, args=[], kwargs={}, errback=None, max_retries=None, interval_start=2, interval_step=2, interval_max=30, callback=None):\n\tretries = 0\n\tinterval_range = fxrange(interval_start, (interval_max + interval_start), interval_step, repeatlast=True)\n\tfor retries in count():\n\t\ttry:\n\t\t\treturn fun(*args, **kwargs)\n\t\texcept catch as exc:\n\t\t\tif (max_retries and (retries >= max_retries)):\n\t\t\t\traise\n\t\t\tif callback:\n\t\t\t\tcallback()\n\t\t\ttts = float((errback(exc, interval_range, retries) if errback else next(interval_range)))\n\t\t\tif tts:\n\t\t\t\tfor _ in range(int(tts)):\n\t\t\t\t\tif callback:\n\t\t\t\t\t\tcallback()\n\t\t\t\t\tsleep(1.0)\n\t\t\t\tsleep(abs((int(tts) - tts)))\n",["retry the function over and over until max retries is exceeded ."]]
["def localize_input(value, default=None):\n\tif isinstance(value, (decimal.Decimal, float, int, long)):\n\t\treturn number_format(value)\n\tif isinstance(value, datetime.datetime):\n\t\tvalue = datetime_safe.new_datetime(value)\n\t\tformat = smart_str((default or get_format('DATETIME_INPUT_FORMATS')[0]))\n\t\treturn value.strftime(format)\n\telif isinstance(value, datetime.date):\n\t\tvalue = datetime_safe.new_date(value)\n\t\tformat = smart_str((default or get_format('DATE_INPUT_FORMATS')[0]))\n\t\treturn value.strftime(format)\n\telif isinstance(value, datetime.time):\n\t\tformat = smart_str((default or get_format('TIME_INPUT_FORMATS')[0]))\n\t\treturn value.strftime(format)\n\treturn value\n",["checks if an input value is a localizable type and returns it formatted with the appropriate formatting string of the current locale ."]]
["def test_set_join_node(tmpdir):\n\tos.chdir(str(tmpdir))\n\twf = pe.Workflow(name=u'test')\n\tinputspec = pe.Node(IdentityInterface(fields=[u'n']), name=u'inputspec')\n\tinputspec.iterables = [(u'n', [1, 2, 1, 3, 2])]\n\tpre_join1 = pe.Node(IncrementInterface(), name=u'pre_join1')\n\twf.connect(inputspec, u'n', pre_join1, u'input1')\n\tjoin = pe.JoinNode(SetInterface(), joinsource=u'inputspec', joinfield=u'input1', name=u'join')\n\twf.connect(pre_join1, u'output1', join, u'input1')\n\twf.run()\n\tassert (_set_len == 3), (u'The\tjoin\tSet\toutput\tvalue\tis\tincorrect:\t%s.' % _set_len)\n",["test collecting join inputs to a set ."]]
["def pbkdf2_bin(data, salt, iterations=1000, keylen=24, hashfunc=None):\n\thashfunc = (hashfunc or hashlib.sha1)\n\tmac = hmac.new(data, None, hashfunc)\n\tdef _pseudorandom(x, mac=mac):\n\t\th = mac.copy()\n\t\th.update(x)\n\t\treturn map(ord, h.digest())\n\tbuf = []\n\tfor block in xrange(1, ((- ((- keylen) \/\/ mac.digest_size)) + 1)):\n\t\trv = u = _pseudorandom((salt + _pack_int(block)))\n\t\tfor i in xrange((iterations - 1)):\n\t\t\tu = _pseudorandom(''.join(map(chr, u)))\n\t\t\trv = starmap(xor, izip(rv, u))\n\t\tbuf.extend(rv)\n\treturn ''.join(map(chr, buf))[:keylen]\n",["returns a binary digest for the pbkdf2 hash algorithm of data with the given salt ."]]
["def stringfilter(func):\n\tdef _dec(*args, **kwargs):\n\t\tif args:\n\t\t\targs = list(args)\n\t\t\targs[0] = force_text(args[0])\n\t\t\tif (isinstance(args[0], SafeData) and getattr(_dec._decorated_function, u'is_safe', False)):\n\t\t\t\treturn mark_safe(func(*args, **kwargs))\n\t\treturn func(*args, **kwargs)\n\t_dec._decorated_function = getattr(func, u'_decorated_function', func)\n\treturn wraps(func)(_dec)\n",["decorator for filters which should only receive unicode objects ."]]
["def setcopyright():\n\t__builtin__.copyright = _Printer('copyright', sys.copyright)\n\tif (sys.platform[:4] == 'java'):\n\t\t__builtin__.credits = _Printer('credits', 'Jython\tis\tmaintained\tby\tthe\tJython\tdevelopers\t(www.jython.org).')\n\telif (sys.platform == 'cli'):\n\t\t__builtin__.credits = _Printer('credits', 'IronPython\tis\tmaintained\tby\tthe\tIronPython\tdevelopers\t(www.ironpython.net).')\n\telse:\n\t\t__builtin__.credits = _Printer('credits', '\t\t\t\tThanks\tto\tCWI,\tCNRI,\tBeOpen.com,\tZope\tCorporation\tand\ta\tcast\tof\tthousands\\n\t\t\t\tfor\tsupporting\tPython\tdevelopment.\t\tSee\twww.python.org\tfor\tmore\tinformation.')\n\there = os.path.dirname(os.__file__)\n\t__builtin__.license = _Printer('license', 'See\thttps:\/\/www.python.org\/psf\/license\/', ['LICENSE.txt', 'LICENSE'], [os.path.join(here, os.pardir), here, os.curdir])\n",["set copyright and credits in __builtin__ ."]]
["def _process_image(filename, coder):\n\twith tf.gfile.FastGFile(filename, 'r') as f:\n\t\timage_data = f.read()\n\tif _is_png(filename):\n\t\tprint(('Converting\tPNG\tto\tJPEG\tfor\t%s' % filename))\n\t\timage_data = coder.png_to_jpeg(image_data)\n\timage = coder.decode_jpeg(image_data)\n\tassert (len(image.shape) == 3)\n\theight = image.shape[0]\n\twidth = image.shape[1]\n\tassert (image.shape[2] == 3)\n\treturn (image_data, height, width)\n",["process a single image file ."]]
["def get_images_table(meta):\n\t(get_images_table,) = from_migration_import('008_add_image_members_table', ['get_images_table'])\n\timages = get_images_table(meta)\n\treturn images\n",["returns the table object for the images table that corresponds to the images table definition of this version ."]]
["def register_check(check, codes=None):\n\tdef _add_check(check, kind, codes, args):\n\t\tif (check in _checks[kind]):\n\t\t\t_checks[kind][check][0].extend((codes or []))\n\t\telse:\n\t\t\t_checks[kind][check] = ((codes or ['']), args)\n\tif inspect.isfunction(check):\n\t\targs = inspect.getargspec(check)[0]\n\t\tif (args and (args[0] in ('physical_line', 'logical_line'))):\n\t\t\tif (codes is None):\n\t\t\t\tcodes = ERRORCODE_REGEX.findall((check.__doc__ or ''))\n\t\t\t_add_check(check, args[0], codes, args)\n\telif inspect.isclass(check):\n\t\tif (inspect.getargspec(check.__init__)[0][:2] == ['self', 'tree']):\n\t\t\t_add_check(check, 'tree', codes, None)\n",["register a new check object ."]]
["def colorize(lead, num, color):\n\ts = (u'%s=%-4s' % (lead, str(num)))\n\tif ((num != 0) and ANSIBLE_COLOR and (color is not None)):\n\t\ts = stringc(s, color)\n\treturn s\n",["returns your text ."]]
["def syslog(server, enable=True):\n\tif (enable and __execute_cmd('config\t-g\tcfgRemoteHosts\t-o\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcfgRhostsSyslogEnable\t1')):\n\t\treturn __execute_cmd('config\t-g\tcfgRemoteHosts\t-o\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcfgRhostsSyslogServer1\t{0}'.format(server))\n\treturn __execute_cmd('config\t-g\tcfgRemoteHosts\t-o\tcfgRhostsSyslogEnable\t0')\n",["configure syslog remote logging ."]]
["def make_server(host=None, port=None, app=None, threaded=False, processes=1, request_handler=None, passthrough_errors=False, ssl_context=None, fd=None):\n\tif (threaded and (processes > 1)):\n\t\traise ValueError('cannot\thave\ta\tmultithreaded\tand\tmulti\tprocess\tserver.')\n\telif threaded:\n\t\treturn ThreadedWSGIServer(host, port, app, request_handler, passthrough_errors, ssl_context, fd=fd)\n\telif (processes > 1):\n\t\treturn ForkingWSGIServer(host, port, app, processes, request_handler, passthrough_errors, ssl_context, fd=fd)\n\telse:\n\t\treturn BaseWSGIServer(host, port, app, request_handler, passthrough_errors, ssl_context, fd=fd)\n",["create a new server instance that is either threaded ."]]
["def get_ranges(headervalue, content_length):\n\tif (not headervalue):\n\t\treturn None\n\tresult = []\n\t(bytesunit, byteranges) = headervalue.split('=', 1)\n\tfor brange in byteranges.split(','):\n\t\t(start, stop) = [x.strip() for x in brange.split('-', 1)]\n\t\tif start:\n\t\t\tif (not stop):\n\t\t\t\tstop = (content_length - 1)\n\t\t\t(start, stop) = (int(start), int(stop))\n\t\t\tif (start >= content_length):\n\t\t\t\tcontinue\n\t\t\tif (stop < start):\n\t\t\t\treturn None\n\t\t\tresult.append((start, (stop + 1)))\n\t\telse:\n\t\t\tif (not stop):\n\t\t\t\treturn None\n\t\t\tresult.append(((content_length - int(stop)), content_length))\n\treturn result\n",["return a list of indices from a range header ."]]
["def morsel_to_cookie(morsel):\n\texpires = None\n\tif morsel['max-age']:\n\t\texpires = (time.time() + morsel['max-age'])\n\telif morsel['expires']:\n\t\texpires = morsel['expires']\n\t\tif (type(expires) == type('')):\n\t\t\ttime_template = '%a,\t%d-%b-%Y\t%H:%M:%S\tGMT'\n\t\t\texpires = time.mktime(time.strptime(expires, time_template))\n\tc = create_cookie(name=morsel.key, value=morsel.value, version=(morsel['version'] or 0), port=None, domain=morsel['domain'], path=morsel['path'], secure=bool(morsel['secure']), expires=expires, discard=False, comment=morsel['comment'], comment_url=bool(morsel['comment']), rest={'HttpOnly': morsel['httponly']}, rfc2109=False)\n\treturn c\n",["convert a morsel object into a cookie containing the one k\/v pair ."]]
["def translate_pattern(pattern, anchor=1, prefix=None, is_regex=0):\n\tif is_regex:\n\t\tif isinstance(pattern, str):\n\t\t\treturn re.compile(pattern)\n\t\telse:\n\t\t\treturn pattern\n\tif pattern:\n\t\tpattern_re = glob_to_re(pattern)\n\telse:\n\t\tpattern_re = ''\n\tif (prefix is not None):\n\t\tempty_pattern = glob_to_re('')\n\t\tprefix_re = glob_to_re(prefix)[:(- len(empty_pattern))]\n\t\tsep = os.sep\n\t\tif (os.sep == '\\\\'):\n\t\t\tsep = '\\\\\\\\'\n\t\tpattern_re = ('^' + sep.join((prefix_re, ('.*' + pattern_re))))\n\telif anchor:\n\t\tpattern_re = ('^' + pattern_re)\n\treturn re.compile(pattern_re)\n",["translate a shell-like wildcard pattern to a compiled regular expression ."]]
["def was_modified_since(header=None, mtime=0, size=0):\n\ttry:\n\t\tif (header is None):\n\t\t\traise ValueError\n\t\tmatches = re.match('^([^;]+)(;\tlength=([0-9]+))?$', header, re.IGNORECASE)\n\t\theader_mtime = parse_http_date(matches.group(1))\n\t\theader_len = matches.group(3)\n\t\tif (header_len and (int(header_len) != size)):\n\t\t\traise ValueError\n\t\tif (mtime > header_mtime):\n\t\t\traise ValueError\n\texcept (AttributeError, ValueError, OverflowError):\n\t\treturn True\n\treturn False\n",["was something modified since the user last downloaded it? header this is the value of the if-modified-since header ."]]
["def use_setuptools(version=DEFAULT_VERSION, download_base=DEFAULT_URL, to_dir=os.curdir):\n\ttry:\n\t\timport setuptools\n\t\tif (setuptools.__version__ == '0.0.1'):\n\t\t\tprint >>sys.stderr, 'You\thave\tan\tobsolete\tversion\tof\tsetuptools\tinstalled.\t\tPlease\\nremove\tit\tfrom\tyour\tsystem\tentirely\tbefore\trerunning\tthis\tscript.'\n\t\t\tsys.exit(2)\n\texcept ImportError:\n\t\tegg = download_setuptools(version, download_base, to_dir)\n\t\tsys.path.insert(0, egg)\n\t\timport setuptools\n\t\tsetuptools.bootstrap_install_from = egg\n\timport pkg_resources\n\ttry:\n\t\tpkg_resources.require(('setuptools>=' + version))\n\texcept pkg_resources.VersionConflict:\n\t\tprint >>sys.stderr, (\"The\trequired\tversion\tof\tsetuptools\t(>=%s)\tis\tnot\tavailable,\tand\\ncan't\tbe\tinstalled\twhile\tthis\tscript\tis\trunning.\tPlease\tinstall\\n\ta\tmore\trecent\tversion\tfirst.\" % version)\n\t\tsys.exit(2)\n",["automatically find\/download setuptools and make it available on sys ."]]
["def get_quantifier(ch, input_iter):\n\tif (ch in '*?+'):\n\t\ttry:\n\t\t\t(ch2, escaped) = input_iter.next()\n\t\texcept StopIteration:\n\t\t\tch2 = None\n\t\tif (ch2 == '?'):\n\t\t\tch2 = None\n\t\tif (ch == '+'):\n\t\t\treturn (1, ch2)\n\t\treturn (0, ch2)\n\tquant = []\n\twhile (ch != '}'):\n\t\t(ch, escaped) = input_iter.next()\n\t\tquant.append(ch)\n\tquant = quant[:(-1)]\n\tvalues = ''.join(quant).split(',')\n\ttry:\n\t\t(ch, escaped) = input_iter.next()\n\texcept StopIteration:\n\t\tch = None\n\tif (ch == '?'):\n\t\tch = None\n\treturn (int(values[0]), ch)\n",["parse a quantifier from the input ."]]
["def make_vals(val, klass, klass_inst=None, prop=None, part=False, base64encode=False):\n\tcinst = None\n\tif isinstance(val, dict):\n\t\tcinst = klass().loadd(val, base64encode=base64encode)\n\telse:\n\t\ttry:\n\t\t\tcinst = klass().set_text(val)\n\t\texcept ValueError:\n\t\t\tif (not part):\n\t\t\t\tcis = [make_vals(sval, klass, klass_inst, prop, True, base64encode) for sval in val]\n\t\t\t\tsetattr(klass_inst, prop, cis)\n\t\t\telse:\n\t\t\t\traise\n\tif part:\n\t\treturn cinst\n\telif cinst:\n\t\tcis = [cinst]\n\t\tsetattr(klass_inst, prop, cis)\n",["creates a class instance with a specified value ."]]
["def authorize(name=None, source_group_name=None, source_group_owner_id=None, ip_protocol=None, from_port=None, to_port=None, cidr_ip=None, group_id=None, source_group_group_id=None, region=None, key=None, keyid=None, profile=None, vpc_id=None, vpc_name=None, egress=False):\n\tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\tgroup = _get_group(conn, name=name, vpc_id=vpc_id, vpc_name=vpc_name, group_id=group_id, region=region, key=key, keyid=keyid, profile=profile)\n\tif group:\n\t\ttry:\n\t\t\tadded = None\n\t\t\tif (not egress):\n\t\t\t\tadded = conn.authorize_security_group(src_security_group_name=source_group_name, src_security_group_owner_id=source_group_owner_id, ip_protocol=ip_protocol, from_port=from_port, to_port=to_port, cidr_ip=cidr_ip, group_id=group.id, src_security_group_group_id=source_group_group_id)\n\t\t\telse:\n\t\t\t\tadded = conn.authorize_security_group_egress(ip_protocol=ip_protocol, from_port=from_port, to_port=to_port, cidr_ip=cidr_ip, group_id=group.id, src_group_id=source_group_group_id)\n\t\t\tif added:\n\t\t\t\tlog.info('Added\trule\tto\tsecurity\tgroup\t{0}\twith\tid\t{1}'.format(group.name, group.id))\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\tmsg = 'Failed\tto\tadd\trule\tto\tsecurity\tgroup\t{0}\twith\tid\t{1}.'.format(group.name, group.id)\n\t\t\t\tlog.error(msg)\n\t\t\t\treturn False\n\t\texcept boto.exception.EC2ResponseError as e:\n\t\t\tmsg = 'Failed\tto\tadd\trule\tto\tsecurity\tgroup\t{0}\twith\tid\t{1}.'.format(group.name, group.id)\n\t\t\tlog.error(msg)\n\t\t\tlog.error(e)\n\t\t\treturn False\n\telse:\n\t\tlog.error('Failed\tto\tadd\trule\tto\tsecurity\tgroup.')\n\t\treturn False\n",["verifies that the action is valid on the target in this context ."]]
["def get_version(package):\n\tinit_py = open(os.path.join(package, '__init__.py')).read()\n\treturn re.search('__version__\t=\t[\\'\"]([^\\'\"]+)[\\'\"]', init_py).group(1)\n",["returns a pep 440-compliant version number from version ."]]
["def get_partial_date_formats():\n\twarnings.warn(\"'django.utils.translation.get_partial_date_formats'\tis\tdeprecated.\tPlease\tupdate\tyour\tcode\tto\tuse\tthe\tnew\ti18n\taware\tformatting.\", PendingDeprecationWarning)\n\tfrom django.conf import settings\n\tyear_month_format = ugettext('YEAR_MONTH_FORMAT')\n\tmonth_day_format = ugettext('MONTH_DAY_FORMAT')\n\tif (year_month_format == 'YEAR_MONTH_FORMAT'):\n\t\tyear_month_format = settings.YEAR_MONTH_FORMAT\n\tif (month_day_format == 'MONTH_DAY_FORMAT'):\n\t\tmonth_day_format = settings.MONTH_DAY_FORMAT\n\treturn (year_month_format, month_day_format)\n",["checks whether translation files provide a translation for some technical message id to store partial date formats ."]]
["def _is_fetching_self(url, method):\n\tif ((method != GET) or ('HTTP_HOST' not in os.environ) or ('PATH_INFO' not in os.environ)):\n\t\treturn False\n\t(scheme, host_port, path, query, fragment) = urlparse.urlsplit(url)\n\tif (host_port == os.environ['HTTP_HOST']):\n\t\tcurrent_path = urllib2.unquote(os.environ['PATH_INFO'])\n\t\tdesired_path = urllib2.unquote(path)\n\t\tif ((current_path == desired_path) or ((current_path in ('', '\/')) and (desired_path in ('', '\/')))):\n\t\t\treturn True\n\treturn False\n",["checks if the fetch is for the same url from which it originated ."]]
["def _getlabel(object_alias):\n\tfinder = _getfinder()\n\targs = {}\n\tattrs = {}\n\taeobj_00 = aetypes.ObjectSpecifier(want=aetypes.Type('cobj'), form='alis', seld=object_alias, fr=None)\n\taeobj_01 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('labi'), fr=aeobj_00)\n\targs['----'] = aeobj_01\n\t(_reply, args, attrs) = finder.send('core', 'getd', args, attrs)\n\tif args.has_key('errn'):\n\t\traise Error, aetools.decodeerror(args)\n\tif args.has_key('----'):\n\t\treturn args['----']\n",["label: get the label for the object ."]]
["def parse_header_links(value):\n\tlinks = []\n\treplace_chars = '\t\\'\"'\n\tfor val in re.split(',\t*<', value):\n\t\ttry:\n\t\t\t(url, params) = val.split(';', 1)\n\t\texcept ValueError:\n\t\t\t(url, params) = (val, '')\n\t\tlink = {'url': url.strip('<>\t\\'\"')}\n\t\tfor param in params.split(';'):\n\t\t\ttry:\n\t\t\t\t(key, value) = param.split('=')\n\t\t\texcept ValueError:\n\t\t\t\tbreak\n\t\t\tlink[key.strip(replace_chars)] = value.strip(replace_chars)\n\t\tlinks.append(link)\n\treturn links\n",["return a dict of parsed link headers proxies ."]]
["def serialize_remote_exception(failure_info, log_failure=True):\n\ttb = traceback.format_exception(*failure_info)\n\tfailure = failure_info[1]\n\tif log_failure:\n\t\tLOG.error(_('Returning\texception\t%s\tto\tcaller'), six.text_type(failure))\n\t\tLOG.error(tb)\n\tkwargs = {}\n\tif hasattr(failure, 'kwargs'):\n\t\tkwargs = failure.kwargs\n\tcls_name = str(failure.__class__.__name__)\n\tmod_name = str(failure.__class__.__module__)\n\tif (cls_name.endswith(_REMOTE_POSTFIX) and mod_name.endswith(_REMOTE_POSTFIX)):\n\t\tcls_name = cls_name[:(- len(_REMOTE_POSTFIX))]\n\t\tmod_name = mod_name[:(- len(_REMOTE_POSTFIX))]\n\tdata = {'class': cls_name, 'module': mod_name, 'message': six.text_type(failure), 'tb': tb, 'args': failure.args, 'kwargs': kwargs}\n\tjson_data = jsonutils.dumps(data)\n\treturn json_data\n",["prepares exception data to be sent over rpc ."]]
["def request_to_dict(request, spider=None):\n\tcb = request.callback\n\tif callable(cb):\n\t\tcb = _find_method(spider, cb)\n\teb = request.errback\n\tif callable(eb):\n\t\teb = _find_method(spider, eb)\n\td = {'url': request.url.decode('ascii'), 'callback': cb, 'errback': eb, 'method': request.method, 'headers': dict(request.headers), 'body': request.body, 'cookies': request.cookies, 'meta': request.meta, '_encoding': request._encoding, 'priority': request.priority, 'dont_filter': request.dont_filter}\n\treturn d\n",["convert request object to a dict ."]]
["def social_auth_by_name_backends(request):\n\tdef context_value():\n\t\tkeys = [key for key in get_backends().keys()]\n\t\taccounts = dict(zip(keys, ([None] * len(keys))))\n\t\tuser = request.user\n\t\tif user_is_authenticated(user):\n\t\t\taccounts.update(((assoc.provider, assoc) for assoc in UserSocialAuth.get_social_auth_for_user(user)))\n\t\treturn accounts\n\treturn {'social_auth': LazyDict(context_value)}\n",["load social auth current user data to context ."]]
["def test_senn_bad_ratio():\n\tratio = (-1.0)\n\tsmote = SMOTEENN(ratio=ratio)\n\tassert_raises(ValueError, smote.fit, X, Y)\n\tratio = 100.0\n\tsmote = SMOTEENN(ratio=ratio)\n\tassert_raises(ValueError, smote.fit, X, Y)\n\tratio = 'rnd'\n\tsmote = SMOTEENN(ratio=ratio)\n\tassert_raises(ValueError, smote.fit, X, Y)\n\tratio = [0.5, 0.5]\n\tsmote = SMOTEENN(ratio=ratio)\n\tassert_raises(ValueError, smote.fit, X, Y)\n",["test either if an error is raised with a wrong decimal value for the ratio ."]]
["def _mkstemp_inner(dir, pre, suf, flags):\n\tnames = _get_candidate_names()\n\tfor seq in xrange(TMP_MAX):\n\t\tname = names.next()\n\t\tfile = _os.path.join(dir, ((pre + name) + suf))\n\t\ttry:\n\t\t\tfd = _os.open(file, flags, 384)\n\t\t\t_set_cloexec(fd)\n\t\t\treturn (fd, _os.path.abspath(file))\n\t\texcept OSError as e:\n\t\t\tif (e.errno == _errno.EEXIST):\n\t\t\t\tcontinue\n\t\t\tif ((_os.name == 'nt') and (e.errno == _errno.EACCES)):\n\t\t\t\tcontinue\n\t\t\traise\n\traise IOError, (_errno.EEXIST, 'No\tusable\ttemporary\tfile\tname\tfound')\n",["code common to mkstemp ."]]
["def _date_from_string(year, year_format, month='', month_format='', day='', day_format='', delim='__'):\n\tformat = delim.join((year_format, month_format, day_format))\n\tdatestr = delim.join((year, month, day))\n\ttry:\n\t\treturn datetime.datetime.strptime(datestr, format).date()\n\texcept ValueError:\n\t\traise Http404((_(\"Invalid\tdate\tstring\t'%(datestr)s'\tgiven\tformat\t'%(format)s'\") % {'datestr': datestr, 'format': format}))\n",["helper: get a datetime ."]]
["def addListsToCraftTypeRepository(fileNameHelp, repository):\n\tsettings.addListsToRepositoryByFunction(fileNameHelp, getProfileDirectory, repository)\n\tdotsMinusOne = (fileNameHelp.count('.') - 1)\n\tx = 0\n\txAddition = 400\n\tfor step in xrange(dotsMinusOne):\n\t\tx += xAddition\n\t\txAddition \/= 2\n\trepository.windowPosition.value = ('%s+0' % x)\n",["add the value to the lists ."]]
["def get_size():\n\tplatf = platform.system()\n\tdimension = None\n\tif (platf == 'Windows'):\n\t\tdimension = _get_size_windows()\n\t\tif (dimension is None):\n\t\t\tdimension = _get_size_tput()\n\telif ((platf == 'Linux') or (platf == 'Darwin') or platf.startswith('CYGWIN')):\n\t\tdimension = _get_size_linux()\n\tif (dimension is None):\n\t\tdimension = (80, 25)\n\treturn dimension\n",["return the vms size object ."]]
["def equal_attributes(obj1, obj2, attributes):\n\tif (not attributes):\n\t\treturn False\n\tfor attr in attributes:\n\t\tif callable(attr):\n\t\t\tif (not (attr(obj1) == attr(obj2))):\n\t\t\t\treturn False\n\t\telse:\n\t\t\tif (not hasattr(obj1, attr)):\n\t\t\t\treturn False\n\t\t\tif (not hasattr(obj2, attr)):\n\t\t\t\treturn False\n\t\t\tif (not (getattr(obj1, attr) == getattr(obj2, attr))):\n\t\t\t\treturn False\n\treturn True\n",["compare two objects attributes ."]]
["def GetResultS(Handle, pIOType, Channel):\n\tif (os.name == 'nt'):\n\t\tstaticLib = ctypes.windll.LoadLibrary('labjackud')\n\t\tpv = ctypes.c_double()\n\t\tec = staticLib.GetResultS(Handle, pIOType, Channel, ctypes.byref(pv))\n\t\tif (ec != 0):\n\t\t\traise LabJackException(ec)\n\t\treturn pv.value\n\telse:\n\t\traise LabJackException(0, 'Function\tonly\tsupported\tfor\tWindows')\n",["put one value to the labjack device eput is equivilent to an addrequest followed by a goone ."]]
["def exec_command_all(*cmdargs, **kwargs):\n\tproc = subprocess.Popen(cmdargs, bufsize=(-1), stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kwargs)\n\t(out, err) = proc.communicate()\n\tif is_py3:\n\t\tencoding = kwargs.get('encoding')\n\t\tif encoding:\n\t\t\tout = out.decode(encoding)\n\t\t\terr = err.decode(encoding)\n\t\telse:\n\t\t\tout = os.fsdecode(out)\n\t\t\terr = os.fsdecode(err)\n\treturn (proc.returncode, out, err)\n",["run the command specified by the passed positional arguments ."]]
["@register.filter(is_safe=False)\ndef yesno(value, arg=None):\n\tif (arg is None):\n\t\targ = ugettext(u'yes,no,maybe')\n\tbits = arg.split(u',')\n\tif (len(bits) < 2):\n\t\treturn value\n\ttry:\n\t\t(yes, no, maybe) = bits\n\texcept ValueError:\n\t\t(yes, no, maybe) = (bits[0], bits[1], bits[1])\n\tif (value is None):\n\t\treturn maybe\n\tif value:\n\t\treturn yes\n\treturn no\n",["given a string mapping values for true ."]]
["def chhome(name, home, **kwargs):\n\tkwargs = salt.utils.clean_kwargs(**kwargs)\n\tpersist = kwargs.pop('persist', False)\n\tif kwargs:\n\t\tsalt.utils.invalid_kwargs(kwargs)\n\tif persist:\n\t\tlog.info(\"Ignoring\tunsupported\t'persist'\targument\tto\tuser.chhome\")\n\tpre_info = info(name)\n\tif (not pre_info):\n\t\traise CommandExecutionError(\"User\t'{0}'\tdoes\tnot\texist\".format(name))\n\tif (home == pre_info['home']):\n\t\treturn True\n\t_dscl(['\/Users\/{0}'.format(name), 'NFSHomeDirectory', pre_info['home'], home], ctype='change')\n\ttime.sleep(1)\n\treturn (info(name).get('home') == home)\n",["set a new home directory for an existing user name username to modify home new home directory to set persist : false set to true to prevent configuration"]]
["def ordinal(value):\n\ttry:\n\t\tvalue = int(value)\n\texcept (TypeError, ValueError):\n\t\treturn value\n\tt = ('th', 'st', 'nd', 'rd', 'th', 'th', 'th', 'th', 'th', 'th')\n\tif ((value % 100) in (11, 12, 13)):\n\t\treturn (u'%d%s' % (value, t[0]))\n\treturn (u'%d%s' % (value, t[(value % 10)]))\n",["converts an integer to its ordinal as a string ."]]
["def setUpModule():\n\tdevice_tracker.update_config(KNOWN_DEV_YAML_PATH, 'device_1', device_tracker.Device(None, None, True, 'device_1', 'DEV1', picture='http:\/\/example.com\/dev1.jpg'))\n\tdevice_tracker.update_config(KNOWN_DEV_YAML_PATH, 'device_2', device_tracker.Device(None, None, True, 'device_2', 'DEV2', picture='http:\/\/example.com\/dev2.jpg'))\n",["initialize a home assistant server ."]]
["def add_metaclass(metaclass):\n\tdef wrapper(cls):\n\t\torig_vars = cls.__dict__.copy()\n\t\torig_vars.pop('__dict__', None)\n\t\torig_vars.pop('__weakref__', None)\n\t\tfor slots_var in orig_vars.get('__slots__', ()):\n\t\t\torig_vars.pop(slots_var)\n\t\treturn metaclass(cls.__name__, cls.__bases__, orig_vars)\n\treturn wrapper\n",["class decorator for creating a class with a metaclass ."]]
["def urlencode(s):\n\treturn ''.join([('%%%02x' % ord(c)) for c in s])\n",["a version of pythons urllib ."]]
["def mktemp(suffix='', prefix=template, dir=None):\n\tif (dir is None):\n\t\tdir = gettempdir()\n\tnames = _get_candidate_names()\n\tfor seq in range(TMP_MAX):\n\t\tname = next(names)\n\t\tfile = _os.path.join(dir, ((prefix + name) + suffix))\n\t\tif (not _exists(file)):\n\t\t\treturn file\n\traise FileExistsError(_errno.EEXIST, 'No\tusable\ttemporary\tfilename\tfound')\n",["user-callable function to return a unique temporary file name ."]]
["def getnode():\n\tglobal _node\n\tif (_node is not None):\n\t\treturn _node\n\timport sys\n\tif (sys.platform == 'win32'):\n\t\tgetters = [_windll_getnode, _netbios_getnode, _ipconfig_getnode]\n\telse:\n\t\tgetters = [_unixdll_getnode, _ifconfig_getnode]\n\tfor getter in (getters + [_random_getnode]):\n\t\ttry:\n\t\t\t_node = getter()\n\t\texcept:\n\t\t\tcontinue\n\t\tif (_node is not None):\n\t\t\treturn _node\n",["get the hardware address as a 48-bit positive integer ."]]
["def dyld_find(name, executable_path=None, env=None):\n\tname = ensure_utf8(name)\n\texecutable_path = ensure_utf8(executable_path)\n\tfor path in dyld_image_suffix_search(chain(dyld_override_search(name, env), dyld_executable_path_search(name, executable_path), dyld_default_search(name, env)), env):\n\t\tif os.path.isfile(path):\n\t\t\treturn path\n\traise ValueError(('dylib\t%s\tcould\tnot\tbe\tfound' % (name,)))\n",["find a library or framework using dyld semantics ."]]
["def safe_join(directory, *pathnames):\n\tfor filename in pathnames:\n\t\tif (filename != ''):\n\t\t\tfilename = posixpath.normpath(filename)\n\t\tfor sep in _os_alt_seps:\n\t\t\tif (sep in filename):\n\t\t\t\traise NotFound()\n\t\tif (os.path.isabs(filename) or (filename == '..') or filename.startswith('..\/')):\n\t\t\traise NotFound()\n\t\tdirectory = os.path.join(directory, filename)\n\treturn directory\n",["safely join directory and filename ."]]
["@csrf_exempt\ndef admin_role(request):\n\tif (settings.AUTH_IP_WHITELIST and (not (get_client_ip(request) in settings.AUTH_IP_WHITELIST))):\n\t\treturn HttpResponse(json.dumps({'error': 'unauthorized_request'}), status=403, content_type='application\/json')\n\treturn HttpResponse(json.dumps({'adminRole': 'admin'}), content_type='application\/json')\n",["check ip whitelist \/ blacklist ."]]
["def getOutput(gcodeText, binary16ByteRepository=None):\n\tif (gcodeText == ''):\n\t\treturn ''\n\tif (binary16ByteRepository == None):\n\t\tbinary16ByteRepository = Binary16ByteRepository()\n\t\tsettings.getReadRepository(binary16ByteRepository)\n\treturn Binary16ByteSkein().getCraftedGcode(gcodeText, binary16ByteRepository)\n",["get the exported version of a gcode file ."]]
["def image_update(call=None, kwargs=None):\n\tif (call != 'function'):\n\t\traise SaltCloudSystemExit('The\timage_allocate\tfunction\tmust\tbe\tcalled\twith\t-f\tor\t--function.')\n\tif (kwargs is None):\n\t\tkwargs = {}\n\timage_id = kwargs.get('image_id', None)\n\timage_name = kwargs.get('image_name', None)\n\tpath = kwargs.get('path', None)\n\tdata = kwargs.get('data', None)\n\tupdate_type = kwargs.get('update_type', None)\n\tupdate_args = ['replace', 'merge']\n\tif (update_type is None):\n\t\traise SaltCloudSystemExit(\"The\timage_update\tfunction\trequires\tan\t'update_type'\tto\tbe\tprovided.\")\n\tif (update_type == update_args[0]):\n\t\tupdate_number = 0\n\telif (update_type == update_args[1]):\n\t\tupdate_number = 1\n\telse:\n\t\traise SaltCloudSystemExit('The\tupdate_type\targument\tmust\tbe\teither\t{0}\tor\t{1}.'.format(update_args[0], update_args[1]))\n\tif image_id:\n\t\tif image_name:\n\t\t\tlog.warning(\"Both\tthe\t'image_id'\tand\t'image_name'\targuments\twere\tprovided.\t'image_id'\twill\ttake\tprecedence.\")\n\telif image_name:\n\t\timage_id = get_image_id(kwargs={'name': image_name})\n\telse:\n\t\traise SaltCloudSystemExit(\"The\timage_update\tfunction\trequires\teither\tan\t'image_id'\tor\tan\t'image_name'\tto\tbe\tprovided.\")\n\tif data:\n\t\tif path:\n\t\t\tlog.warning(\"Both\tthe\t'data'\tand\t'path'\targuments\twere\tprovided.\t'data'\twill\ttake\tprecedence.\")\n\telif path:\n\t\tdata = salt.utils.fopen(path, mode='r').read()\n\telse:\n\t\traise SaltCloudSystemExit(\"The\timage_update\tfunction\trequires\teither\t'data'\tor\ta\tfile\t'path'\tto\tbe\tprovided.\")\n\t(server, user, password) = _get_xml_rpc()\n\tauth = ':'.join([user, password])\n\tresponse = server.one.image.update(auth, int(image_id), data, int(update_number))\n\tret = {'action': 'image.update', 'updated': response[0], 'image_id': response[1], 'error_code': response[2]}\n\treturn ret\n",["set the given properties on an image and update it ."]]
["def expand_makefile_vars(s, vars):\n\twhile 1:\n\t\tm = (_findvar1_rx.search(s) or _findvar2_rx.search(s))\n\t\tif m:\n\t\t\t(beg, end) = m.span()\n\t\t\ts = ((s[0:beg] + vars.get(m.group(1))) + s[end:])\n\t\telse:\n\t\t\tbreak\n\treturn s\n",["expand makefile-style variables -- \"${foo}\" or \"$\" -- in string according to vars ."]]
["def xframe_options_deny(view_func):\n\tdef wrapped_view(*args, **kwargs):\n\t\tresp = view_func(*args, **kwargs)\n\t\tif (resp.get('X-Frame-Options') is None):\n\t\t\tresp['X-Frame-Options'] = 'DENY'\n\t\treturn resp\n\treturn wraps(view_func)(wrapped_view)\n",["modifies a view function so its response has the x-frame-options http header set to deny as long as the response doesnt already have that header set ."]]
["@treeio_login_required\n@module_admin_required()\ndef page_view(request, page_id, response_format='html'):\n\tpage = get_object_or_404(Page, pk=page_id)\n\treturn render_to_response('core\/administration\/page_view', {'page': page}, context_instance=RequestContext(request), response_format=response_format)\n",["static page view ."]]
["def get_image(conn, vm_):\n\tvm_image = config.get_cloud_config_value('image', vm_, __opts__, default='').encode('ascii', 'salt-cloud-force-ascii')\n\tif (not vm_image):\n\t\tlog.debug('No\timage\tset,\tmust\tbe\tboot\tfrom\tvolume')\n\t\treturn None\n\timage_list = conn.image_list()\n\tfor img in image_list:\n\t\tif (vm_image in (image_list[img]['id'], img)):\n\t\t\treturn image_list[img]['id']\n\ttry:\n\t\timage = conn.image_show(vm_image)\n\t\treturn image['id']\n\texcept novaclient.exceptions.NotFound as exc:\n\t\traise SaltCloudNotFound(\"The\tspecified\timage,\t'{0}',\tcould\tnot\tbe\tfound:\t{1}\".format(vm_image, str(exc)))\n",["return the image object to use ."]]
["def count_special_lines(word, filename, invert=False):\n\ttry:\n\t\tcmd = ['grep', '-c']\n\t\tif invert:\n\t\t\tcmd.append('-v')\n\t\tcmd.extend([word, filename])\n\t\tout = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n\t\treturn int(out.communicate()[0].split()[0])\n\texcept:\n\t\tpass\n\treturn 0\n",["searching for special words using the grep tool grep is used to speed up the searching and counting the number of hits is returned ."]]
["def _format_range_context(start, stop):\n\tbeginning = (start + 1)\n\tlength = (stop - start)\n\tif (not length):\n\t\tbeginning -= 1\n\tif (length <= 1):\n\t\treturn '{}'.format(beginning)\n\treturn '{},{}'.format(beginning, ((beginning + length) - 1))\n",["convert range to the \"ed\" format ."]]
["def expires(delta):\n\ttry:\n\t\tdatetime\n\texcept NameError:\n\t\traise Exception, 'requires\tPython\t2.3\tor\tlater'\n\tif isinstance(delta, (int, long)):\n\t\tdelta = datetime.timedelta(seconds=delta)\n\to = (datetime.datetime.utcnow() + delta)\n\theader('Expires', o.strftime('%a,\t%d\t%b\t%Y\t%T\tGMT'))\n",["outputs an expires header for delta from now ."]]
["def _get_epochs():\n\traw = read_raw_fif(raw_fname)\n\traw.add_proj([], remove_existing=True)\n\tevents = read_events(event_name)\n\tpicks = _get_picks(raw)\n\tpicks = picks[np.round(np.linspace(0, (len(picks) - 1), n_chan)).astype(int)]\n\tpicks = np.concatenate([[2, 3, 4, 6, 7], picks])\n\tepochs = Epochs(raw, events[:5], event_id, tmin, tmax, picks=picks)\n\tepochs.info['bads'] = [epochs.ch_names[(-1)]]\n\treturn epochs\n",["get epochs ."]]
["def competency():\n\ts3.filter = (FS('person_id$human_resource.type') == 2)\n\tfield = s3db.hrm_competency.person_id\n\tfield.widget = S3PersonAutocompleteWidget(ajax_filter='~.human_resource.type=2')\n\treturn s3db.hrm_competency_controller()\n",["restful crud controller used to allow searching for people by skill ."]]
["def PrintUsageExit(code):\n\tprint (sys.modules['__main__'].__doc__ % sys.argv[0])\n\tsys.stdout.flush()\n\tsys.stderr.flush()\n\tsys.exit(code)\n",["prints usage information and exits with a status code ."]]
["def get_ladder_capture(state):\n\tfeature = np.zeros((1, state.size, state.size))\n\tfor (x, y) in state.get_legal_moves():\n\t\tfeature[(0, x, y)] = state.is_ladder_capture((x, y))\n\treturn feature\n",["a feature wrapping gamestate ."]]
["def catalog():\n\tglobal _default\n\tt = getattr(_active, u'value', None)\n\tif (t is not None):\n\t\treturn t\n\tif (_default is None):\n\t\tfrom django.conf import settings\n\t\t_default = translation(settings.LANGUAGE_CODE)\n\treturn _default\n",["returns the current active catalog for further processing ."]]
["def IE_Dispatcher(s):\n\tif (len(s) < 1):\n\t\treturn Raw(s)\n\tietype = ord(s[0])\n\tcls = ietypecls.get(ietype, Raw)\n\tif ((cls == Raw) and ((ietype & 128) == 128)):\n\t\tcls = IE_NotImplementedTLV\n\treturn cls(s)\n",["choose the correct information element class ."]]
["def delete_namespace_content(context, namespace_id, session):\n\tcount = 0\n\tquery = session.query(models.MetadefObject).filter_by(namespace_id=namespace_id)\n\tcount = query.delete(synchronize_session='fetch')\n\treturn count\n",["use this def only if the ns for the id has been verified as visible ."]]
["def dictsortreversed(value, arg):\n\tdecorated = [(resolve_variable(('var.' + arg), {'var': item}), item) for item in value]\n\tdecorated.sort()\n\tdecorated.reverse()\n\treturn [item[1] for item in decorated]\n",["takes a list of dicts ."]]
["def getSelectedPluginModuleFromPath(filePath, plugins):\n\tfor plugin in plugins:\n\t\tif plugin.value:\n\t\t\treturn gcodec.getModuleFromPath(plugin.name, filePath)\n\treturn None\n",["get the selected plugin module ."]]
["def printc(text, color):\n\tif sys.stdout.isatty():\n\t\tprint (((('\\x1b[' + codeCodes[color]) + 'm') + text) + '\\x1b[0m')\n\telse:\n\t\tprint text\n",["print in color ."]]
["def from_key_val_list(value):\n\tif (value is None):\n\t\treturn None\n\tif isinstance(value, (str, bytes, bool, int)):\n\t\traise ValueError('cannot\tencode\tobjects\tthat\tare\tnot\t2-tuples')\n\treturn OrderedDict(value)\n",["take an object and test to see if it can be represented as a dictionary ."]]
["def test_ada_fit_invalid_ratio():\n\tratio = (1.0 \/ 10000.0)\n\tada = ADASYN(ratio=ratio, random_state=RND_SEED)\n\tassert_raises(RuntimeError, ada.fit, X, Y)\n",["test either if an error is raised when the balancing ratio to fit is smaller than the one of the data ."]]
["def anon_url(*url):\n\turl = u''.join(map(str, url))\n\turi_pattern = u'^https?:\/\/'\n\tunicode_uri_pattern = re.compile(uri_pattern, re.UNICODE)\n\tif (not re.search(unicode_uri_pattern, url)):\n\t\turl = (u'http:\/\/' + url)\n\treturn u'{}{}'.format(sickrage.srCore.srConfig.ANON_REDIRECT, url)\n",["return a url string consisting of the anonymous redirect url and an arbitrary number of values appended ."]]
["def cinder_localization_strings(logical_line, tokens):\n\tgen = check_l18n()\n\tnext(gen)\n\ttry:\n\t\tmap(gen.send, tokens)\n\t\tgen.close()\n\texcept LocalizationError as e:\n\t\t(yield e.args)\n",["check localization in line ."]]
["def receiver(signal, **kwargs):\n\tdef _decorator(func):\n\t\tif isinstance(signal, (list, tuple)):\n\t\t\tfor s in signal:\n\t\t\t\ts.connect(func, **kwargs)\n\t\telse:\n\t\t\tsignal.connect(func, **kwargs)\n\t\treturn func\n\treturn _decorator\n",["a decorator for connecting receivers to signals ."]]
["def filter_factory(global_conf, **local_conf):\n\tconf = global_conf.copy()\n\tconf.update(local_conf)\n\tregister_swift_info('formpost')\n\treturn (lambda app: FormPost(app, conf))\n",["returns a wsgi filter app for use with paste ."]]
["def test_unit_division_by_string():\n\tu1 = u.cm\n\tus = u'kg'\n\tassert ((us \/ u1) == (u.Unit(us) \/ u1))\n\tassert ((u1 \/ us) == (u1 \/ u.Unit(us)))\n",["check that multiplication with strings produces the correct unit ."]]
["def enabled(name='allprofiles'):\n\tret = {'name': name, 'result': True, 'changes': {}, 'comment': ''}\n\taction = False\n\tcheck_name = None\n\tif (name != 'allprofiles'):\n\t\tcheck_name = True\n\tcurrent_config = __salt__['firewall.get_config']()\n\tif (check_name and (name not in current_config)):\n\t\tret['result'] = False\n\t\tret['comment'] = 'Profile\t{0}\tdoes\tnot\texist\tin\tfirewall.get_config'.format(name)\n\t\treturn ret\n\tfor key in current_config:\n\t\tif (not current_config[key]):\n\t\t\tif (check_name and (key != name)):\n\t\t\t\tcontinue\n\t\t\taction = True\n\t\t\tret['changes'] = {'fw': 'enabled'}\n\t\t\tbreak\n\tif __opts__['test']:\n\t\tret['result'] = ((not action) or None)\n\t\treturn ret\n\tif action:\n\t\tret['result'] = __salt__['firewall.enable'](name)\n\t\tif (not ret['result']):\n\t\t\tif check_name:\n\t\t\t\tmsg = 'Firewall\tprofile\t{0}\tcould\tnot\tbe\tenabled'.format(name)\n\t\t\telse:\n\t\t\t\tmsg = 'Could\tnot\tenable\tthe\tFW'\n\t\t\tret['comment'] = msg\n\telse:\n\t\tif check_name:\n\t\t\tmsg = 'Firewall\tprofile\t{0}\tis\tenabled'.format(name)\n\t\telse:\n\t\t\tmsg = 'All\tthe\tfirewall\tprofiles\tare\tenabled'\n\t\tret['comment'] = msg\n\treturn ret\n",["check to see if the named service is enabled to start on boot cli example: ."]]
["def report_tests(output, flaky_tests):\n\ttests = list((test.id() for (test, _) in flaky_tests))\n\tfor test in sorted(tests):\n\t\toutput.write('{}\\n'.format(test))\n",["print all flaky tests ."]]
["def security_group_get_by_name(context, project_id, group_name, columns_to_join=None):\n\treturn IMPL.security_group_get_by_name(context, project_id, group_name, columns_to_join=None)\n",["returns a security group with the specified name from a project ."]]
["def portableInstall(useGtk=True):\n\treactor = PortableGtkReactor()\n\tfrom twisted.internet.main import installReactor\n\tinstallReactor(reactor)\n\treturn reactor\n",["configure the twisted mainloop to be run inside the gtk mainloop ."]]
["def writeOutput(fileName=''):\n\tfileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName)\n\tif (fileName != ''):\n\t\tskeinforge_craft.writeChainTextWithNounMessage(fileName, 'unpause')\n",["lash a gcode linear move file ."]]
["def is_conservative(field):\n\tif (field == Vector(0)):\n\t\treturn True\n\tframe = list(field.separate())[0]\n\treturn (curl(field, frame).simplify() == Vector(0))\n",["checks if a field is conservative ."]]
["def _on_process_docstring(app, what, name, obj, options, lines):\n\tif ((what == 'module') and (name == 'falcon')):\n\t\tlines[:] = []\n\t\treturn\n\tlines[:] = [_process_line(line) for line in lines]\n",["process the docstring for a given python object ."]]
["def is_running(proxyname):\n\treturn {'result': _is_proxy_running(proxyname)}\n",["check if a service is running ."]]
["def maybe_iso8601(dt):\n\tif (not dt):\n\t\treturn\n\tif isinstance(dt, datetime):\n\t\treturn dt\n\treturn parse_iso8601(dt)\n",["either datetime | str -> datetime or none -> none ."]]
["def processXMLElement(xmlElement):\n\tgroup.processShape(Difference, xmlElement)\n",["process the xml element ."]]
["def get_configured_provider():\n\treturn config.is_provider_configured(__opts__, (__active_provider_name__ or __virtualname__), ('user', 'tenant', 'identity_url', 'compute_region'))\n",["return the first configured instance ."]]
["def _parseClientTCP(**kwargs):\n\tkwargs['port'] = int(kwargs['port'])\n\ttry:\n\t\tkwargs['timeout'] = int(kwargs['timeout'])\n\texcept KeyError:\n\t\tpass\n\treturn kwargs\n",["perform any argument value coercion necessary for tcp client parameters ."]]
["def get_configured_provider():\n\treturn config.is_provider_configured(__opts__, (__active_provider_name__ or __virtualname__), ('user', 'tenant', 'identity_url', 'compute_region'))\n",["return the first configured instance ."]]
["def exec_python(pycode):\n\tenv = copy.deepcopy(os.environ)\n\tenv['PATH'] = _env_path\n\tout = subprocess.Popen([_pyexe, '-c', pycode], env=env, stdout=subprocess.PIPE, shell=False).stdout.read()\n\tout = out.decode('ascii').strip()\n\treturn out\n",["wrap running python script in a subprocess ."]]
["def xkcd_palette(colors):\n\tpalette = [xkcd_rgb[name] for name in colors]\n\treturn color_palette(palette, len(palette))\n",["make a palette with color names from the xkcd color survey ."]]
["def dmp_quo(f, g, u, K):\n\treturn dmp_div(f, g, u, K)[0]\n",["returns exact polynomial quotient in k[x] ."]]
["def reload_(name):\n\tterm(name)\n",["reload the named service cli example: ."]]
["def strip_spaces_between_tags(value):\n\treturn re.sub(u'>\\\\s+<', u'><', force_text(value))\n",["returns the given html with spaces between tags removed ."]]
["def get_ipver_str(ip_version):\n\treturn IP_VERSION_DICT.get(ip_version, '')\n",["convert an ip version number to a human-friendly string ."]]
["@require_admin_context\ndef get_volume_summary_all(context):\n\tquery = model_query(context, func.count(models.Volume.id), func.sum(models.Volume.size), read_deleted='no')\n\tif (query is None):\n\t\treturn []\n\tresult = query.first()\n\treturn ((result[0] or 0), (result[1] or 0))\n",["get all volume summary ."]]
["def quoteArguments(arguments):\n\treturn '\t'.join(map(cmdLineQuote, arguments))\n",["quote an iterable of command-line arguments for passing to createprocess or a similar api ."]]
["def compile_dir(dfn):\n\treturn\n\tsubprocess.call([PYTHON, '-OO', '-m', 'compileall', '-f', dfn])\n",["compile * ."]]
["def timeuntil(d, now=None):\n\treturn timesince(d, now, reversed=True)\n",["like timesince ."]]
["def getNewDerivation(elementNode):\n\treturn evaluate.EmptyObject(elementNode)\n",["get new derivation ."]]
["def iterkeys(d, **kw):\n\treturn iter(getattr(d, _iterkeys)(**kw))\n",["return an iterator over the keys of a dictionary ."]]
["def register(func, msg_type=None, dispatcher_name=None, active_by_default=True):\n\tif (msg_type and (msg_type not in MSG_TYPE_MAP)):\n\t\traise InvalidHandlerType(('Invalid\tmessage\ttype\t[%s]:\ttype\tshould\tbe\tin\t%s' % (msg_type, str(MSG_TYPES))))\n\thandler = Handler(func=func, name=dispatcher_name)\n\tif (msg_type is None):\n\t\t_registry[RAW_TYPE].append(handler)\n\telse:\n\t\t_registry[msg_type].append(handler)\n\tif active_by_default:\n\t\t_active.add(dispatcher_name)\n",["required method to auto register this checker ."]]
["def description():\n\tfor desc in _description.splitlines():\n\t\tprint desc\n",["get description of brainstorm dataset ."]]
["def captured_stdout():\n\treturn captured_output('stdout')\n",["capture the output of sys ."]]
["def captured_stdout():\n\treturn captured_output('stdout')\n",["capture the output of sys ."]]
["def reload():\n\tbrowser.reload()\n\treturn browser.get_url()\n",["reload a service ."]]
["def __routes_doctest():\n\tpass\n",["dummy function for doctesting routes ."]]
["@functools.lru_cache(maxsize=None)\ndef get_callable(lookup_view):\n\tif callable(lookup_view):\n\t\treturn lookup_view\n\tif (not isinstance(lookup_view, str)):\n\t\traise ViewDoesNotExist((\"'%s'\tis\tnot\ta\tcallable\tor\ta\tdot-notation\tpath\" % lookup_view))\n\t(mod_name, func_name) = get_mod_func(lookup_view)\n\tif (not func_name):\n\t\traise ImportError((\"Could\tnot\timport\t'%s'.\tThe\tpath\tmust\tbe\tfully\tqualified.\" % lookup_view))\n\ttry:\n\t\tmod = import_module(mod_name)\n\texcept ImportError:\n\t\t(parentmod, submod) = get_mod_func(mod_name)\n\t\tif (submod and (not module_has_submodule(import_module(parentmod), submod))):\n\t\t\traise ViewDoesNotExist((\"Could\tnot\timport\t'%s'.\tParent\tmodule\t%s\tdoes\tnot\texist.\" % (lookup_view, mod_name)))\n\t\telse:\n\t\t\traise\n\telse:\n\t\ttry:\n\t\t\tview_func = getattr(mod, func_name)\n\t\texcept AttributeError:\n\t\t\traise ViewDoesNotExist((\"Could\tnot\timport\t'%s'.\tView\tdoes\tnot\texist\tin\tmodule\t%s.\" % (lookup_view, mod_name)))\n\t\telse:\n\t\t\tif (not callable(view_func)):\n\t\t\t\traise ViewDoesNotExist((\"Could\tnot\timport\t'%s.%s'.\tView\tis\tnot\tcallable.\" % (mod_name, func_name)))\n\t\t\treturn view_func\n",["convert a string version of a function name to the callable object ."]]
["@handle_response_format\n@treeio_login_required\ndef status_delete(request, status_id, response_format='html'):\n\tstatus = get_object_or_404(TicketStatus, pk=status_id)\n\tif (not request.user.profile.has_permission(status, mode='w')):\n\t\treturn user_denied(request, \"You\tdon't\thave\taccess\tto\tthis\tTicket\tStatus\", response_format)\n\tif request.POST:\n\t\tif ('delete' in request.POST):\n\t\t\tif ('trash' in request.POST):\n\t\t\t\tstatus.trash = True\n\t\t\t\tstatus.save()\n\t\t\telse:\n\t\t\t\tstatus.delete()\n\t\t\treturn HttpResponseRedirect(reverse('services_settings_view'))\n\t\telif ('cancel' in request.POST):\n\t\t\treturn HttpResponseRedirect(reverse('services_status_view', args=[status.id]))\n\tcontext = _get_default_context(request)\n\tcontext.update({'status': status})\n\treturn render_to_response('services\/status_delete', context, context_instance=RequestContext(request), response_format=response_format)\n",["itemstatus delete ."]]
["def disconnectNetToMs(Facility_presence=0, ProgressIndicator_presence=0, UserUser_presence=0, AllowedActions_presence=0):\n\ta = TpPd(pd=3)\n\tb = MessageType(mesType=37)\n\tc = Cause()\n\tpacket = ((a \/ b) \/ c)\n\tif (Facility_presence is 1):\n\t\td = FacilityHdr(ieiF=28, eightBitF=0)\n\t\tpacket = (packet \/ d)\n\tif (ProgressIndicator_presence is 1):\n\t\te = ProgressIndicatorHdr(ieiPI=30, eightBitPI=0)\n\t\tpacket = (packet \/ e)\n\tif (UserUser_presence is 1):\n\t\tf = UserUserHdr(ieiUU=126, eightBitUU=0)\n\t\tpacket = (packet \/ f)\n\tif (AllowedActions_presence is 1):\n\t\tg = AllowedActionsHdr(ieiAA=123, eightBitAA=0)\n\t\tpacket = (packet \/ g)\n\treturn packet\n",["disconnect section 9 ."]]
["def mpl_palette(name, n_colors=6):\n\tbrewer_qual_pals = {'Accent': 8, 'Dark2': 8, 'Paired': 12, 'Pastel1': 9, 'Pastel2': 8, 'Set1': 9, 'Set2': 8, 'Set3': 12}\n\tif name.endswith('_d'):\n\t\tpal = ['#333333']\n\t\tpal.extend(color_palette(name.replace('_d', '_r'), 2))\n\t\tcmap = blend_palette(pal, n_colors, as_cmap=True)\n\telse:\n\t\tcmap = getattr(mpl.cm, name)\n\tif (name in brewer_qual_pals):\n\t\tbins = np.linspace(0, 1, brewer_qual_pals[name])[:n_colors]\n\telse:\n\t\tbins = np.linspace(0, 1, (n_colors + 2))[1:(-1)]\n\tpalette = list(map(tuple, cmap(bins)[:, :3]))\n\treturn palette\n",["return discrete colors from a matplotlib palette ."]]
["def test_hsl_to_rgb_part_18():\n\tassert (hsl_to_rgb(300, 100, 0) == (0, 0, 0))\n\tassert (hsl_to_rgb(300, 100, 10) == (51, 0, 51))\n\tassert (hsl_to_rgb(300, 100, 20) == (102, 0, 102))\n\tassert (hsl_to_rgb(300, 100, 30) == (153, 0, 153))\n\tassert (hsl_to_rgb(300, 100, 40) == (204, 0, 204))\n\tassert (hsl_to_rgb(300, 100, 50) == (255, 0, 255))\n\tassert (hsl_to_rgb(300, 100, 60) == (255, 51, 255))\n\tassert (hsl_to_rgb(300, 100, 70) == (255, 102, 255))\n\tassert (hsl_to_rgb(300, 100, 80) == (255, 153, 255))\n\tassert (hsl_to_rgb(300, 100, 90) == (255, 204, 255))\n\tassert (hsl_to_rgb(300, 100, 100) == (255, 255, 255))\n",["test hsl to rgb color function ."]]
["def _GetTextInside(text, start_pattern):\n\tmatching_punctuation = {'(': ')', '{': '}', '[': ']'}\n\tclosing_punctuation = set(itervalues(matching_punctuation))\n\tmatch = re.search(start_pattern, text, re.M)\n\tif (not match):\n\t\treturn None\n\tstart_position = match.end(0)\n\tassert (start_position > 0), 'start_pattern\tmust\tends\twith\tan\topening\tpunctuation.'\n\tassert (text[(start_position - 1)] in matching_punctuation), 'start_pattern\tmust\tends\twith\tan\topening\tpunctuation.'\n\tpunctuation_stack = [matching_punctuation[text[(start_position - 1)]]]\n\tposition = start_position\n\twhile (punctuation_stack and (position < len(text))):\n\t\tif (text[position] == punctuation_stack[(-1)]):\n\t\t\tpunctuation_stack.pop()\n\t\telif (text[position] in closing_punctuation):\n\t\t\treturn None\n\t\telif (text[position] in matching_punctuation):\n\t\t\tpunctuation_stack.append(matching_punctuation[text[position]])\n\t\tposition += 1\n\tif punctuation_stack:\n\t\treturn None\n\treturn text[start_position:(position - 1)]\n",["retrieves all the text between matching open and close parentheses ."]]
["def _handle_ns(packageName, path_item):\n\timporter = get_importer(path_item)\n\tif (importer is None):\n\t\treturn None\n\tloader = importer.find_module(packageName)\n\tif (loader is None):\n\t\treturn None\n\tmodule = sys.modules.get(packageName)\n\tif (module is None):\n\t\tmodule = sys.modules[packageName] = types.ModuleType(packageName)\n\t\tmodule.__path__ = []\n\t\t_set_parent_ns(packageName)\n\telif (not hasattr(module, '__path__')):\n\t\traise TypeError('Not\ta\tpackage:', packageName)\n\thandler = _find_adapter(_namespace_handlers, importer)\n\tsubpath = handler(importer, path_item, packageName, module)\n\tif (subpath is not None):\n\t\tpath = module.__path__\n\t\tpath.append(subpath)\n\t\tloader.load_module(packageName)\n\t\tsys_path = [((p and _normalize_cached(p)) or p) for p in sys.path]\n\t\tdef sort_key(p):\n\t\t\tparts = p.split(os.sep)\n\t\t\tparts = parts[:(- (packageName.count('.') + 1))]\n\t\t\treturn sys_path.index(_normalize_cached(os.sep.join(parts)))\n\t\tpath.sort(key=sort_key)\n\t\tmodule.__path__[:] = [_normalize_cached(p) for p in path]\n\treturn subpath\n",["ensure that named package includes a subpath of path_item ."]]
["def get_indexes(**kwargs):\n\treturn get_indexes_async(**kwargs).get_result()\n",["returns a dictionary of fieldname -> infodict for the given table ."]]
["def _encode_basestring_ascii(s):\n\ttry:\n\t\tif (isinstance(s, str) and (HAS_UTF8.search(s) is not None)):\n\t\t\ts = s.decode('utf-8')\n\texcept:\n\t\tpass\n\tdef replace(match):\n\t\ts = match.group(0)\n\t\ttry:\n\t\t\treturn ESCAPE_DCT[s]\n\t\texcept KeyError:\n\t\t\tn = ord(s)\n\t\t\tif (n < 65536):\n\t\t\t\treturn ('\\\\u%04x' % (n,))\n\t\t\telse:\n\t\t\t\tn -= 65536\n\t\t\t\ts1 = (55296 | ((n >> 10) & 1023))\n\t\t\t\ts2 = (56320 | (n & 1023))\n\t\t\t\treturn ('\\\\u%04x\\\\u%04x' % (s1, s2))\n\treturn (('\"' + str(ESCAPE_ASCII.sub(replace, s))) + '\"')\n",["return an ascii-only json representation of a python string ."]]
["def get_environ_proxies(url):\n\tget_proxy = (lambda k: (os.environ.get(k) or os.environ.get(k.upper())))\n\tno_proxy = get_proxy('no_proxy')\n\tnetloc = urlparse(url).netloc\n\tif no_proxy:\n\t\tno_proxy = no_proxy.split(',')\n\t\tfor host in no_proxy:\n\t\t\tif (netloc.endswith(host) or netloc.split(':')[0].endswith(host)):\n\t\t\t\treturn {}\n\tif proxy_bypass(netloc):\n\t\treturn {}\n\treturn getproxies()\n",["return a dict of environment proxies ."]]
["@contextfunction\ndef attachments(context, object=None):\n\trequest = context['request']\n\tprofile = request.user.profile\n\tresponse_format = 'html'\n\tif ('response_format' in context):\n\t\tresponse_format = context['response_format']\n\tupdate = isinstance(object, UpdateRecord)\n\tif (not update):\n\t\tattachments = Attachment.objects.filter(attached_object=object)\n\t\tif profile.has_permission(object, mode='w'):\n\t\t\ttemplate = 'core\/tags\/attachments'\n\t\telse:\n\t\t\ttemplate = 'core\/tags\/attachments_block'\n\telse:\n\t\tattachments = Attachment.objects.filter(attached_record=object)\n\t\tif (profile == object.author):\n\t\t\ttemplate = 'core\/tags\/attachments_record'\n\t\telse:\n\t\t\ttemplate = 'core\/tags\/attachments_record_block'\n\treturn Markup(render_to_string(template, {'object': object, 'attachments': attachments}, context_instance=RequestContext(request), response_format=response_format))\n",["attachments for an object or update record ."]]
["def getfile(object):\n\tif ismodule(object):\n\t\tif hasattr(object, '__file__'):\n\t\t\treturn object.__file__\n\t\traise TypeError('{!r}\tis\ta\tbuilt-in\tmodule'.format(object))\n\tif isclass(object):\n\t\tobject = sys.modules.get(object.__module__)\n\t\tif hasattr(object, '__file__'):\n\t\t\treturn object.__file__\n\t\traise TypeError('{!r}\tis\ta\tbuilt-in\tclass'.format(object))\n\tif ismethod(object):\n\t\tobject = object.im_func\n\tif isfunction(object):\n\t\tobject = object.func_code\n\tif istraceback(object):\n\t\tobject = object.tb_frame\n\tif isframe(object):\n\t\tobject = object.f_code\n\tif iscode(object):\n\t\treturn object.co_filename\n\traise TypeError('{!r}\tis\tnot\ta\tmodule,\tclass,\tmethod,\tfunction,\ttraceback,\tframe,\tor\tcode\tobject'.format(object))\n",["work out which source or compiled file an object was defined in ."]]
["def config_dict(handle, conf_mappings, handler=None):\n\tselected_config = get_config(handle)\n\tselected_config.add_listener(_SyncListener(conf_mappings, handler).update)\n\treturn conf_mappings\n",["convert content of config-file into dictionary ."]]
["def GetFeeds(client):\n\tfeed_service = client.GetService('FeedService', 'v201609')\n\tfeeds = []\n\tmore_pages = True\n\tselector = {'fields': ['Id', 'Name', 'Attributes'], 'predicates': [{'field': 'Origin', 'operator': 'EQUALS', 'values': ['USER']}, {'field': 'FeedStatus', 'operator': 'EQUALS', 'values': ['ENABLED']}], 'paging': {'startIndex': 0, 'numberResults': PAGE_SIZE}}\n\twhile more_pages:\n\t\tpage = feed_service.get(selector)\n\t\tif ('entries' in page):\n\t\t\tfeeds.extend(page['entries'])\n\t\tselector['paging']['startIndex'] += PAGE_SIZE\n\t\tmore_pages = (selector['paging']['startIndex'] < int(page['totalNumEntries']))\n\treturn feeds\n",["returns a list of all enabled feeds ."]]
["def turn_off_internet(verbose=False):\n\tglobal INTERNET_OFF\n\tglobal _orig_opener\n\tif INTERNET_OFF:\n\t\treturn\n\tINTERNET_OFF = True\n\t__tracebackhide__ = True\n\tif verbose:\n\t\tprint(u'Internet\taccess\tdisabled')\n\t_orig_opener = urllib.request.build_opener()\n\tno_proxy_handler = urllib.request.ProxyHandler({})\n\topener = urllib.request.build_opener(no_proxy_handler)\n\turllib.request.install_opener(opener)\n\tsocket.create_connection = check_internet_off(socket_create_connection)\n\tsocket.socket.bind = check_internet_off(socket_bind)\n\tsocket.socket.connect = check_internet_off(socket_connect)\n\treturn socket\n",["disable internet access via python by preventing connections from being created using the socket module ."]]
["def disk_usage(path):\n\ttry:\n\t\tst = os.statvfs(path)\n\texcept UnicodeEncodeError:\n\t\tif ((not PY3) and isinstance(path, unicode)):\n\t\t\ttry:\n\t\t\t\tpath = path.encode(sys.getfilesystemencoding())\n\t\t\texcept UnicodeEncodeError:\n\t\t\t\tpass\n\t\t\tst = os.statvfs(path)\n\t\telse:\n\t\t\traise\n\ttotal = (st.f_blocks * st.f_frsize)\n\tavail_to_root = (st.f_bfree * st.f_frsize)\n\tavail_to_user = (st.f_bavail * st.f_frsize)\n\tused = (total - avail_to_root)\n\ttotal_user = (used + avail_to_user)\n\tusage_percent_user = usage_percent(used, total_user, _round=1)\n\treturn sdiskusage(total=total, used=used, free=avail_to_user, percent=usage_percent_user)\n",["return disk usage associated with path ."]]
["def escape_all(v, linkify_only_full=False):\n\tif isinstance(v, basestring):\n\t\tv = jinja2.escape(force_text(v))\n\t\tv = linkify_with_outgoing(v, only_full=linkify_only_full)\n\t\treturn v\n\telif isinstance(v, list):\n\t\tfor (i, lv) in enumerate(v):\n\t\t\tv[i] = escape_all(lv, linkify_only_full=linkify_only_full)\n\telif isinstance(v, dict):\n\t\tfor (k, lv) in v.iteritems():\n\t\t\tv[k] = escape_all(lv, linkify_only_full=linkify_only_full)\n\telif isinstance(v, Translation):\n\t\tv = jinja2.escape(force_text(v))\n\treturn v\n",["escape html in json value ."]]
["def geo_apps(namespace=True, runtests=False):\n\tfrom django.db import connection\n\tfrom django.contrib.gis.geos import GEOS_PREPARE\n\tfrom django.contrib.gis.gdal import HAS_GDAL\n\tapps = ['geoapp', 'relatedapp']\n\tif (not connection.ops.mysql):\n\t\tapps.append('distapp')\n\tif (connection.ops.postgis and connection.ops.geography):\n\t\tapps.append('geogapp')\n\tif HAS_GDAL:\n\t\tif (connection.ops.postgis and GEOS_PREPARE):\n\t\t\tapps.append('geo3d')\n\t\tapps.append('layermap')\n\tif runtests:\n\t\treturn [('django.contrib.gis.tests', app) for app in apps]\n\telif namespace:\n\t\treturn [('django.contrib.gis.tests.%s' % app) for app in apps]\n\telse:\n\t\treturn apps\n",["returns a list of geodjango test applications that reside in django ."]]
["def median(values):\n\tvalues = sorted(values)\n\tlength = len(values)\n\tmid = (length \/\/ 2)\n\tif (length % 2):\n\t\treturn values[mid]\n\telse:\n\t\treturn ((values[(mid - 1)] + values[mid]) \/ 2)\n",["computes an average of the median of each realigned timeseries parameters in_files: one or more realigned nifti 4d time series returns out_file: a 3d nifti file ."]]
["def _collapse_address_list_recursive(addresses):\n\tret_array = []\n\toptimized = False\n\tfor cur_addr in addresses:\n\t\tif (not ret_array):\n\t\t\tret_array.append(cur_addr)\n\t\t\tcontinue\n\t\tif (cur_addr in ret_array[(-1)]):\n\t\t\toptimized = True\n\t\telif (cur_addr == ret_array[(-1)].supernet().subnet()[1]):\n\t\t\tret_array.append(ret_array.pop().supernet())\n\t\t\toptimized = True\n\t\telse:\n\t\t\tret_array.append(cur_addr)\n\tif optimized:\n\t\treturn _collapse_address_list_recursive(ret_array)\n\treturn ret_array\n",["loops through the addresses ."]]
["def GetAsync(keys, **kwargs):\n\textra_hook = kwargs.pop('extra_hook', None)\n\tconfig = _GetConfigFromKwargs(kwargs)\n\t(keys, multiple) = NormalizeAndTypeCheckKeys(keys)\n\tdef local_extra_hook(entities):\n\t\tif multiple:\n\t\t\tresult = entities\n\t\telse:\n\t\t\tif (entities[0] is None):\n\t\t\t\traise datastore_errors.EntityNotFoundError()\n\t\t\tresult = entities[0]\n\t\tif extra_hook:\n\t\t\treturn extra_hook(result)\n\t\treturn result\n\treturn _GetConnection().async_get(config, keys, local_extra_hook)\n",["asynchronously retrieves one or more entities from the datastore ."]]
["def put_policy(Bucket, Policy, region=None, key=None, keyid=None, profile=None):\n\ttry:\n\t\tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\t\tif (Policy is None):\n\t\t\tPolicy = '{}'\n\t\telif (not isinstance(Policy, six.string_types)):\n\t\t\tPolicy = json.dumps(Policy)\n\t\tconn.put_bucket_policy(Bucket=Bucket, Policy=Policy)\n\t\treturn {'updated': True, 'name': Bucket}\n\texcept ClientError as e:\n\t\treturn {'updated': False, 'error': __utils__['boto3.get_error'](e)}\n",["given a valid config ."]]
["def nsga2select(population, fitnesses, survivors, allowequality=True):\n\tfronts = const_non_dominated_sort(population, key=(lambda x: fitnesses[x]), allowequality=allowequality)\n\tindividuals = set()\n\tfor front in fronts:\n\t\tremaining = (survivors - len(individuals))\n\t\tif (not (remaining > 0)):\n\t\t\tbreak\n\t\tif (len(front) > remaining):\n\t\t\tcrowd_dist = const_crowding_distance(front, fitnesses)\n\t\t\tfront = sorted(front, key=(lambda x: crowd_dist[x]), reverse=True)\n\t\t\tfront = set(front[:remaining])\n\t\tindividuals |= front\n\treturn list(individuals)\n",["the nsga-ii selection strategy ."]]
["def nsmallest(arr, n, keep='first'):\n\tif (keep == 'last'):\n\t\tarr = arr[::(-1)]\n\tnarr = len(arr)\n\tn = min(n, narr)\n\tsdtype = str(arr.dtype)\n\tarr = arr.view(_dtype_map.get(sdtype, sdtype))\n\tkth_val = algos.kth_smallest(arr.copy(), (n - 1))\n\treturn _finalize_nsmallest(arr, kth_val, n, keep, narr)\n",["find the n smallest elements in a dataset ."]]
["def traverse_imports(names):\n\tpending = [names]\n\twhile pending:\n\t\tnode = pending.pop()\n\t\tif (node.type == token.NAME):\n\t\t\t(yield node.value)\n\t\telif (node.type == syms.dotted_name):\n\t\t\t(yield ''.join([ch.value for ch in node.children]))\n\t\telif (node.type == syms.dotted_as_name):\n\t\t\tpending.append(node.children[0])\n\t\telif (node.type == syms.dotted_as_names):\n\t\t\tpending.extend(node.children[::(-2)])\n\t\telse:\n\t\t\traise AssertionError('unknown\tnode\ttype')\n",["walks over all the names imported in a dotted_as_names node ."]]
["def serve(request, path, document_root=None, insecure=False, **kwargs):\n\tif ((not settings.DEBUG) and (not insecure)):\n\t\traise ImproperlyConfigured(\"The\tstaticfiles\tview\tcan\tonly\tbe\tused\tin\tdebug\tmode\tor\tif\tthe\tthe\t--insecure\toption\tof\t'runserver'\tis\tused\")\n\tnormalized_path = posixpath.normpath(urllib.unquote(path)).lstrip('\/')\n\tabsolute_path = finders.find(normalized_path)\n\tif (not absolute_path):\n\t\tif (path.endswith('\/') or (path == '')):\n\t\t\traise Http404('Directory\tindexes\tare\tnot\tallowed\there.')\n\t\traise Http404((\"'%s'\tcould\tnot\tbe\tfound\" % path))\n\t(document_root, path) = os.path.split(absolute_path)\n\treturn static.serve(request, path, document_root=document_root, **kwargs)\n",["serve static files below a given point in the directory structure or from locations inferred from the staticfiles finders ."]]
["def addSparseEndpoints(doubleExtrusionWidth, endpoints, fillLine, horizontalSegmentLists, infillSolidity, removedEndpoints, solidSurfaceThickness, surroundingXIntersections):\n\thorizontalEndpoints = horizontalSegmentLists[fillLine]\n\tfor segment in horizontalEndpoints:\n\t\taddSparseEndpointsFromSegment(doubleExtrusionWidth, endpoints, fillLine, horizontalSegmentLists, infillSolidity, removedEndpoints, segment, solidSurfaceThickness, surroundingXIntersections)\n",["add sparse endpoints ."]]
["@csrf_protect\n@permission_required('comments.can_moderate')\ndef approve(request, comment_id, next=None):\n\tcomment = get_object_or_404(comments.get_model(), pk=comment_id, site__pk=settings.SITE_ID)\n\tif (request.method == 'POST'):\n\t\tperform_approve(request, comment)\n\t\treturn next_redirect(request, next, approve_done, c=comment.pk)\n\telse:\n\t\treturn render_to_response('comments\/approve.html', {'comment': comment, 'next': next}, template.RequestContext(request))\n",["approve a comment ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["confirm this module is on a debian based system ."]]
["def socket_pair():\n\tport = socket()\n\tport.bind(('', 0))\n\tport.listen(1)\n\tclient = socket()\n\tclient.setblocking(False)\n\tclient.connect_ex(('127.0.0.1', port.getsockname()[1]))\n\tclient.setblocking(True)\n\tserver = port.accept()[0]\n\tserver.send(b('x'))\n\tassert (client.recv(1024) == b('x'))\n\tclient.send(b('y'))\n\tassert (server.recv(1024) == b('y'))\n\tserver.setblocking(False)\n\tclient.setblocking(False)\n\treturn (server, client)\n",["establish and return a pair of network sockets connected to each other ."]]
["def ceil_shift(n, b):\n\tif ((not isinstance(n, (int, long))) or (not isinstance(b, (int, long)))):\n\t\traise TypeError(('unsupported\toperand\ttype(s):\t%r\tand\t%r' % (type(n).__name__, type(b).__name__)))\n\tassert ((n >= 0) and (b >= 0))\n\tmask = ((1L << b) - 1)\n\tif (n & mask):\n\t\treturn ((n >> b) + 1)\n\telse:\n\t\treturn (n >> b)\n",["return ceil without performing any floating-point or division operations ."]]
["def _get_location(vm_=None):\n\tlocations = avail_locations()\n\tvm_location = str(config.get_cloud_config_value('zone', vm_, __opts__, search_global=False))\n\tif (not vm_location):\n\t\traise SaltCloudNotFound('No\tlocation\tspecified\tfor\tthis\tVM.')\n\tif (vm_location in locations):\n\t\treturn vm_location\n\traise SaltCloudNotFound(\"The\tspecified\tlocation,\t'{0}',\tcould\tnot\tbe\tfound.\".format(vm_location))\n",["return the vms location ."]]
["def fire_coroutine_threadsafe(coro, loop):\n\tident = loop.__dict__.get('_thread_ident')\n\tif ((ident is not None) and (ident == threading.get_ident())):\n\t\traise RuntimeError('Cannot\tbe\tcalled\tfrom\twithin\tthe\tevent\tloop')\n\tif (not coroutines.iscoroutine(coro)):\n\t\traise TypeError(('A\tcoroutine\tobject\tis\trequired:\t%s' % coro))\n\tdef callback():\n\t\t'Callback\tto\tfire\tcoroutine.'\n\t\tensure_future(coro, loop=loop)\n\tloop.call_soon_threadsafe(callback)\n\treturn\n",["submit a coroutine object to a given event loop ."]]
["def test_install_from_wheel_gui_entrypoint(script, data):\n\tresult = script.pip('install', 'script.wheel3==0.1', '--no-index', ('--find-links=' + data.find_links), expect_error=False)\n\tif (os.name == 'nt'):\n\t\twrapper_file = (script.bin \/ 't1.exe')\n\telse:\n\t\twrapper_file = (script.bin \/ 't1')\n\tassert (wrapper_file in result.files_created)\n",["test installing scripts ."]]
["def test_basic_call_on_method_through_api_instance_coroutine():\n\tclass API(object, ):\n\t\tdef hello_world(self):\n\t\t\treturn 'Hello\tWorld!'\n\tapi_instance = API()\n\t@hug.call()\n\t@asyncio.coroutine\n\tdef hello_world():\n\t\treturn api_instance.hello_world()\n\tassert (api_instance.hello_world() == 'Hello\tWorld!')\n\tassert (hug.test.get(api, '\/hello_world').data == 'Hello\tWorld!')\n",["test to ensure the most basic call still works if applied to a method ."]]
["@pytest.fixture(scope='module', params=list(gpu_cpu_32_16), ids=idfunc)\ndef backend_tests(request):\n\tbe = gen_backend(backend=request.param[0], datatype=request.param[1], batch_size=128, device_id=request.config.getoption('--device_id'), rng_seed=0)\n\tdef cleanup():\n\t\tbe = request.getfuncargvalue('backend_tests')\n\t\tdel be\n\trequest.addfinalizer(cleanup)\n\treturn be\n",["fixture that returns cpu and gpu backends for 16 and 32 bit ."]]
["def write_file(filename, contents):\n\tf = open(filename, 'w')\n\ttry:\n\t\tfor line in contents:\n\t\t\tf.write((line + '\\n'))\n\tfinally:\n\t\tf.close()\n",["write the output file for module\/package <name> ."]]
["def parse_opcode_signature(env, sig, signode):\n\tm = opcode_sig_re.match(sig)\n\tif (m is None):\n\t\traise ValueError\n\t(opname, arglist) = m.groups()\n\tsignode += addnodes.desc_name(opname, opname)\n\tif (arglist is not None):\n\t\tparamlist = addnodes.desc_parameterlist()\n\t\tsignode += paramlist\n\t\tparamlist += addnodes.desc_parameter(arglist, arglist)\n\treturn opname.strip()\n",["transform an opcode signature into rst nodes ."]]
["def checkcache(filename=None):\n\tif (filename is None):\n\t\tfilenames = list(cache.keys())\n\telif (filename in cache):\n\t\tfilenames = [filename]\n\telse:\n\t\treturn\n\tfor filename in filenames:\n\t\t(size, mtime, lines, fullname) = cache[filename]\n\t\tif (mtime is None):\n\t\t\tcontinue\n\t\ttry:\n\t\t\tstat = os.stat(fullname)\n\t\texcept OSError:\n\t\t\tdel cache[filename]\n\t\t\tcontinue\n\t\tif ((size != stat.st_size) or (mtime != stat.st_mtime)):\n\t\t\tdel cache[filename]\n",["discard cache entries that are out of date ."]]
["def is_opentype_cff_font(filename):\n\tif (os.path.splitext(filename)[1].lower() == u'.otf'):\n\t\tresult = _is_opentype_cff_font_cache.get(filename)\n\t\tif (result is None):\n\t\t\twith open(filename, u'rb') as fd:\n\t\t\t\ttag = fd.read(4)\n\t\t\tresult = (tag == 'OTTO')\n\t\t\t_is_opentype_cff_font_cache[filename] = result\n\t\treturn result\n\treturn False\n",["returns true if the given font is a postscript compact font format font embedded in an opentype wrapper ."]]
["def do_get_current_language_bidi(parser, token):\n\targs = token.contents.split()\n\tif ((len(args) != 3) or (args[1] != 'as')):\n\t\traise TemplateSyntaxError((\"'get_current_language_bidi'\trequires\t'as\tvariable'\t(got\t%r)\" % args))\n\treturn GetCurrentLanguageBidiNode(args[2])\n",["this will store the current language layout in the context ."]]
["def test_bc_fit():\n\tratio = 'auto'\n\tbc = BalanceCascade(ratio=ratio, random_state=RND_SEED)\n\tbc.fit(X, Y)\n\tassert_equal(bc.min_c_, 0)\n\tassert_equal(bc.maj_c_, 1)\n\tassert_equal(bc.stats_c_[0], 8)\n\tassert_equal(bc.stats_c_[1], 12)\n",["test the fitting method ."]]
["def test_iht_sample_wrong_X():\n\tiht = InstanceHardnessThreshold(random_state=RND_SEED)\n\tiht.fit(X, Y)\n\tassert_raises(RuntimeError, iht.sample, np.random.random((100, 40)), np.array((([0] * 50) + ([1] * 50))))\n",["test either if an error is raised when x is different at fitting and sampling ."]]
["def _get_data():\n\traw = read_raw_fif(raw_fname).crop(0.0, 5.0).load_data()\n\tdata_picks = pick_types(raw.info, meg=True, eeg=True)\n\tother_picks = pick_types(raw.info, meg=False, stim=True, eog=True)\n\tpicks = np.sort(np.concatenate((data_picks[::16], other_picks)))\n\traw = raw.pick_channels([raw.ch_names[p] for p in picks])\n\traw.info.normalize_proj()\n\tecg = RawArray(np.zeros((1, len(raw.times))), create_info(['ECG\t063'], raw.info['sfreq'], 'ecg'))\n\tfor key in ('dev_head_t', 'buffer_size_sec', 'highpass', 'lowpass', 'dig'):\n\t\tecg.info[key] = raw.info[key]\n\traw.add_channels([ecg])\n\tsrc = read_source_spaces(src_fname)\n\ttrans = read_trans(trans_fname)\n\tsphere = make_sphere_model('auto', 'auto', raw.info)\n\tstc = _make_stc(raw, src)\n\treturn (raw, src, stc, trans, sphere)\n",["get data ."]]
["def H_from_ransac(fp, tp, model, maxiter=1000, match_theshold=10):\n\timport ransac\n\tdata = vstack((fp, tp))\n\t(H, ransac_data) = ransac.ransac(data.T, model, 4, maxiter, match_theshold, 10, return_all=True)\n\treturn (H, ransac_data['inliers'])\n",["robust estimation of homography h from point correspondences using ransac ."]]
["def allow_lazy(func, *resultclasses):\n\t@wraps(func)\n\tdef wrapper(*args, **kwargs):\n\t\tfor arg in (list(args) + list(six.itervalues(kwargs))):\n\t\t\tif isinstance(arg, Promise):\n\t\t\t\tbreak\n\t\telse:\n\t\t\treturn func(*args, **kwargs)\n\t\treturn lazy(func, *resultclasses)(*args, **kwargs)\n\treturn wrapper\n",["a decorator that allows a function to be called with one or more lazy arguments ."]]
["def get_minions():\n\tserv = _get_serv(ret=None)\n\tsql = 'select\tdistinct(id)\tfrom\treturns'\n\tdata = serv.query(sql)\n\tret = []\n\tif data:\n\t\tfor jid in data[0]['points']:\n\t\t\tret.append(jid[1])\n\treturn ret\n",["return a list of minions ."]]
["def upload():\n\tos.system('cd\tbuild\/html;\trsync\t-avz\t.\tpandas@pandas.pydata.org:\/usr\/share\/nginx\/pandas\/pandas-docs\/vbench\/\t-essh')\n",["connect to server and upload files ."]]
["def isotime(at=None, subsecond=False):\n\tif (not at):\n\t\tat = utcnow()\n\tst = at.strftime((_ISO8601_TIME_FORMAT if (not subsecond) else _ISO8601_TIME_FORMAT_SUBSECOND))\n\ttz = (at.tzinfo.tzname(None) if at.tzinfo else 'UTC')\n\tst += ('Z' if (tz == 'UTC') else tz)\n\treturn st\n",["stringify time in iso 8601 format ."]]
["def do_reverse(value):\n\tif isinstance(value, basestring):\n\t\treturn value[::(-1)]\n\ttry:\n\t\treturn reversed(value)\n\texcept TypeError:\n\t\ttry:\n\t\t\trv = list(value)\n\t\t\trv.reverse()\n\t\t\treturn rv\n\t\texcept TypeError:\n\t\t\traise FilterArgumentError('argument\tmust\tbe\titerable')\n",["reverse the object or return an iterator the iterates over it the other way round ."]]
["def RemoveFlags(flag_values=FLAGS):\n\tfor flag_name in NamesOfDefinedFlags():\n\t\tmodule_bar.RemoveOneFlag(flag_name, flag_values=flag_values)\n\tmodule_bar.RemoveFlags(flag_values=flag_values)\n",["deletes the flag definitions done by the above defineflags() ."]]
["def getCubicPathByBeginEnd(begin, controlPoints, elementNode, end):\n\treturn svg_reader.getCubicPoints(begin, controlPoints, end, lineation.getNumberOfBezierPoints(begin, elementNode, end))\n",["get the cubic path by begin and end ."]]
["def print_(*strings, **kwargs):\n\tif (not strings):\n\t\tstrings = [u'']\n\tassert isinstance(strings[0], six.text_type)\n\ttxt = u'\t'.join(strings)\n\ttxt += kwargs.get('end', u'\\n')\n\tif six.PY2:\n\t\ttxt = txt.encode(_out_encoding(), 'replace')\n\tsys.stdout.write(txt)\n",["like print ."]]
["def nlargest(n, iterable, key=None):\n\tif (n == 1):\n\t\tit = iter(iterable)\n\t\thead = list(islice(it, 1))\n\t\tif (not head):\n\t\t\treturn []\n\t\tif (key is None):\n\t\t\treturn [max(chain(head, it))]\n\t\treturn [max(chain(head, it), key=key)]\n\ttry:\n\t\tsize = len(iterable)\n\texcept (TypeError, AttributeError):\n\t\tpass\n\telse:\n\t\tif (n >= size):\n\t\t\treturn sorted(iterable, key=key, reverse=True)[:n]\n\tif (key is None):\n\t\tit = izip(iterable, imap(neg, count()))\n\t\tresult = _nlargest(n, it)\n\t\treturn map(itemgetter(0), result)\n\t(in1, in2) = tee(iterable)\n\tit = izip(imap(key, in1), imap(neg, count()), in2)\n\tresult = _nlargest(n, it)\n\treturn map(itemgetter(2), result)\n",["find the n largest elements in a dataset ."]]
["def load_file(filename, maxbytes=_unspecified):\n\tfilename = _path_string(filename)\n\tif (maxbytes is _unspecified):\n\t\tmaxbytes = (-1)\n\telif (not isinstance(maxbytes, int)):\n\t\traise TypeError('maxbytes\tmust\tbe\tan\tinteger')\n\treturn _lib.RAND_load_file(filename, maxbytes)\n",["opens filename with encoding and return its contents ."]]
["def _loadLinuxSo():\n\ttry:\n\t\tl = ctypes.CDLL('liblabjackusb.so', use_errno=True)\n\texcept TypeError:\n\t\tl = ctypes.CDLL('liblabjackusb.so')\n\tl.LJUSB_Stream.errcheck = errcheck\n\tl.LJUSB_Read.errcheck = errcheck\n\treturn l\n",["attempts to load the liblabjackusb ."]]
["def run_migrations_online():\n\tconnectable = settings.engine\n\twith connectable.connect() as connection:\n\t\tcontext.configure(connection=connection, target_metadata=target_metadata, compare_type=COMPARE_TYPE)\n\t\twith context.begin_transaction():\n\t\t\tcontext.run_migrations()\n",["run migrations in online mode ."]]
["def image_member_delete(context, memb_id, session=None):\n\tsession = (session or get_session())\n\tmember_ref = _image_member_get(context, memb_id, session)\n\t_image_member_delete(context, member_ref, session)\n",["delete an imagemember object ."]]
["def module_to_dict(module, omittable=(lambda k: k.startswith('_'))):\n\treturn dict(((k, repr(v)) for (k, v) in module.__dict__.items() if (not omittable(k))))\n",["converts a module namespace to a python dictionary ."]]
["def test_sobel_v_mask():\n\tnp.random.seed(0)\n\tresult = filters.sobel_v(np.random.uniform(size=(10, 10)), np.zeros((10, 10), bool))\n\tassert_allclose(result, 0)\n",["vertical sobel on a masked array should be zero ."]]
["def check_err(code):\n\tif (code == OGRERR_NONE):\n\t\treturn\n\telif (code in OGRERR_DICT):\n\t\t(e, msg) = OGRERR_DICT[code]\n\t\traise e, msg\n\telse:\n\t\traise OGRException(('Unknown\terror\tcode:\t\"%s\"' % code))\n",["checks the given ogrerr ."]]
["def never_cache(view_func):\n\t@wraps(view_func)\n\tdef _wrapped_view_func(request, *args, **kwargs):\n\t\tresponse = view_func(request, *args, **kwargs)\n\t\tadd_never_cache_headers(response)\n\t\treturn response\n\treturn _wrapped_view_func\n",["decorator that adds headers to a response so that it will never be cached ."]]
["def decode(txt):\n\t_nid = NameID()\n\tfor part in txt.split(','):\n\t\tif (part.find('=') != (-1)):\n\t\t\t(i, val) = part.split('=')\n\t\t\ttry:\n\t\t\t\tsetattr(_nid, ATTR[int(i)], unquote(val))\n\t\t\texcept:\n\t\t\t\tpass\n\treturn _nid\n",["decode a raw base64 string ."]]
["def smart_text(s, encoding='utf-8', strings_only=False, errors='strict'):\n\tif isinstance(s, Promise):\n\t\treturn s\n\treturn force_text(s, encoding, strings_only, errors)\n",["returns a text object representing s -- unicode on python 2 and str on python 3 ."]]
["def get_keywords(lexer):\n\tif (not hasattr(lexer, 'tokens')):\n\t\treturn []\n\tif ('keywords' in lexer.tokens):\n\t\ttry:\n\t\t\treturn lexer.tokens['keywords'][0][0].words\n\t\texcept:\n\t\t\tpass\n\tkeywords = []\n\tfor vals in lexer.tokens.values():\n\t\tfor val in vals:\n\t\t\ttry:\n\t\t\t\tif isinstance(val[0], words):\n\t\t\t\t\tkeywords.extend(val[0].words)\n\t\t\t\telse:\n\t\t\t\t\tini_val = val[0]\n\t\t\t\t\tif ((')\\\\b' in val[0]) or (')(\\\\s+)' in val[0])):\n\t\t\t\t\t\tval = re.sub('\\\\\\\\.', '', val[0])\n\t\t\t\t\t\tval = re.sub('[^0-9a-zA-Z|]+', '', val)\n\t\t\t\t\t\tif ('|' in ini_val):\n\t\t\t\t\t\t\tkeywords.extend(val.split('|'))\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tkeywords.append(val)\n\t\t\texcept Exception:\n\t\t\t\tcontinue\n\treturn keywords\n",["get the keywords needed to look up the version information ."]]
["def iterate(obj, opts):\n\tchildren = obj.get_children(**opts)\n\tif (not children):\n\t\treturn [obj]\n\ttraversal = deque()\n\tstack = deque([obj])\n\twhile stack:\n\t\tt = stack.popleft()\n\t\ttraversal.append(t)\n\t\tfor c in t.get_children(**opts):\n\t\t\tstack.append(c)\n\treturn iter(traversal)\n",["traverse the given expression structure ."]]
["@magic_arguments('frobnicate')\n@argument('-f', '--foo', help='an\targument')\ndef magic_foo5(self, args):\n\treturn parse_argstring(magic_foo5, args)\n",["a docstring ."]]
["def _getAccessibleAttribute(attributeName, elementNode):\n\tif (attributeName in globalGetAccessibleAttributeSet):\n\t\treturn getattr(Document(elementNode), attributeName, None)\n\treturn None\n",["get the accessible attribute ."]]
["def escapedCDATA(data):\n\tif isinstance(data, unicode):\n\t\tdata = data.encode('utf-8')\n\treturn data.replace(']]>', ']]]]><![CDATA[>')\n",["escape cdata for inclusion in a document ."]]
["def FakeAccess(path, mode):\n\tif ((not os.path.exists(path)) or (mode != os.R_OK)):\n\t\treturn False\n\telse:\n\t\treturn True\n",["fake version of os ."]]
["def set_rules(rules, overwrite=True, use_conf=False):\n\tinit(use_conf=False)\n\t_ENFORCER.set_rules(rules, overwrite, use_conf)\n",["set rules based on the provided dict of rules ."]]
["def _cleanse_dict(original):\n\treturn dict(((k, v) for (k, v) in original.iteritems() if (not ('_pass' in k))))\n",["strip all admin_password ."]]
["def educate_dashes_oldschool_inverted(s):\n\treturn s.replace('---', '&#8211;').replace('--', '&#8212;')\n",["parameter: string ."]]
["def run_stderr(cmd, cwd=None, stdin=None, runas=None, shell=DEFAULT_SHELL, python_shell=None, env=None, clean_env=False, template=None, rstrip=True, umask=None, output_loglevel='debug', log_callback=None, timeout=None, reset_system_locale=True, ignore_retcode=False, saltenv='base', use_vt=False, password=None, **kwargs):\n\tpython_shell = _python_shell_default(python_shell, kwargs.get('__pub_jid', ''))\n\tret = _run(cmd, runas=runas, cwd=cwd, stdin=stdin, shell=shell, python_shell=python_shell, env=env, clean_env=clean_env, template=template, rstrip=rstrip, umask=umask, output_loglevel=output_loglevel, log_callback=log_callback, timeout=timeout, reset_system_locale=reset_system_locale, ignore_retcode=ignore_retcode, use_vt=use_vt, saltenv=saltenv, password=password, **kwargs)\n\tlog_callback = _check_cb(log_callback)\n\tlvl = _check_loglevel(output_loglevel)\n\tif (lvl is not None):\n\t\tif ((not ignore_retcode) and (ret['retcode'] != 0)):\n\t\t\tif (lvl < LOG_LEVELS['error']):\n\t\t\t\tlvl = LOG_LEVELS['error']\n\t\t\tmsg = \"Command\t'{0}'\tfailed\twith\treturn\tcode:\t{1}\".format(cmd, ret['retcode'])\n\t\t\tlog.error(log_callback(msg))\n\t\tif ret['stdout']:\n\t\t\tlog.log(lvl, 'stdout:\t{0}'.format(log_callback(ret['stdout'])))\n\t\tif ret['stderr']:\n\t\t\tlog.log(lvl, 'stderr:\t{0}'.format(log_callback(ret['stderr'])))\n\t\tif ret['retcode']:\n\t\t\tlog.log(lvl, 'retcode:\t{0}'.format(ret['retcode']))\n\treturn ret['stderr']\n",["wrapper for :py:func:cmdmod ."]]
["def disable(name, lbn, target, profile='default', tgt_type='glob', expr_form=None):\n\tif (expr_form is not None):\n\t\tsalt.utils.warn_until('Fluorine', \"the\ttarget\ttype\tshould\tbe\tpassed\tusing\tthe\t'tgt_type'\targument\tinstead\tof\t'expr_form'.\tSupport\tfor\tusing\t'expr_form'\twill\tbe\tremoved\tin\tSalt\tFluorine.\")\n\t\ttgt_type = expr_form\n\treturn _talk2modjk(name, lbn, target, 'worker_disable', profile, tgt_type)\n",["disable the named service to start at boot cli example: ."]]
["def num2strg(num):\n\ts = str(num)\n\tif s.endswith('.0'):\n\t\ts = s[:(-2)]\n\treturn s\n",["attempt to emulate excels default conversion from number to string ."]]
["def educate_ellipses(s):\n\treturn s.replace('...', '&#8230;').replace('.\t.\t.', '&#8230;')\n",["parameter: string ."]]
["def getstatus(file):\n\timport warnings\n\twarnings.warn('commands.getstatus()\tis\tdeprecated', DeprecationWarning)\n\treturn getoutput(('ls\t-ld' + mkarg(file)))\n",["return output of \"ls -ld <file>\" in a string ."]]
["def get_bind_addr(conf, default_port=None):\n\treturn (conf.bind_host, (conf.bind_port or default_port))\n",["return the host and port to bind to ."]]
["@register.filter(is_safe=True)\n@stringfilter\ndef striptags(value):\n\treturn strip_tags(value)\n",["strips all [x]html tags ."]]
["def unbox_usecase3(x):\n\t(a, b) = x\n\tres = a\n\tfor v in b:\n\t\tres += v\n\treturn res\n",["expect a tuple ."]]
["def _api_get_cats(name, output, kwargs):\n\treturn report(output, keyword='categories', data=list_cats(False))\n",["api: accepts output ."]]
["def cmd_list_available(args, opts):\n\tfor x in jsonrpc_call(opts, 'crawler\/spiders', 'list'):\n\t\tprint x\n",["list-available - list name of available spiders ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only work on proxy ."]]
["@task\ndef html(options):\n\tsubprocess.check_call(['make', 'html'], cwd='doc')\n\tbuiltdocs = ((paver.path.path('doc') \/ options.sphinx.builddir) \/ 'html')\n\toptions.html.builddir.rmtree()\n\tbuiltdocs.copytree(options.html.builddir)\n",["downloads the resource at the given url and parses via beautifulsoup ."]]
["def timedelta_to_integral_minutes(delta):\n\treturn (timedelta_to_integral_seconds(delta) \/\/ 60)\n",["convert a pd ."]]
["def format_rfc3339(datetime_instance=None):\n\treturn (datetime_instance.isoformat('T') + 'Z')\n",["formats a datetime per rfc 3339 ."]]
["def ismount(path):\n\ttry:\n\t\treturn ismount_raw(path)\n\texcept OSError:\n\t\treturn False\n",["test whether a path is a mount point ."]]
["def post():\n\ttablename = 'cms_post'\n\ts3db.set_method(module, resourcename, method='discuss', action=discuss)\n\tdef prep(r):\n\t\tif r.interactive:\n\t\t\tif (r.method in ('create', 'update')):\n\t\t\t\ttable = r.table\n\t\t\t\tseries = get_vars.get('~.series_id$name', None)\n\t\t\t\tif series:\n\t\t\t\t\tstable = db.cms_series\n\t\t\t\t\trow = db((stable.name == series)).select(stable.id, limitby=(0, 1)).first()\n\t\t\t\t\tif row:\n\t\t\t\t\t\tfield = table.series_id\n\t\t\t\t\t\tfield.default = row.id\n\t\t\t\t\t\tfield.readable = field.writable = False\n\t\t\t\tlocation_id = get_vars.get('(location)', None)\n\t\t\t\tif location_id:\n\t\t\t\t\tfield = table.location_id\n\t\t\t\t\tfield.default = location_id\n\t\t\t\t\tfield.readable = field.writable = False\n\t\t\t\tpage = get_vars.get('page', None)\n\t\t\t\turl = get_vars.get('url')\n\t\t\t\tif page:\n\t\t\t\t\ttable.name.default = page\n\t\t\t\t\ttable.name.readable = table.name.writable = False\n\t\t\t\t\t_crud = s3.crud_strings[tablename]\n\t\t\t\t\t_crud.label_create = T('New\tPage')\n\t\t\t\t\t_crud.title_update = T('Edit\tPage')\n\t\t\t\t\tif (not url):\n\t\t\t\t\t\turl = URL(c='default', f='index', vars={'page': page})\n\t\t\t\t\ts3db.configure(tablename, create_next=url, update_next=url)\n\t\t\t\t_module = get_vars.get('module', None)\n\t\t\t\tif _module:\n\t\t\t\t\ttable.avatar.readable = table.avatar.writable = False\n\t\t\t\t\ttable.location_id.readable = table.location_id.writable = False\n\t\t\t\t\ttable.date.readable = table.date.writable = False\n\t\t\t\t\ttable.expired.readable = table.expired.writable = False\n\t\t\t\t\ttable.body.widget = s3base.s3_richtext_widget\n\t\t\t\t\tresource = get_vars.get('resource', None)\n\t\t\t\t\tif (resource in ('about', 'contact', 'help', 'index')):\n\t\t\t\t\t\tif (resource == 'about'):\n\t\t\t\t\t\t\ttable.name.default = 'About\tPage'\n\t\t\t\t\t\telif (resource == 'contact'):\n\t\t\t\t\t\t\ttable.name.default = 'Contact\tPage'\n\t\t\t\t\t\telif (resource == 'help'):\n\t\t\t\t\t\t\ttable.name.default = 'Help\tPage'\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\ttable.name.default = 'Home\tPage'\n\t\t\t\t\t\ttable.replies.readable = table.replies.writable = False\n\t\t\t\t\t\tif (not url):\n\t\t\t\t\t\t\turl = URL(c=_module, f=resource)\n\t\t\t\t\telse:\n\t\t\t\t\t\trecord = get_vars.get('record', None)\n\t\t\t\t\t\tif record:\n\t\t\t\t\t\t\ttable.name.default = ('%s\t%s\tProfile\tPage' % (resource, record))\n\t\t\t\t\t\t\ttable.title.readable = table.title.writable = False\n\t\t\t\t\t\t\ttable.replies.readable = table.replies.writable = False\n\t\t\t\t\t\t\tif (not url):\n\t\t\t\t\t\t\t\turl = URL(c=_module, f=resource, args=[record, 'profile'])\n\t\t\t\t\t\telif resource:\n\t\t\t\t\t\t\ttable.name.default = ('%s\tSummary\tPage\tHeader' % resource)\n\t\t\t\t\t\t\ttable.title.readable = table.title.writable = False\n\t\t\t\t\t\t\ttable.replies.readable = table.replies.writable = False\n\t\t\t\t\t\t\tif (not url):\n\t\t\t\t\t\t\t\turl = URL(c=_module, f=resource, args='summary')\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\ttable.name.default = ('%s\tHome\tPage' % _module)\n\t\t\t\t\t\t\t_crud = s3.crud_strings[tablename]\n\t\t\t\t\t\t\t_crud.label_create = T('New\tPage')\n\t\t\t\t\t\t\t_crud.title_update = T('Edit\tPage')\n\t\t\t\t\t\t\tif (not url):\n\t\t\t\t\t\t\t\turl = URL(c=_module, f='index')\n\t\t\t\t\ts3db.configure(tablename, create_next=url, update_next=url)\n\t\t\t\tlayer_id = get_vars.get('layer_id', None)\n\t\t\t\tif layer_id:\n\t\t\t\t\ttable.name.default = ('Metadata\tPage\tfor\tLayer\t%s' % layer_id)\n\t\t\t\t\ttable.name.readable = table.name.writable = False\n\t\t\t\t\ttable.avatar.readable = table.avatar.writable = False\n\t\t\t\t\ttable.location_id.readable = table.location_id.writable = False\n\t\t\t\t\ttable.title.readable = table.title.writable = False\n\t\t\t\t\ttable.replies.readable = table.replies.writable = False\n\t\t\t\t\ttable.date.readable = table.date.writable = False\n\t\t\t\t\ttable.expired.readable = table.expired.writable = False\n\t\t\t\t\t_crud = s3.crud_strings[tablename]\n\t\t\t\t\t_crud.label_create = T('Add\tMetadata')\n\t\t\t\t\t_crud.title_update = T('Edit\tMetadata')\n\t\t\t\tif (r.component_name == 'module'):\n\t\t\t\t\tmodules = {}\n\t\t\t\t\t_modules = current.deployment_settings.modules\n\t\t\t\t\tfor module in _modules:\n\t\t\t\t\t\tif (module in ('appadmin', 'errors', 'ocr')):\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tmodules[module] = _modules[module].name_nice\n\t\t\t\t\ts3db.cms_post_module.field.requires = IS_IN_SET_LAZY((lambda : sort_dict_by_values(modules)))\n\t\treturn True\n\ts3.prep = prep\n\toutput = s3_rest_controller(rheader=s3db.cms_rheader)\n\treturn output\n",["sends a post request ."]]
["def double_redirect_view(request):\n\treturn HttpResponseRedirect('\/test_client\/permanent_redirect_view\/')\n",["a view that redirects all requests to a redirection view ."]]
["def make_style(config_string=''):\n\tstyle = Style()\n\tcolor_settings = termcolors.parse_color_setting(config_string)\n\tfor role in termcolors.PALETTES[termcolors.NOCOLOR_PALETTE]:\n\t\tif color_settings:\n\t\t\tformat = color_settings.get(role, {})\n\t\t\tstyle_func = termcolors.make_style(**format)\n\t\telse:\n\t\t\tdef style_func(x):\n\t\t\t\treturn x\n\t\tsetattr(style, role, style_func)\n\tstyle.ERROR_OUTPUT = style.ERROR\n\treturn style\n",["returns a function with default parameters for colorize() example: bold_red = make_style(opts= ."]]
["def get_config_var(name):\n\treturn get_config_vars().get(name)\n",["return the value of a single variable using the dictionary returned by get_config_vars() ."]]
["def test_finalize():\n\tassert ('' == render('{{\tx\t}}', {'x': None}))\n",["we want none to show up as ."]]
["def group(seq, size):\n\tif (not hasattr(seq, 'next')):\n\t\tseq = iter(seq)\n\twhile True:\n\t\t(yield [seq.next() for i in xrange(size)])\n",["team controller - uses the group table from pr ."]]
["def _get_marker_param(params):\n\treturn params.pop('marker', None)\n",["extract marker id from request or fail ."]]
["def init(mpstate):\n\treturn SerialModule(mpstate)\n",["initialise module ."]]
["def processXMLElement(xmlElement):\n\tgroup.processShape(Difference, xmlElement)\n",["process the xml element ."]]
["def educate_quotes(s):\n\ts = single_quote_start_re.sub('&#8217;', s)\n\ts = double_quote_start_re.sub('&#8221;', s)\n\ts = double_quote_sets_re.sub('&#8220;&#8216;', s)\n\ts = single_quote_sets_re.sub('&#8216;&#8220;', s)\n\ts = decade_abbr_re.sub('&#8217;', s)\n\ts = opening_single_quotes_regex.sub('\\\\1&#8216;', s)\n\ts = closing_single_quotes_regex.sub('\\\\1&#8217;', s)\n\ts = closing_single_quotes_regex_2.sub('\\\\1&#8217;\\\\2', s)\n\ts = s.replace(\"'\", '&#8216;')\n\ts = opening_double_quotes_regex.sub('\\\\1&#8220;', s)\n\ts = closing_double_quotes_regex.sub('&#8221;', s)\n\ts = closing_double_quotes_regex_2.sub('\\\\1&#8221;', s)\n\treturn s.replace('\"', '&#8220;')\n",["parameter: string ."]]
["def get_docstring_and_rest(filename):\n\twith open(filename, 'rb') as fid:\n\t\tcontent = fid.read()\n\tcontent = content.replace('\\r\\n', '\\n')\n\ttry:\n\t\tnode = ast.parse(content)\n\texcept SyntaxError:\n\t\treturn (SYNTAX_ERROR_DOCSTRING, content.decode('utf-8'))\n\tif (not isinstance(node, ast.Module)):\n\t\traise TypeError('This\tfunction\tonly\tsupports\tmodules.\tYou\tprovided\t{0}'.format(node.__class__.__name__))\n\tif (node.body and isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, ast.Str)):\n\t\tdocstring_node = node.body[0]\n\t\tdocstring = docstring_node.value.s\n\t\tif hasattr(docstring, 'decode'):\n\t\t\tdocstring = docstring.decode('utf-8')\n\t\trest = content.decode('utf-8').split('\\n', docstring_node.lineno)[(-1)]\n\t\treturn (docstring, rest)\n\telse:\n\t\traise ValueError('Could\tnot\tfind\tdocstring\tin\tfile\t\"{0}\".\tA\tdocstring\tis\trequired\tby\tsphinx-gallery'.format(filename))\n",["separate filename content between docstring and the rest strongly inspired from ast ."]]
["def test_hsl_to_rgb_part_13():\n\tassert (hsl_to_rgb(0, 100, 0) == (0, 0, 0))\n\tassert (hsl_to_rgb(0, 100, 10) == (51, 0, 0))\n\tassert (hsl_to_rgb(0, 100, 20) == (102, 0, 0))\n\tassert (hsl_to_rgb(0, 100, 30) == (153, 0, 0))\n\tassert (hsl_to_rgb(0, 100, 40) == (204, 0, 0))\n\tassert (hsl_to_rgb(0, 100, 50) == (255, 0, 0))\n\tassert (hsl_to_rgb(0, 100, 60) == (255, 51, 51))\n\tassert (hsl_to_rgb(0, 100, 70) == (255, 102, 102))\n\tassert (hsl_to_rgb(0, 100, 80) == (255, 153, 153))\n\tassert (hsl_to_rgb(0, 100, 90) == (255, 204, 204))\n\tassert (hsl_to_rgb(0, 100, 100) == (255, 255, 255))\n",["test hsl to rgb color function ."]]
["def serialize_all(nodes, stream=None, Dumper=Dumper, canonical=None, indent=None, width=None, allow_unicode=None, line_break=None, encoding='utf-8', explicit_start=None, explicit_end=None, version=None, tags=None):\n\tgetvalue = None\n\tif (stream is None):\n\t\ttry:\n\t\t\tfrom cStringIO import StringIO\n\t\texcept ImportError:\n\t\t\tfrom StringIO import StringIO\n\t\tstream = StringIO()\n\t\tgetvalue = stream.getvalue\n\tdumper = Dumper(stream, canonical=canonical, indent=indent, width=width, allow_unicode=allow_unicode, line_break=line_break, encoding=encoding, version=version, tags=tags, explicit_start=explicit_start, explicit_end=explicit_end)\n\tdumper.open()\n\tfor node in nodes:\n\t\tdumper.serialize(node)\n\tdumper.close()\n\tif getvalue:\n\t\treturn getvalue()\n",["serialize a sequence of representation trees into a yaml stream ."]]
["def template():\n\tdef prep(r):\n\t\tif r.component:\n\t\t\tif (r.component_name == 'translate'):\n\t\t\t\ttable = s3db.survey_translate\n\t\t\t\tif (r.component_id == None):\n\t\t\t\t\ttable.file.readable = False\n\t\t\t\t\ttable.file.writable = False\n\t\t\t\telse:\n\t\t\t\t\ttable.language.writable = False\n\t\t\t\t\ttable.code.writable = False\n\t\t\t\ts3db.configure('survey_translate', deletable=False)\n\t\telse:\n\t\t\ttable = r.table\n\t\t\ts3_action_buttons(r)\n\t\t\trows = db((table.status == 1)).select(table.id)\n\t\t\ttry:\n\t\t\t\ts3.actions[1]['restrict'].extend((str(row.id) for row in rows))\n\t\t\texcept KeyError:\n\t\t\t\ts3.actions[1]['restrict'] = [str(row.id) for row in rows]\n\t\t\texcept IndexError:\n\t\t\t\tpass\n\t\t\ts3.dataTableStyleAlert = [str(row.id) for row in rows]\n\t\t\trows = db((table.status == 3)).select(table.id)\n\t\t\ts3.dataTableStyleDisabled = [str(row.id) for row in rows]\n\t\t\ts3.dataTableStyleWarning = [str(row.id) for row in rows]\n\t\t\trows = db((table.status == 4)).select(table.id)\n\t\t\ts3.dataTableStyleWarning.extend((str(row.id) for row in rows))\n\t\t\ts3db.configure('survey_template', orderby='survey_template.status', create_next=URL(c='survey', f='template'), update_next=URL(c='survey', f='template'))\n\t\treturn True\n\ts3.prep = prep\n\tdef postp(r, output):\n\t\tif r.component:\n\t\t\ttemplate_id = r.id\n\t\t\tif (r.component_name == 'translate'):\n\t\t\t\ts3_action_buttons(r)\n\t\t\t\ts3.actions.extend([dict(label=str(T('Download')), _class='action-btn', url=r.url(method='translate_download', component='translate', component_id='[id]', representation='xls')), dict(label=str(T('Upload')), _class='action-btn', url=URL(c=module, f='template', args=[template_id, 'translate', '[id]']))])\n\t\treturn output\n\ts3.postp = postp\n\tif request.ajax:\n\t\tpost = request.post_vars\n\t\taction = post.get('action')\n\t\ttemplate_id = post.get('parent_id')\n\t\tsection_id = post.get('section_id')\n\t\tsection_text = post.get('section_text')\n\t\tif ((action == 'section') and (template_id != None)):\n\t\t\tid = db.survey_section.insert(name=section_text, template_id=template_id, cloned_section_id=section_id)\n\t\t\tif (id is None):\n\t\t\t\tprint 'Failed\tto\tinsert\trecord'\n\t\t\treturn\n\ts3db.configure('survey_template', listadd=False)\n\toutput = s3_rest_controller(rheader=s3db.survey_template_rheader)\n\treturn output\n",["get a rendered template as a string iterator ."]]
["def dmp_add(f, g, u, K):\n\tif (not u):\n\t\treturn dup_add(f, g, K)\n\tdf = dmp_degree(f, u)\n\tif (df < 0):\n\t\treturn g\n\tdg = dmp_degree(g, u)\n\tif (dg < 0):\n\t\treturn f\n\tv = (u - 1)\n\tif (df == dg):\n\t\treturn dmp_strip([dmp_add(a, b, v, K) for (a, b) in zip(f, g)], u)\n\telse:\n\t\tk = abs((df - dg))\n\t\tif (df > dg):\n\t\t\t(h, f) = (f[:k], f[k:])\n\t\telse:\n\t\t\t(h, g) = (g[:k], g[k:])\n\t\treturn (h + [dmp_add(a, b, v, K) for (a, b) in zip(f, g)])\n",["add dense polynomials in k[x] ."]]
["def load_pkcs7_data(type, buffer):\n\tif isinstance(buffer, _text_type):\n\t\tbuffer = buffer.encode('ascii')\n\tbio = _new_mem_buf(buffer)\n\tif (type == FILETYPE_PEM):\n\t\tpkcs7 = _lib.PEM_read_bio_PKCS7(bio, _ffi.NULL, _ffi.NULL, _ffi.NULL)\n\telif (type == FILETYPE_ASN1):\n\t\tpkcs7 = _lib.d2i_PKCS7_bio(bio, _ffi.NULL)\n\telse:\n\t\traise ValueError('type\targument\tmust\tbe\tFILETYPE_PEM\tor\tFILETYPE_ASN1')\n\tif (pkcs7 == _ffi.NULL):\n\t\t_raise_current_error()\n\tpypkcs7 = PKCS7.__new__(PKCS7)\n\tpypkcs7._pkcs7 = _ffi.gc(pkcs7, _lib.PKCS7_free)\n\treturn pypkcs7\n",["load pkcs7 data from a buffer ."]]
["def getTransformedPathByPrefix(path, prefix, xmlElement):\n\tif (len(path) < 2):\n\t\tprint 'Warning,\tbug,\tpath\tis\ttoo\tsmall\tin\tevaluate\tin\tsetPathByPrefix.'\n\t\treturn\n\tpathByKey = getTransformedPathByKey((prefix + 'path'), xmlElement)\n\tif (len(pathByKey) < len(path)):\n\t\tfor pointIndex in xrange(len(pathByKey)):\n\t\t\tpath[pointIndex] = pathByKey[pointIndex]\n\telse:\n\t\tpath = pathByKey\n\tpath[0] = getVector3ByPrefix(path[0], (prefix + 'pathStart'), xmlElement)\n\tpath[(-1)] = getVector3ByPrefix(path[(-1)], (prefix + 'pathEnd'), xmlElement)\n\treturn path\n",["get path from prefix and xml element ."]]
["def _CopyQueryOptionsObjectToProtocolBuffer(query, options, params):\n\toffset = 0\n\tweb_safe_string = None\n\tcursor_type = None\n\toffset = options.offset\n\tif options.cursor:\n\t\tcursor = options.cursor\n\t\tif cursor.per_result:\n\t\t\tcursor_type = search_service_pb.SearchParams.PER_RESULT\n\t\telse:\n\t\t\tcursor_type = search_service_pb.SearchParams.SINGLE\n\t\tif (isinstance(cursor, Cursor) and cursor.web_safe_string):\n\t\t\tweb_safe_string = cursor._internal_cursor\n\t_CopyQueryOptionsToProtocolBuffer(query, offset, options.limit, options.number_found_accuracy, web_safe_string, cursor_type, options.ids_only, options.returned_fields, options.snippeted_fields, options.returned_expressions, options.sort_options, params)\n",["copies a queryoptions object to a searchparams proto buff ."]]
["def _space_prefix(pref, full, sep=None, indent=None, include_sep=True):\n\tif (sep is None):\n\t\tsep = os.path.sep\n\tpref = pref.split(sep)\n\tfull = full.split(sep)\n\tpadding = []\n\twhile (pref and full and (pref[0] == full[0])):\n\t\tif (indent is None):\n\t\t\tpadding.append(('\t' * (len(full[0]) + len(sep))))\n\t\telse:\n\t\t\tpadding.append(('\t' * indent))\n\t\tfull.pop(0)\n\t\tpref.pop(0)\n\tif padding:\n\t\tif include_sep:\n\t\t\treturn ((''.join(padding) + sep) + sep.join(full))\n\t\telse:\n\t\t\treturn (''.join(padding) + sep.join(full))\n\telse:\n\t\treturn sep.join(full)\n",["anything shared by pref and full will be replaced with spaces in full ."]]
["def _setwindowsize(folder_alias, (w, h)):\n\tfinder = _getfinder()\n\targs = {}\n\tattrs = {}\n\t_code = 'core'\n\t_subcode = 'setd'\n\taevar00 = [w, h]\n\taeobj_0 = aetypes.ObjectSpecifier(want=aetypes.Type('cfol'), form='alis', seld=folder_alias, fr=None)\n\taeobj_1 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('cwnd'), fr=aeobj_0)\n\taeobj_2 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('ptsz'), fr=aeobj_1)\n\targs['----'] = aeobj_2\n\targs['data'] = aevar00\n\t(_reply, args, attrs) = finder.send(_code, _subcode, args, attrs)\n\tif ('errn' in args):\n\t\traise Error, aetools.decodeerror(args)\n\treturn (w, h)\n",["set the size of a finder window for folder to ."]]
["def add_lazy_relation(cls, field, relation, operation):\n\tif (relation == RECURSIVE_RELATIONSHIP_CONSTANT):\n\t\tapp_label = cls._meta.app_label\n\t\tmodel_name = cls.__name__\n\telse:\n\t\ttry:\n\t\t\t(app_label, model_name) = relation.split('.')\n\t\texcept ValueError:\n\t\t\tapp_label = cls._meta.app_label\n\t\t\tmodel_name = relation\n\t\texcept AttributeError:\n\t\t\tapp_label = relation._meta.app_label\n\t\t\tmodel_name = relation._meta.object_name\n\tmodel = get_model(app_label, model_name, seed_cache=False, only_installed=False)\n\tif model:\n\t\toperation(field, model, cls)\n\telse:\n\t\tkey = (app_label, model_name)\n\t\tvalue = (cls, field, operation)\n\t\tpending_lookups.setdefault(key, []).append(value)\n",["adds a lookup on cls when a related field is defined using a string ."]]
["def dmp_sqf_norm(f, u, K):\n\tif (not u):\n\t\treturn dup_sqf_norm(f, K)\n\tif (not K.is_Algebraic):\n\t\traise DomainError('ground\tdomain\tmust\tbe\talgebraic')\n\tg = dmp_raise(K.mod.rep, (u + 1), 0, K.dom)\n\tF = dmp_raise([K.one, (- K.unit)], u, 0, K)\n\ts = 0\n\twhile True:\n\t\t(h, _) = dmp_inject(f, u, K, front=True)\n\t\tr = dmp_resultant(g, h, (u + 1), K.dom)\n\t\tif dmp_sqf_p(r, u, K.dom):\n\t\t\tbreak\n\t\telse:\n\t\t\t(f, s) = (dmp_compose(f, F, u, K), (s + 1))\n\treturn (s, f, r)\n",["square-free norm of f in k[x] ."]]
["def generate_jwt():\n\tnow = int(time.time())\n\theader_json = json.dumps({'typ': 'JWT', 'alg': 'RS256'})\n\tpayload_json = json.dumps({'iat': now, 'exp': (now + 3600), 'iss': DEFAUTL_SERVICE_ACCOUNT, 'scope': TARGET_AUD, 'aud': 'https:\/\/www.googleapis.com\/oauth2\/v4\/token'})\n\theaderAndPayload = '{}.{}'.format(base64.urlsafe_b64encode(header_json), base64.urlsafe_b64encode(payload_json))\n\t(key_name, signature) = app_identity.sign_blob(headerAndPayload)\n\tsigned_jwt = '{}.{}'.format(headerAndPayload, base64.urlsafe_b64encode(signature))\n\treturn signed_jwt\n",["generates a signed json web token using the google app engine default service account ."]]
["def u(p, r):\n\tif (r not in [1, 2]):\n\t\traise ValueError('Value\tof\tr\tshould\tlie\tbetween\t1\tand\t2')\n\t(p1, p2, p3) = p\n\tif (r == 1):\n\t\tksi = Matrix([[1], [0]])\n\telse:\n\t\tksi = Matrix([[0], [1]])\n\ta = (((((sigma1 * p1) + (sigma2 * p2)) + (sigma3 * p3)) \/ (E + m)) * ksi)\n\tif (a == 0):\n\t\ta = zeros(2, 1)\n\treturn (sqrt((E + m)) * Matrix([[ksi[(0, 0)]], [ksi[(1, 0)]], [a[(0, 0)]], [a[(1, 0)]]]))\n",["p = ; r = 0 ."]]
["def load_plugins(dirs):\n\tdirs = [os.path.expanduser(d) for d in dirs]\n\tfor directory in dirs:\n\t\tif os.path.isdir(directory):\n\t\t\tlivestreamer.load_plugins(directory)\n\t\telse:\n\t\t\tconsole.logger.warning('Plugin\tpath\t{0}\tdoes\tnot\texist\tor\tis\tnot\ta\tdirectory!', directory)\n",["imports the modules for a sequence of plugin names ."]]
["def _run_finalizers(minpriority=None):\n\tif (minpriority is None):\n\t\tf = (lambda p: (p[0][0] is not None))\n\telse:\n\t\tf = (lambda p: ((p[0][0] is not None) and (p[0][0] >= minpriority)))\n\titems = [x for x in _finalizer_registry.items() if f(x)]\n\titems.sort(reverse=True)\n\tfor (key, finalizer) in items:\n\t\tsub_debug('calling\t%s', finalizer)\n\t\ttry:\n\t\t\tfinalizer()\n\t\texcept Exception:\n\t\t\timport traceback\n\t\t\ttraceback.print_exc()\n\tif (minpriority is None):\n\t\t_finalizer_registry.clear()\n",["run all finalizers whose exit priority is not none and at least minpriority finalizers with highest priority are called first; finalizers with the same priority will be called"]]
["def recursive_repr(func):\n\trepr_running = set()\n\t@wraps(func)\n\tdef wrapper(self):\n\t\tkey = (id(self), get_ident())\n\t\tif (key in repr_running):\n\t\t\treturn '...'\n\t\trepr_running.add(key)\n\t\ttry:\n\t\t\treturn func(self)\n\t\tfinally:\n\t\t\trepr_running.discard(key)\n\treturn wrapper\n",["decorator to make a repr function return fillvalue for a recursive call ."]]
["@csrf_protect\n@permission_required('comments.can_moderate')\ndef delete(request, comment_id, next=None):\n\tcomment = get_object_or_404(comments.get_model(), pk=comment_id, site__pk=settings.SITE_ID)\n\tif (request.method == 'POST'):\n\t\tperform_delete(request, comment)\n\t\treturn next_redirect(request, next, delete_done, c=comment.pk)\n\telse:\n\t\treturn render_to_response('comments\/delete.html', {'comment': comment, 'next': next}, template.RequestContext(request))\n",["deletes a comment ."]]
["def tryall(context, prefix=None):\n\tcontext = context.copy()\n\tresults = {}\n\tfor (k, v) in context.iteritems():\n\t\tif (not hasattr(v, '__call__')):\n\t\t\tcontinue\n\t\tif (prefix and (not k.startswith(prefix))):\n\t\t\tcontinue\n\t\tprint (k + ':'),\n\t\ttry:\n\t\t\tr = v()\n\t\t\tdictincr(results, r)\n\t\t\tprint r\n\t\texcept:\n\t\t\tprint 'ERROR'\n\t\t\tdictincr(results, 'ERROR')\n\t\t\tprint ('\t\t\t' + '\\n\t\t\t'.join(traceback.format_exc().split('\\n')))\n\tprint ('-' * 40)\n\tprint 'results:'\n\tfor (k, v) in results.iteritems():\n\t\tprint ('\t' * 2), (str(k) + ':'), v\n",["tries a series of functions and prints their results ."]]
["def test_equalize_channels():\n\t(raw, events, picks) = _get_data()\n\tepochs1 = Epochs(raw, events, event_id, tmin, tmax, picks=picks, proj=False, preload=True)\n\tepochs2 = epochs1.copy()\n\tch_names = epochs1.ch_names[2:]\n\tepochs1.drop_channels(epochs1.ch_names[:1])\n\tepochs2.drop_channels(epochs2.ch_names[1:2])\n\tmy_comparison = [epochs1, epochs2]\n\tequalize_channels(my_comparison)\n\tfor e in my_comparison:\n\t\tassert_equal(ch_names, e.ch_names)\n",["test equalization of channels ."]]
["def get_script_name(environ):\n\tif (settings.FORCE_SCRIPT_NAME is not None):\n\t\treturn force_text(settings.FORCE_SCRIPT_NAME)\n\tscript_url = get_bytes_from_wsgi(environ, 'SCRIPT_URL', '')\n\tif (not script_url):\n\t\tscript_url = get_bytes_from_wsgi(environ, 'REDIRECT_URL', '')\n\tif script_url:\n\t\tif ('\/\/' in script_url):\n\t\t\tscript_url = _slashes_re.sub('\/', script_url)\n\t\tpath_info = get_bytes_from_wsgi(environ, 'PATH_INFO', '')\n\t\tscript_name = (script_url[:(- len(path_info))] if path_info else script_url)\n\telse:\n\t\tscript_name = get_bytes_from_wsgi(environ, 'SCRIPT_NAME', '')\n\treturn script_name.decode(UTF_8)\n",["returns the equivalent of the http requests script_name environment variable ."]]
["def indentXML(elem, level=0):\n\ti = (u'\\n' + (level * u'\t\t'))\n\tif len(elem):\n\t\tif ((not elem.text) or (not elem.text.strip())):\n\t\t\telem.text = (i + u'\t\t')\n\t\tif ((not elem.tail) or (not elem.tail.strip())):\n\t\t\telem.tail = i\n\t\tfor elem in elem:\n\t\t\tindentXML(elem, (level + 1))\n\t\tif ((not elem.tail) or (not elem.tail.strip())):\n\t\t\telem.tail = i\n\telif (level and ((not elem.tail) or (not elem.tail.strip()))):\n\t\telem.tail = i\n",["does our pretty printing ."]]
["def rldecode(data):\n\tdecoded = []\n\ti = 0\n\twhile (i < len(data)):\n\t\tlength = ord(data[i])\n\t\tif (length == 128):\n\t\t\tbreak\n\t\tif ((length >= 0) and (length < 128)):\n\t\t\trun = data[(i + 1):((i + 1) + (length + 1))]\n\t\t\tdecoded.append(run)\n\t\t\ti = ((i + 1) + (length + 1))\n\t\tif (length > 128):\n\t\t\trun = (data[(i + 1)] * (257 - length))\n\t\t\tdecoded.append(run)\n\t\t\ti = ((i + 1) + 1)\n\treturn ''.join(decoded)\n",["runlength decoder implementation based on pdf reference version 1 ."]]
["def setSVGCarvingCorners(cornerMaximum, cornerMinimum, layerHeight, loopLayers):\n\tfor loopLayer in loopLayers:\n\t\tfor loop in loopLayer.loops:\n\t\t\tfor point in loop:\n\t\t\t\tpointVector3 = Vector3(point.real, point.imag, loopLayer.z)\n\t\t\t\tcornerMaximum.maximize(pointVector3)\n\t\t\t\tcornerMinimum.minimize(pointVector3)\n\thalfLayerThickness = (0.5 * layerHeight)\n\tcornerMaximum.z += halfLayerThickness\n\tcornerMinimum.z -= halfLayerThickness\n",["parse svg text and store the layers ."]]
["def LoadSingleCron(cron_info, open_fn=None):\n\tbuilder = yaml_object.ObjectBuilder(CronInfoExternal)\n\thandler = yaml_builder.BuilderHandler(builder)\n\tlistener = yaml_listener.EventListener(handler)\n\tlistener.Parse(cron_info)\n\tcron_info = handler.GetResults()\n\tif (len(cron_info) < 1):\n\t\traise MalformedCronfigurationFile('Empty\tcron\tconfiguration.')\n\tif (len(cron_info) > 1):\n\t\traise MalformedCronfigurationFile('Multiple\tcron\tsections\tin\tconfiguration.')\n\treturn cron_info[0]\n",["load a cron ."]]
["def fix_uri_credentials(uri, to_quoted):\n\tif (not uri):\n\t\treturn\n\tlocation = glance.store.swift.StoreLocation({})\n\tif to_quoted:\n\t\tlocation.parse_uri = types.MethodType(legacy_parse_uri, location)\n\telse:\n\t\tlocation._get_credstring = types.MethodType(legacy__get_credstring, location)\n\tdecrypted_uri = None\n\ttry:\n\t\tdecrypted_uri = decrypt_location(uri)\n\texcept (TypeError, ValueError) as e:\n\t\traise exception.Invalid(str(e))\n\tlocation.parse_uri(decrypted_uri)\n\treturn encrypt_location(location.get_uri())\n",["fix the given uris embedded credentials by round-tripping with storelocation ."]]
["def update_qos(tenant_id, qos_id, new_qos_name=None):\n\tsession = db.get_session()\n\ttry:\n\t\tqos = session.query(l2network_models.QoS).filter_by(tenant_id=tenant_id).filter_by(qos_id=qos_id).one()\n\t\tif new_qos_name:\n\t\t\tqos['qos_name'] = new_qos_name\n\t\tsession.merge(qos)\n\t\tsession.flush()\n\t\treturn qos\n\texcept exc.NoResultFound:\n\t\traise c_exc.QosNotFound(qos_id=qos_id, tenant_id=tenant_id)\n",["updates a qos to tenant association ."]]
["def getPointsFromLoop(loop, radius, thresholdRatio=0.9):\n\tradius = abs(radius)\n\tpoints = []\n\tfor pointIndex in xrange(len(loop)):\n\t\tpointBegin = loop[pointIndex]\n\t\tpointEnd = loop[((pointIndex + 1) % len(loop))]\n\t\tpoints.append(pointBegin)\n\t\taddPointsFromSegment(pointBegin, pointEnd, points, radius, thresholdRatio)\n\treturn points\n",["get the points from every point on a loop and between points ."]]
["def _processproperty(processname, property):\n\tfinder = _getfinder()\n\targs = {}\n\tattrs = {}\n\taeobj_00 = aetypes.ObjectSpecifier(want=aetypes.Type('prcs'), form='name', seld=processname, fr=None)\n\taeobj_01 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type(property), fr=aeobj_00)\n\targs['----'] = aeobj_01\n\t(_reply, args, attrs) = finder.send('core', 'getd', args, attrs)\n\tif ('errn' in args):\n\t\traise Error, aetools.decodeerror(args)\n\tif ('----' in args):\n\t\treturn args['----']\n",["return the partition size and memory used for processname ."]]
["@context.quietfunc\n@with_device\ndef exists(path):\n\twith AdbClient() as c:\n\t\treturn bool(c.stat(path))\n",["given a domain name ."]]
["def stub_out_http_backend(stubs):\n\tclass FakeHTTPConnection(object, ):\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tpass\n\t\tdef getresponse(self):\n\t\t\tif len(FAKE_RESPONSE_STACK):\n\t\t\t\treturn FAKE_RESPONSE_STACK.pop()\n\t\t\treturn utils.FakeHTTPResponse()\n\t\tdef request(self, *_args, **_kwargs):\n\t\t\tpass\n\t\tdef close(self):\n\t\t\tpass\n\tdef fake_get_conn_class(self, *args, **kwargs):\n\t\treturn FakeHTTPConnection\n\tstubs.Set(Store, '_get_conn_class', fake_get_conn_class)\n",["stubs out the httplib ."]]
["def _ar_transparams(params):\n\tnewparams = ((1 - np.exp((- params))) \/ (1 + np.exp((- params)))).copy()\n\ttmp = ((1 - np.exp((- params))) \/ (1 + np.exp((- params)))).copy()\n\tfor j in range(1, len(params)):\n\t\ta = newparams[j]\n\t\tfor kiter in range(j):\n\t\t\ttmp[kiter] -= (a * newparams[((j - kiter) - 1)])\n\t\tnewparams[:j] = tmp[:j]\n\treturn newparams\n",["transforms params to induce stationarity\/invertability ."]]
["def _syscmd_file(target, default=''):\n\tif (sys.platform in ('dos', 'win32', 'win16', 'os2')):\n\t\treturn default\n\ttarget = _follow_symlinks(target)\n\ttry:\n\t\tf = os.popen(('file\t\"%s\"\t2>\t\/dev\/null' % target))\n\texcept (AttributeError, os.error):\n\t\treturn default\n\toutput = string.strip(f.read())\n\trc = f.close()\n\tif ((not output) or rc):\n\t\treturn default\n\telse:\n\t\treturn output\n",["interface to the systems file command ."]]
["def _get_win_folder_from_registry(csidl_name):\n\timport _winreg\n\tshell_folder_name = {'CSIDL_APPDATA': 'AppData', 'CSIDL_COMMON_APPDATA': 'Common\tAppData', 'CSIDL_LOCAL_APPDATA': 'Local\tAppData'}[csidl_name]\n\tkey = _winreg.OpenKey(_winreg.HKEY_CURRENT_USER, 'Software\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Explorer\\\\Shell\tFolders')\n\t(directory, _type) = _winreg.QueryValueEx(key, shell_folder_name)\n\treturn directory\n",["this is a fallback technique at best ."]]
["def runSome():\n\ttests = []\n\tnames = ['testParseHostname', 'testExtractMastersSingle', 'testExtractMastersMultiple']\n\ttests.extend(list(list(map(BasicTestCase, names))))\n\tsuite = unittest.TestSuite(tests)\n\tunittest.TextTestRunner(verbosity=2).run(suite)\n",["unittest runner ."]]
["def test_retry_on_normal_error(collect):\n\tkey_name = 'test-key-name'\n\tb = B(name=key_name)\n\tcollect.inject(Exception('Normal\terror'))\n\td = wabs_deleter.Deleter(BlobService('test', 'ing'), 'test-container')\n\td.delete(b)\n\twhile (len(collect.aborted_keys) < 2):\n\t\tgevent.sleep(0.1)\n\tassert (not collect.deleted_keys)\n\tcollect.inject(None)\n\td.close()\n\tassert (collect.deleted_keys == [key_name])\n",["ensure retries are processed for most errors ."]]
["def mkdtemp(suffix='', prefix=template, dir=None):\n\tif (dir is None):\n\t\tdir = gettempdir()\n\tnames = _get_candidate_names()\n\tfor seq in xrange(TMP_MAX):\n\t\tname = names.next()\n\t\tfile = _os.path.join(dir, ((prefix + name) + suffix))\n\t\ttry:\n\t\t\t_os.mkdir(file, 448)\n\t\t\treturn file\n\t\texcept OSError as e:\n\t\t\tif (e.errno == _errno.EEXIST):\n\t\t\t\tcontinue\n\t\t\traise\n\traise IOError, (_errno.EEXIST, 'No\tusable\ttemporary\tdirectory\tname\tfound')\n",["user-callable function to create and return a unique temporary directory ."]]
["def keep_lazy(*resultclasses):\n\tif (not resultclasses):\n\t\traise TypeError('You\tmust\tpass\tat\tleast\tone\targument\tto\tkeep_lazy().')\n\tdef decorator(func):\n\t\tlazy_func = lazy(func, *resultclasses)\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tfor arg in (list(args) + list(kwargs.values())):\n\t\t\t\tif isinstance(arg, Promise):\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn lazy_func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n",["a decorator that allows a function to be called with one or more lazy arguments ."]]
["def rank(X, cond=1e-12):\n\tX = np.asarray(X)\n\tif (len(X.shape) == 2):\n\t\tD = scipy.linalg.svdvals(X)\n\t\treturn int(np.add.reduce(np.greater((D \/ D.max()), cond).astype(np.int32)))\n\telse:\n\t\treturn int((not np.alltrue(np.equal(X, 0.0))))\n",["return the rank of a matrix x based on its generalized inverse ."]]
["@register.tag\ndef ssi(parser, token):\n\tbits = token.split_contents()\n\tparsed = False\n\tif (len(bits) not in (2, 3)):\n\t\traise TemplateSyntaxError(u\"'ssi'\ttag\ttakes\tone\targument:\tthe\tpath\tto\tthe\tfile\tto\tbe\tincluded\")\n\tif (len(bits) == 3):\n\t\tif (bits[2] == u'parsed'):\n\t\t\tparsed = True\n\t\telse:\n\t\t\traise TemplateSyntaxError((u\"Second\t(optional)\targument\tto\t%s\ttag\tmust\tbe\t'parsed'\" % bits[0]))\n\tfilepath = parser.compile_filter(bits[1])\n\treturn SsiNode(filepath, parsed)\n",["outputs the contents of a given file into the page ."]]
["def test_allknn_fit():\n\tallknn = AllKNN(random_state=RND_SEED)\n\tallknn.fit(X, Y)\n\tassert_equal(allknn.min_c_, 0)\n\tassert_equal(allknn.maj_c_, 2)\n\tassert_equal(allknn.stats_c_[0], 4)\n\tassert_equal(allknn.stats_c_[1], 16)\n\tassert_equal(allknn.stats_c_[2], 20)\n",["test the fitting method ."]]
["def unwrap_order_by(clause):\n\tcols = util.column_set()\n\tresult = []\n\tstack = deque([clause])\n\twhile stack:\n\t\tt = stack.popleft()\n\t\tif (isinstance(t, ColumnElement) and ((not isinstance(t, UnaryExpression)) or (not operators.is_ordering_modifier(t.modifier)))):\n\t\t\tif isinstance(t, _label_reference):\n\t\t\t\tt = t.element\n\t\t\tif isinstance(t, _textual_label_reference):\n\t\t\t\tcontinue\n\t\t\tif (t not in cols):\n\t\t\t\tcols.add(t)\n\t\t\t\tresult.append(t)\n\t\telse:\n\t\t\tfor c in t.get_children():\n\t\t\t\tstack.append(c)\n\treturn result\n",["break up an order by expression into individual column-expressions ."]]
["def splithost(url):\n\tglobal _hostprog\n\tif (_hostprog is None):\n\t\timport re\n\t\t_hostprog = re.compile('^\/\/([^\/?]*)(.*)$')\n\tmatch = _hostprog.match(url)\n\tif match:\n\t\thost_port = match.group(1)\n\t\tpath = match.group(2)\n\t\tif (path and (not path.startswith('\/'))):\n\t\t\tpath = ('\/' + path)\n\t\treturn (host_port, path)\n\treturn (None, url)\n",["splithost --> host[:port] ."]]
["def _auth_return_future(f):\n\treplacer = ArgReplacer(f, 'callback')\n\t@functools.wraps(f)\n\tdef wrapper(*args, **kwargs):\n\t\tfuture = TracebackFuture()\n\t\t(callback, args, kwargs) = replacer.replace(future, args, kwargs)\n\t\tif (callback is not None):\n\t\t\tfuture.add_done_callback(functools.partial(_auth_future_to_callback, callback))\n\t\tf(*args, **kwargs)\n\t\treturn future\n\treturn wrapper\n",["similar to tornado ."]]
["@Profiler.profile\ndef test_core_reuse_stmt_compiled_cache(n):\n\tcompiled_cache = {}\n\tstmt = select([Customer.__table__]).where((Customer.id == bindparam('id')))\n\twith engine.connect().execution_options(compiled_cache=compiled_cache) as conn:\n\t\tfor id_ in random.sample(ids, n):\n\t\t\trow = conn.execute(stmt, id=id_).first()\n\t\t\ttuple(row)\n",["test core ."]]
["def loadValueFromFile(filename, variable, passphrase=None):\n\tif passphrase:\n\t\tmode = 'rb'\n\telse:\n\t\tmode = 'r'\n\tfileObj = open(filename, mode)\n\td = {'__file__': filename}\n\tif passphrase:\n\t\tdata = fileObj.read()\n\t\tdata = _decrypt(passphrase, data)\n\t\texec data in d, d\n\telse:\n\t\texec fileObj in d, d\n\tvalue = d[variable]\n\treturn value\n",["load the value of a variable in a python file ."]]
["def _parse_date_asctime(dt):\n\tparts = dt.split()\n\tif (len(parts) == 5):\n\t\tparts.insert(4, '+0000')\n\tif (len(parts) != 6):\n\t\treturn None\n\treturn _parse_date_rfc822('\t'.join([parts[0], parts[2], parts[1], parts[5], parts[3], parts[4]]))\n",["parse asctime-style dates ."]]
["def check_random_state(seed):\n\tif ((seed is None) or (seed is np.random)):\n\t\treturn np.random.mtrand._rand\n\tif isinstance(seed, (int, np.integer)):\n\t\treturn np.random.RandomState(seed)\n\tif isinstance(seed, np.random.RandomState):\n\t\treturn seed\n\traise ValueError(('%r\tcannot\tbe\tused\tto\tseed\ta\tnumpy.random.RandomState\tinstance' % seed))\n",["turn seed into a np ."]]
["def hashPasswordTuple(password, digestMod=hashlib.sha512, iterations=10000, saltSize=32):\n\tsalt = os.urandom(saltSize)\n\tpassword = password.encode(u'utf-8')\n\thash = pbkdf2(password, salt, iterations, digestMod)\n\tdigestname = digestMod.__name__.replace(u'openssl_', u'')\n\treturn (digestname, iterations, salt, hash)\n",["module function to hash a password according to the pbkdf2 specification ."]]
["def header_elements(fieldname, fieldvalue):\n\tif (not fieldvalue):\n\t\treturn []\n\tresult = []\n\tfor element in RE_HEADER_SPLIT.split(fieldvalue):\n\t\tif (fieldname.startswith('Accept') or (fieldname == 'TE')):\n\t\t\thv = AcceptElement.from_str(element)\n\t\telse:\n\t\t\thv = HeaderElement.from_str(element)\n\t\tresult.append(hv)\n\treturn list(reversed(sorted(result)))\n",["return a sorted headerelement list from a comma-separated header string ."]]
["def string_output(func, argtypes, offset=(-1), str_result=False):\n\tfunc.argtypes = argtypes\n\tif str_result:\n\t\tfunc.restype = gdal_char_p\n\telse:\n\t\tfunc.restype = c_int\n\tdef _check_str(result, func, cargs):\n\t\treturn check_string(result, func, cargs, offset=offset, str_result=str_result)\n\tfunc.errcheck = _check_str\n\treturn func\n",["generates a ctypes prototype for the given function with the given argument types that returns a string from a gdal pointer ."]]
["def parse_dict_header(value):\n\tresult = {}\n\tfor item in _parse_list_header(value):\n\t\tif ('=' not in item):\n\t\t\tresult[item] = None\n\t\t\tcontinue\n\t\t(name, value) = item.split('=', 1)\n\t\tif (value[:1] == value[(-1):] == '\"'):\n\t\t\tvalue = unquote_header_value(value[1:(-1)])\n\t\tresult[name] = value\n\treturn result\n",["parse lists of key ."]]
["def _has_init(directory):\n\tmod_or_pack = join(directory, '__init__')\n\tfor ext in (PY_SOURCE_EXTS + ('pyc', 'pyo')):\n\t\tif exists(((mod_or_pack + '.') + ext)):\n\t\t\treturn ((mod_or_pack + '.') + ext)\n\treturn None\n",["if the given directory has a valid __init__ file ."]]
["def _generate_cache_key(request, headerlist, key_prefix):\n\tctx = md5.new()\n\tfor header in headerlist:\n\t\tvalue = request.META.get(header, None)\n\t\tif (value is not None):\n\t\t\tctx.update(value)\n\treturn ('views.decorators.cache.cache_page.%s.%s.%s' % (key_prefix, request.path, ctx.hexdigest()))\n",["returns a cache key from the headers given in the header list ."]]
["def markdownFromFile(*args, **kwargs):\n\tpos = [u'input', u'output', u'extensions', u'encoding']\n\tc = 0\n\tfor arg in args:\n\t\tif (pos[c] not in kwargs):\n\t\t\tkwargs[pos[c]] = arg\n\t\tc += 1\n\t\tif (c == len(pos)):\n\t\t\tbreak\n\tmd = Markdown(**kwargs)\n\tmd.convertFile(kwargs.get(u'input', None), kwargs.get(u'output', None), kwargs.get(u'encoding', None))\n",["read markdown code from a file and write it to a file or a stream ."]]
["def find_free_port(interface='127.0.0.1', socket_family=socket.AF_INET, socket_type=socket.SOCK_STREAM):\n\taddress = socket.getaddrinfo(interface, 0)[0][4]\n\tprobe = socket.socket(socket_family, socket_type)\n\ttry:\n\t\tprobe.bind(address)\n\t\treturn probe.getsockname()\n\tfinally:\n\t\tprobe.close()\n",["ask the platform to allocate a free port on the specified interface ."]]
["@register.inclusion_tag(engine.get_template('inclusion.html'))\ndef inclusion_unlimited_args_from_template(one, two='hi', *args):\n\treturn {'result': ('inclusion_unlimited_args_from_template\t-\tExpected\tresult:\t%s' % ',\t'.join((str(arg) for arg in ([one, two] + list(args)))))}\n",["expected inclusion_unlimited_args_from_template __doc__ ."]]
["@register.tag(u'get_available_languages')\ndef do_get_available_languages(parser, token):\n\targs = token.contents.split()\n\tif ((len(args) != 3) or (args[1] != u'as')):\n\t\traise TemplateSyntaxError((u\"'get_available_languages'\trequires\t'as\tvariable'\t(got\t%r)\" % args))\n\treturn GetAvailableLanguagesNode(args[2])\n",["this will store a list of available languages in the context ."]]
["def libvlc_audio_set_format_callbacks(mp, setup, cleanup):\n\tf = (_Cfunctions.get('libvlc_audio_set_format_callbacks', None) or _Cfunction('libvlc_audio_set_format_callbacks', ((1,), (1,), (1,)), None, None, MediaPlayer, AudioSetupCb, AudioCleanupCb))\n\treturn f(mp, setup, cleanup)\n",["set decoded audio format ."]]
["def list_items(queue, backend='sqlite'):\n\tqueue_funcs = salt.loader.queues(__opts__)\n\tcmd = '{0}.list_items'.format(backend)\n\tif (cmd not in queue_funcs):\n\t\traise SaltInvocationError('Function\t\"{0}\"\tis\tnot\tavailable'.format(cmd))\n\tret = queue_funcs[cmd](queue=queue)\n\treturn ret\n",["print out items in lib matching query ."]]
["def image_property_delete(context, prop_ref, image_ref, session=None):\n\tsession = (session or get_session())\n\tprop = session.query(models.ImageProperty).filter_by(image_id=image_ref, name=prop_ref).one()\n\tprop.delete(session=session)\n\treturn prop\n",["used internally by image_property_create and image_property_update ."]]
["def get_delta(name):\n\t[curr_metrics, last_metrics] = get_metrics()\n\tmetric_name_list = name.split('_')[1:]\n\tmetric_name = '_'.join(metric_name_list)\n\ttry:\n\t\tdelta = ((float(curr_metrics['data'][metric_name]) - float(last_metrics['data'][metric_name])) \/ (curr_metrics['time'] - last_metrics['time']))\n\t\tif (delta < 0):\n\t\t\tif Debug:\n\t\t\t\tprint (name + '\tis\tless\t0.\tSetting\tvalue\tto\t0.')\n\t\t\tdelta = 0\n\texcept KeyError:\n\t\tif Debug:\n\t\t\tprint (('Key\t' + name) + \"\tcan't\tbe\tfound.\")\n\t\tdelta = 0.0\n\treturn delta\n",["return change over time for the requested metric ."]]
["def test_sample_wrong_X():\n\tada = ADASYN(random_state=RND_SEED)\n\tada.fit(X, Y)\n\tassert_raises(RuntimeError, ada.sample, np.random.random((100, 40)), np.array((([0] * 50) + ([1] * 50))))\n",["test either if an error is raised when x is different at fitting and sampling ."]]
["def urlsafe_decrypt(key, ciphertext):\n\tciphertext = base64.urlsafe_b64decode(str(ciphertext))\n\tcypher = AES.new(key, AES.MODE_CBC, ciphertext[:16])\n\tpadded = cypher.decrypt(ciphertext[16:])\n\treturn padded[:padded.rfind(chr(0))]\n",["decrypts url-safe base64 encoded ciphertext ."]]
["def new_figure_manager(num, *args, **kwargs):\n\t_focus = windowing.FocusManager()\n\tFigureClass = kwargs.pop('FigureClass', Figure)\n\tfigure = FigureClass(*args, **kwargs)\n\twindow = Tk.Tk()\n\tcanvas = FigureCanvasTkAgg(figure, master=window)\n\tfigManager = FigureManagerTkAgg(canvas, num, window)\n\tif matplotlib.is_interactive():\n\t\tfigManager.show()\n\treturn figManager\n",["create a new figure manager instance ."]]
["def convert_image(source, dest, out_format, run_as_root=False):\n\tcmd = ('qemu-img', 'convert', '-O', out_format, source, dest)\n\tutils.execute(run_as_root=run_as_root, *cmd)\n",["convert image to other format ."]]
["def compress_kml(kml):\n\tkmz = cStringIO.StringIO()\n\tzf = zipfile.ZipFile(kmz, 'a', zipfile.ZIP_DEFLATED)\n\tzf.writestr('doc.kml', kml.encode(settings.DEFAULT_CHARSET))\n\tzf.close()\n\tkmz.seek(0)\n\treturn kmz.read()\n",["returns compressed kmz from the given kml string ."]]
["def new_figure_manager_given_figure(num, figure):\n\tcanvas = FigureCanvasGDK(figure)\n\tmanager = FigureManagerBase(canvas, num)\n\treturn manager\n",["create a new figure manager instance for the given figure ."]]
["@pytest.yield_fixture(params=[None, tdata])\ndef temp_server(request):\n\tdata = request.param\n\ts = Server(copy(data), formats=all_formats)\n\ts.app.testing = True\n\twith s.app.test_client() as c:\n\t\t(yield c)\n",["for when we want to mutate the server ."]]
["def python_3000_has_key(logical_line, noqa):\n\tpos = logical_line.find('.has_key(')\n\tif ((pos > (-1)) and (not noqa)):\n\t\t(yield (pos, \"W601\t.has_key()\tis\tdeprecated,\tuse\t'in'\"))\n",["the {} ."]]
["def test_read_noformat_arbitrary():\n\t_identifiers.update(_IDENTIFIERS_ORIGINAL)\n\twith pytest.raises(io_registry.IORegistryError) as exc:\n\t\tTestData.read(object())\n\tassert str(exc.value).startswith(u'Format\tcould\tnot\tbe\tidentified.')\n",["test that all identifier functions can accept arbitrary input ."]]
["def new_figure_manager_given_figure(num, figure):\n\tcanvas = FigureCanvasGDK(figure)\n\tmanager = FigureManagerBase(canvas, num)\n\treturn manager\n",["create a new figure manager instance for the given figure ."]]
["def _unpack_ipv4(ip_str):\n\tif (not ip_str.lower().startswith('0000:0000:0000:0000:0000:ffff:')):\n\t\treturn None\n\treturn ip_str.rsplit(':', 1)[1]\n",["unpack an ipv4 address that was mapped in a compressed ipv6 address ."]]
["def urlizetrunc(value, limit):\n\tfrom django.utils.html import urlize\n\treturn urlize(value, trim_url_limit=int(limit), nofollow=True)\n",["converts urls into clickable links ."]]
["def header_decode(s):\n\ts = s.replace('_', '\t')\n\treturn re.sub('=\\\\w{2}', _unquote_match, s)\n",["decode a string encoded with rfc 2045 mime header q encoding ."]]
["def _auth(uri):\n\t(user, password) = _get_credentials()\n\tif ((user is False) or (password is False)):\n\t\treturn False\n\tbasic = _HTTPBasicAuthHandler()\n\tbasic.add_password(realm='Tomcat\tManager\tApplication', uri=uri, user=user, passwd=password)\n\tdigest = _HTTPDigestAuthHandler()\n\tdigest.add_password(realm='Tomcat\tManager\tApplication', uri=uri, user=user, passwd=password)\n\treturn _build_opener(basic, digest)\n",["return the auth object ."]]
["def check_const_string(result, func, cargs, offset=None, cpl=False):\n\tif offset:\n\t\tcheck_err(result, cpl=cpl)\n\t\tptr = ptr_byref(cargs, offset)\n\t\treturn ptr.value\n\telse:\n\t\treturn result\n",["similar functionality to check_string ."]]
["@register.simple_tag\ndef simple_only_unlimited_args(*args):\n\treturn ('simple_only_unlimited_args\t-\tExpected\tresult:\t%s' % ',\t'.join([unicode(arg) for arg in args]))\n",["expected simple_only_unlimited_args __doc__ ."]]
["def p_expression_name(p):\n\ttry:\n\t\tp[0] = names[p[1]]\n\texcept LookupError:\n\t\tprint (\"Undefined\tname\t'%s'\" % p[1])\n\t\tp[0] = 0\n",["expression : name ."]]
["def getMaximumSpan(loop):\n\textent = (getMaximumByPathComplex(loop) - getMinimumByPathComplex(loop))\n\treturn max(extent.real, extent.imag)\n",["get the maximum span of the loop ."]]
["def draw_if_interactive():\n\tif matplotlib.is_interactive():\n\t\tfigManager = Gcf.get_active()\n\t\tif (figManager is not None):\n\t\t\tfigManager.canvas.invalidate()\n",["is called after every pylab drawing command ."]]
["def store_rendered_templates(store, signal, sender, template, context, **kwargs):\n\tstore.setdefault('template', []).append(template)\n\tstore.setdefault('context', ContextList()).append(context)\n",["stores templates and contexts that are rendered ."]]
["def _reinstall_default_lookups():\n\t_install_lookups(dict(instance_state=_default_state_getter, instance_dict=_default_dict_getter, manager_of_class=_default_manager_getter))\n\t_instrumentation_factory._extended = False\n",["restore simplified lookups ."]]
["def _initialize_builtins():\n\tfor filename in os.listdir(_handler_dir):\n\t\tif os.path.isfile(_get_yaml_path(filename, '')):\n\t\t\t_available_builtins.append(filename)\n",["scan the immediate subdirectories of the builtins module ."]]
["def _remove_dead_thread_references():\n\tfor thread_reference in set(_thread_references):\n\t\tif (thread_reference() is None):\n\t\t\t_thread_references.discard(thread_reference)\n",["remove inactive threads from _thread_references ."]]
["@blueprint.route('\/large_graph', methods=['GET'])\ndef large_graph():\n\tjob = job_from_request()\n\treturn flask.render_template('models\/images\/classification\/large_graph.html', job=job)\n",["show the loss\/accuracy graph ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["napalm library must be installed for this module to work ."]]
["def article(word, function=INDEFINITE):\n\treturn (((function == DEFINITE) and definite_article(word)) or indefinite_article(word))\n",["returns the indefinite or definite article for the given word ."]]
["def outerjoin(left, right, onclause=None, join_to_left=None):\n\treturn _ORMJoin(left, right, onclause, True)\n",["produce a left outer join between left and right clauses ."]]
["def ImportStateName(state):\n\treturn {STATE_READ: 'READ', STATE_GETTING: 'SENDING', STATE_GOT: 'SENT', STATE_NOT_SENT: 'NOT_SENT'}[state]\n",["converts a numeric state identifier to a string ."]]
["def packRequest_window_change(geometry):\n\t(rows, cols, xpixel, ypixel) = geometry\n\treturn struct.pack('>4L', cols, rows, xpixel, ypixel)\n",["pack a window-change request so that it is suitable for sending ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load if the postgres module is present ."]]
["def getAlterationFileLineBlindly(fileName):\n\treturn ('(<alterationFile>)\t%s\t(<\/alterationFile>)' % fileName)\n",["get the alteration file line from the filename ."]]
["def quality_parsed(mime_type, parsed_ranges):\n\treturn fitness_and_quality_parsed(mime_type, parsed_ranges)[1]\n",["find the best match for a given mime-type against a list of media_ranges that have already been parsed by parse_media_range() ."]]
["def cmd_list_resources(args, opts):\n\tfor x in json_get(opts, '')['resources']:\n\t\tprint(x)\n",["list-resources - list available web service resources ."]]
["def target_update(target, deps, cmd):\n\tif target_outdated(target, deps):\n\t\tsystem(cmd)\n",["update a target with a given command given a list of dependencies ."]]
["def list_dbs(**client_args):\n\tclient = _client(**client_args)\n\treturn client.get_list_database()\n",["list all influxdb databases ."]]
["def remove_file_failed(failed_file):\n\ttry:\n\t\tek(os.remove, failed_file)\n\texcept Exception:\n\t\tpass\n",["remove file from filesystem ."]]
["def reload_localzone():\n\tglobal _cache_tz\n\t_cache_tz = pytz.timezone(get_localzone_name())\n",["reload the cached localzone ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load if the influxdb module is available ."]]
["def default_user_agent():\n\t_implementation = platform.python_implementation()\n\tif (_implementation == 'CPython'):\n\t\t_implementation_version = platform.python_version()\n\telif (_implementation == 'PyPy'):\n\t\t_implementation_version = ('%s.%s.%s' % (sys.pypy_version_info.major, sys.pypy_version_info.minor, sys.pypy_version_info.micro))\n\t\tif (sys.pypy_version_info.releaselevel != 'final'):\n\t\t\t_implementation_version = ''.join([_implementation_version, sys.pypy_version_info.releaselevel])\n\telif (_implementation == 'Jython'):\n\t\t_implementation_version = platform.python_version()\n\telif (_implementation == 'IronPython'):\n\t\t_implementation_version = platform.python_version()\n\telse:\n\t\t_implementation_version = 'Unknown'\n\ttry:\n\t\tp_system = platform.system()\n\t\tp_release = platform.release()\n\texcept IOError:\n\t\tp_system = 'Unknown'\n\t\tp_release = 'Unknown'\n\treturn '\t'.join([('python-requests\/%s' % __version__), ('%s\/%s' % (_implementation, _implementation_version)), ('%s\/%s' % (p_system, p_release))])\n",["return a string representing the default user agent ."]]
["def file_dict(*packages):\n\terrors = []\n\tret = {}\n\tpkgs = {}\n\tcmd = 'dpkg\t-l\t{0}'.format('\t'.join(packages))\n\tout = __salt__['cmd.run_all'](cmd, python_shell=False)\n\tif (out['retcode'] != 0):\n\t\tmsg = ('Error:\t\t' + out['stderr'])\n\t\tlog.error(msg)\n\t\treturn msg\n\tout = out['stdout']\n\tfor line in out.splitlines():\n\t\tif line.startswith('ii\t'):\n\t\t\tcomps = line.split()\n\t\t\tpkgs[comps[1]] = {'version': comps[2], 'description': '\t'.join(comps[3:])}\n\t\tif ('No\tpackages\tfound' in line):\n\t\t\terrors.append(line)\n\tfor pkg in pkgs:\n\t\tfiles = []\n\t\tcmd = 'dpkg\t-L\t{0}'.format(pkg)\n\t\tfor line in __salt__['cmd.run'](cmd, python_shell=False).splitlines():\n\t\t\tfiles.append(line)\n\t\tret[pkg] = files\n\treturn {'errors': errors, 'packages': ret}\n",["list the files that belong to a package ."]]
["def cr_context(method):\n\tmethod._api = 'cr_context'\n\treturn method\n",["decorate a traditional-style method that takes cr ."]]
["def get_scheme_names():\n\tschemes = _INSTALL_SCHEMES.keys()\n\tschemes.sort()\n\treturn tuple(schemes)\n",["return a tuple containing the schemes names ."]]
["def sector():\n\treturn s3_rest_controller('org', 'sector')\n",["restful crud controller ."]]
["def cooperate(iterator):\n\treturn _theCooperator.cooperate(iterator)\n",["start running the given iterator as a long-running cooperative task ."]]
["def count(session, query):\n\tcounts = query.selectable.with_only_columns([func.count()])\n\tnum_results = session.execute(counts.order_by(None)).scalar()\n\tif ((num_results is None) or (query._limit is not None)):\n\t\treturn query.order_by(None).count()\n\treturn num_results\n",["count -> int return the number of occurrences of substring sub in string s[start:end] ."]]
["def coiterate(iterator):\n\treturn _theCooperator.coiterate(iterator)\n",["cooperatively iterate over the given iterator ."]]
["def generate_timestamp():\n\treturn unicode_type(int(time.time()))\n",["get seconds since epoch ."]]
["def missing(name, limit=''):\n\tif (limit == 'upstart'):\n\t\treturn (not _service_is_upstart(name))\n\telif (limit == 'sysvinit'):\n\t\treturn (not _service_is_sysv(name))\n\telif (_service_is_upstart(name) or _service_is_sysv(name)):\n\t\treturn False\n\telse:\n\t\treturn True\n",["the inverse of service ."]]
["def init(mpstate):\n\treturn SerialModule(mpstate)\n",["set attributes into both configuration and knowledge base singletons based upon command line and configuration file options ."]]
["def multicall(conf, context, topic, msg, timeout, connection_pool):\n\tLOG.debug(_('Making\tsynchronous\tcall\ton\t%s\t...'), topic)\n\tmsg_id = uuid.uuid4().hex\n\tmsg.update({'_msg_id': msg_id})\n\tLOG.debug((_('MSG_ID\tis\t%s') % msg_id))\n\t_add_unique_id(msg)\n\tpack_context(msg, context)\n\twith _reply_proxy_create_sem:\n\t\tif (not connection_pool.reply_proxy):\n\t\t\tconnection_pool.reply_proxy = ReplyProxy(conf, connection_pool)\n\tmsg.update({'_reply_q': connection_pool.reply_proxy.get_reply_q()})\n\twait_msg = MulticallProxyWaiter(conf, msg_id, timeout, connection_pool)\n\twith ConnectionContext(conf, connection_pool) as conn:\n\t\tconn.topic_send(topic, rpc_common.serialize_msg(msg), timeout)\n\treturn wait_msg\n",["make a call that returns multiple times ."]]
["def addFacesByConvexBottomTopLoop(faces, indexedLoopBottom, indexedLoopTop):\n\tif ((len(indexedLoopBottom) == 0) or (len(indexedLoopTop) == 0)):\n\t\treturn\n\tfor indexedPointIndex in xrange(max(len(indexedLoopBottom), len(indexedLoopTop))):\n\t\tindexedConvex = []\n\t\tif (len(indexedLoopBottom) > 1):\n\t\t\tindexedConvex.append(indexedLoopBottom[indexedPointIndex])\n\t\t\tindexedConvex.append(indexedLoopBottom[((indexedPointIndex + 1) % len(indexedLoopBottom))])\n\t\telse:\n\t\t\tindexedConvex.append(indexedLoopBottom[0])\n\t\tif (len(indexedLoopTop) > 1):\n\t\t\tindexedConvex.append(indexedLoopTop[((indexedPointIndex + 1) % len(indexedLoopTop))])\n\t\t\tindexedConvex.append(indexedLoopTop[indexedPointIndex])\n\t\telse:\n\t\t\tindexedConvex.append(indexedLoopTop[0])\n\t\taddFacesByConvex(faces, indexedConvex)\n",["add faces from loops ."]]
["def parse_http_date(date):\n\tfor regex in (RFC1123_DATE, RFC850_DATE, ASCTIME_DATE):\n\t\tm = regex.match(date)\n\t\tif (m is not None):\n\t\t\tbreak\n\telse:\n\t\traise ValueError((u'%r\tis\tnot\tin\ta\tvalid\tHTTP\tdate\tformat' % date))\n\ttry:\n\t\tyear = int(m.group(u'year'))\n\t\tif (year < 100):\n\t\t\tif (year < 70):\n\t\t\t\tyear += 2000\n\t\t\telse:\n\t\t\t\tyear += 1900\n\t\tmonth = (MONTHS.index(m.group(u'mon').lower()) + 1)\n\t\tday = int(m.group(u'day'))\n\t\thour = int(m.group(u'hour'))\n\t\tmin = int(m.group(u'min'))\n\t\tsec = int(m.group(u'sec'))\n\t\tresult = datetime.datetime(year, month, day, hour, min, sec)\n\t\treturn calendar.timegm(result.utctimetuple())\n\texcept Exception:\n\t\tsix.reraise(ValueError, ValueError((u'%r\tis\tnot\ta\tvalid\tdate' % date)), sys.exc_info()[2])\n",["parses a date format as specified by http rfc2616 section 3 ."]]
["def update_md5(filenames):\n\timport re\n\tfrom md5 import md5\n\tfor name in filenames:\n\t\tbase = os.path.basename(name)\n\t\tf = open(name, 'rb')\n\t\tmd5_data[base] = md5(f.read()).hexdigest()\n\t\tf.close()\n\tdata = [('\t\t\t\t%r:\t%r,\\n' % it) for it in md5_data.items()]\n\tdata.sort()\n\trepl = ''.join(data)\n\timport inspect\n\tsrcfile = inspect.getsourcefile(sys.modules[__name__])\n\tf = open(srcfile, 'rb')\n\tsrc = f.read()\n\tf.close()\n\tmatch = re.search('\\nmd5_data\t=\t{\\n([^}]+)}', src)\n\tif (not match):\n\t\tprint >>sys.stderr, 'Internal\terror!'\n\t\tsys.exit(2)\n\tsrc = ((src[:match.start(1)] + repl) + src[match.end(1):])\n\tf = open(srcfile, 'w')\n\tf.write(src)\n\tf.close()\n",["update our built-in md5 registry ."]]
["def traceParseAction(f):\n\tf = _trim_arity(f)\n\tdef z(*paArgs):\n\t\tthisFunc = f.func_name\n\t\t(s, l, t) = paArgs[(-3):]\n\t\tif (len(paArgs) > 3):\n\t\t\tthisFunc = ((paArgs[0].__class__.__name__ + '.') + thisFunc)\n\t\tsys.stderr.write((\">>entering\t%s(line:\t'%s',\t%d,\t%s)\\n\" % (thisFunc, line(l, s), l, t)))\n\t\ttry:\n\t\t\tret = f(*paArgs)\n\t\texcept Exception as exc:\n\t\t\tsys.stderr.write(('<<leaving\t%s\t(exception:\t%s)\\n' % (thisFunc, exc)))\n\t\t\traise\n\t\tsys.stderr.write(('<<leaving\t%s\t(ret:\t%s)\\n' % (thisFunc, ret)))\n\t\treturn ret\n\ttry:\n\t\tz.__name__ = f.__name__\n\texcept AttributeError:\n\t\tpass\n\treturn z\n",["decorator for debugging parse actions ."]]
["def _dict_pprinter_factory(start, end, basetype=None):\n\tdef inner(obj, p, cycle):\n\t\ttyp = type(obj)\n\t\tif ((basetype is not None) and (typ is not basetype) and (typ.__repr__ != basetype.__repr__)):\n\t\t\treturn p.text(typ.__repr__(obj))\n\t\tif cycle:\n\t\t\treturn p.text('{...}')\n\t\tp.begin_group(1, start)\n\t\tkeys = obj.keys()\n\t\tif (not (p.max_seq_length and (len(obj) >= p.max_seq_length))):\n\t\t\ttry:\n\t\t\t\tkeys = sorted(keys)\n\t\t\texcept Exception:\n\t\t\t\tpass\n\t\tfor (idx, key) in p._enumerate(keys):\n\t\t\tif idx:\n\t\t\t\tp.text(',')\n\t\t\t\tp.breakable()\n\t\t\tp.pretty(key)\n\t\t\tp.text(':\t')\n\t\t\tp.pretty(obj[key])\n\t\tp.end_group(1, end)\n\treturn inner\n",["factory that returns a pprint function used by the default pprint of dicts and dict proxies ."]]
["def writeOutput(fileName=''):\n\tfileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName)\n\tif (fileName != ''):\n\t\tskeinforge_craft.writeChainTextWithNounMessage(fileName, 'unpause')\n",["carve a gnu triangulated surface file ."]]
["def _handle_ns(packageName, path_item):\n\timporter = get_importer(path_item)\n\tif (importer is None):\n\t\treturn None\n\tloader = importer.find_module(packageName)\n\tif (loader is None):\n\t\treturn None\n\tmodule = sys.modules.get(packageName)\n\tif (module is None):\n\t\tmodule = sys.modules[packageName] = types.ModuleType(packageName)\n\t\tmodule.__path__ = []\n\t\t_set_parent_ns(packageName)\n\telif (not hasattr(module, '__path__')):\n\t\traise TypeError('Not\ta\tpackage:', packageName)\n\thandler = _find_adapter(_namespace_handlers, importer)\n\tsubpath = handler(importer, path_item, packageName, module)\n\tif (subpath is not None):\n\t\tpath = module.__path__\n\t\tpath.append(subpath)\n\t\tloader.load_module(packageName)\n\t\tsys_path = [((p and _normalize_cached(p)) or p) for p in sys.path]\n\t\tdef sort_key(p):\n\t\t\tparts = p.split(os.sep)\n\t\t\tparts = parts[:(- (packageName.count('.') + 1))]\n\t\t\treturn sys_path.index(_normalize_cached(os.sep.join(parts)))\n\t\tpath.sort(key=sort_key)\n\t\tmodule.__path__[:] = [_normalize_cached(p) for p in path]\n\treturn subpath\n",["ensure that named package includes a subpath of path_item ."]]
["def submit_row(context):\n\topts = context['opts']\n\tchange = context['change']\n\tis_popup = context['is_popup']\n\tsave_as = context['save_as']\n\treturn {'onclick_attrib': ((opts.get_ordered_objects() and change and 'onclick=\"submitOrderForm();\"') or ''), 'show_delete_link': ((not is_popup) and context['has_delete_permission'] and (change or context['show_delete'])), 'show_save_as_new': ((not is_popup) and change and save_as), 'show_save_and_add_another': (context['has_add_permission'] and (not is_popup) and ((not save_as) or context['add'])), 'show_save_and_continue': ((not is_popup) and context['has_change_permission']), 'is_popup': is_popup, 'show_save': True}\n",["displays the row of buttons for delete and save ."]]
["def list_users(profile=None, api_key=None):\n\treturn salt.utils.pagerduty.list_items('users', 'id', __salt__['config.option'](profile), api_key, opts=__opts__)\n",["list all drac users cli example: ."]]
["def merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n\tif (session_setting is None):\n\t\treturn request_setting\n\tif (request_setting is None):\n\t\treturn session_setting\n\tif (not (isinstance(session_setting, Mapping) and isinstance(request_setting, Mapping))):\n\t\treturn request_setting\n\tmerged_setting = dict_class(to_key_val_list(session_setting))\n\tmerged_setting.update(to_key_val_list(request_setting))\n\tfor (k, v) in request_setting.items():\n\t\tif (v is None):\n\t\t\tdel merged_setting[k]\n\tmerged_setting = dict(((k, v) for (k, v) in merged_setting.items() if (v is not None)))\n\treturn merged_setting\n",["determines appropriate setting for a given request ."]]
["def get_nav_close(fund_type='all', sub_type='all'):\n\tct._write_head()\n\tnums = _get_fund_num((ct.SINA_NAV_COUNT_URL % (ct.P_TYPE['http'], ct.DOMAINS['vsf'], ct.NAV_CLOSE_KEY, ct.NAV_CLOSE_API, ct.NAV_CLOSE_T2[fund_type], ct.NAV_CLOSE_T3[sub_type])))\n\tfund_df = _parse_fund_data((ct.SINA_NAV_DATA_URL % (ct.P_TYPE['http'], ct.DOMAINS['vsf'], ct.NAV_OPEN_KEY, ct.NAV_CLOSE_API, nums, ct.NAV_CLOSE_T2[fund_type], ct.NAV_CLOSE_T3[sub_type])), 'close')\n\treturn fund_df\n",["parameters type:string 1 ."]]
["def verify_rsa_sha1(request, rsa_public_key):\n\tnorm_params = normalize_parameters(request.params)\n\turi = normalize_base_string_uri(request.uri)\n\tmessage = construct_base_string(request.http_method, uri, norm_params).encode(u'utf-8')\n\tsig = binascii.a2b_base64(request.signature.encode(u'utf-8'))\n\talg = _jwt_rs1_signing_algorithm()\n\tkey = _prepare_key_plus(alg, rsa_public_key)\n\treturn alg.verify(message, key, sig)\n",["verify a rsassa-pkcs #1 v1 ."]]
["def script_from_examples(s):\n\toutput = []\n\tfor piece in DocTestParser().parse(s):\n\t\tif isinstance(piece, Example):\n\t\t\toutput.append(piece.source[:(-1)])\n\t\t\twant = piece.want\n\t\t\tif want:\n\t\t\t\toutput.append('#\tExpected:')\n\t\t\t\toutput += [('##\t' + l) for l in want.split('\\n')[:(-1)]]\n\t\telse:\n\t\t\toutput += [_comment_line(l) for l in piece.split('\\n')[:(-1)]]\n\twhile (output and (output[(-1)] == '#')):\n\t\toutput.pop()\n\twhile (output and (output[0] == '#')):\n\t\toutput.pop(0)\n\treturn ('\\n'.join(output) + '\\n')\n",["extract script from text with examples ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load if boto libraries exist and if boto libraries are greater than a given version ."]]
["def append(name, value, convert=False, delimiter=DEFAULT_TARGET_DELIM):\n\tname = re.sub(delimiter, DEFAULT_TARGET_DELIM, name)\n\tret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n\tgrain = __salt__['grains.get'](name, None)\n\tif grain:\n\t\tif isinstance(grain, list):\n\t\t\tif (value in grain):\n\t\t\t\tret['comment'] = 'Value\t{1}\tis\talready\tin\tthe\tlist\tfor\tgrain\t{0}'.format(name, value)\n\t\t\t\treturn ret\n\t\t\tif __opts__['test']:\n\t\t\t\tret['result'] = None\n\t\t\t\tret['comment'] = 'Value\t{1}\tin\tgrain\t{0}\tis\tset\tto\tbe\tadded'.format(name, value)\n\t\t\t\tret['changes'] = {'added': value}\n\t\t\t\treturn ret\n\t\t\t__salt__['grains.append'](name, value)\n\t\t\tret['comment'] = 'Value\t{1}\twas\tadded\tto\tgrain\t{0}'.format(name, value)\n\t\t\tret['changes'] = {'added': value}\n\t\telif (convert is True):\n\t\t\tif __opts__['test']:\n\t\t\t\tret['result'] = None\n\t\t\t\tret['comment'] = 'Grain\t{0}\tis\tset\tto\tbe\tconverted\tto\tlist\tand\tvalue\t{1}\twill\tbe\tadded'.format(name, value)\n\t\t\t\tret['changes'] = {'added': value}\n\t\t\t\treturn ret\n\t\t\tgrain = [grain]\n\t\t\tgrain.append(value)\n\t\t\t__salt__['grains.setval'](name, grain)\n\t\t\tret['comment'] = 'Value\t{1}\twas\tadded\tto\tgrain\t{0}'.format(name, value)\n\t\t\tret['changes'] = {'added': value}\n\t\telse:\n\t\t\tret['result'] = False\n\t\t\tret['comment'] = 'Grain\t{0}\tis\tnot\ta\tvalid\tlist'.format(name)\n\telse:\n\t\tret['result'] = False\n\t\tret['comment'] = 'Grain\t{0}\tdoes\tnot\texist'.format(name)\n\treturn ret\n",["append a rule to the specified table\/chain ."]]
["def filter_type_between(left, right, supports_midi=False, is_drum_pad=False, supports_instrument=False):\n\tTypes = Live.Browser.FilterType\n\tif (right and (right.type in (DeviceType.instrument, DeviceType.midi_effect))):\n\t\treturn Types.midi_effect_hotswap\n\tif (left and (left.type in (DeviceType.instrument, DeviceType.audio_effect))):\n\t\treturn Types.audio_effect_hotswap\n\tif supports_midi:\n\t\tif supports_instrument:\n\t\t\treturn (Types.drum_pad_hotswap if is_drum_pad else Types.instrument_hotswap)\n\t\telse:\n\t\t\treturn Types.midi_effect_hotswap\n\treturn Types.audio_effect_hotswap\n",["given left and right are two consecutive devices in a valid device chain ."]]
["def get_scene_absolute_numbering_for_show(indexer_id, indexer):\n\tif (indexer_id is None):\n\t\treturn {}\n\tindexer_id = int(indexer_id)\n\tindexer = int(indexer)\n\tresult = {}\n\tfor dbData in [x[u'doc'] for x in sickrage.srCore.mainDB.db.get_many(u'scene_numbering', indexer_id, with_doc=True)]:\n\t\tabsolute_number = int((dbData[u'absolute_number'] or 0))\n\t\tscene_absolute_number = int((dbData[u'scene_absolute_number'] or 0))\n\t\tif ((int(dbData[u'indexer']) != indexer) or (scene_absolute_number == 0)):\n\t\t\tcontinue\n\t\tresult[absolute_number] = scene_absolute_number\n\treturn result\n",["returns a dict of : mappings for an entire show ."]]
["def _convert_to_idn(url):\n\tparts = list(urlparse.urlsplit(url))\n\ttry:\n\t\tparts[1].encode(u'ascii')\n\texcept UnicodeEncodeError:\n\t\thost = parts[1].rsplit(u':', 1)\n\t\tnewhost = []\n\t\tport = u''\n\t\tif (len(host) == 2):\n\t\t\tport = host.pop()\n\t\tfor h in host[0].split(u'.'):\n\t\t\tnewhost.append(h.encode(u'idna').decode(u'utf-8'))\n\t\tparts[1] = u'.'.join(newhost)\n\t\tif port:\n\t\t\tparts[1] += (u':' + port)\n\t\treturn urlparse.urlunsplit(parts)\n\telse:\n\t\treturn url\n",["convert a url to idn notation ."]]
["def serve(request, path, document_root=None, insecure=False, **kwargs):\n\tif ((not settings.DEBUG) and (not insecure)):\n\t\traise ImproperlyConfigured(\"The\tstaticfiles\tview\tcan\tonly\tbe\tused\tin\tdebug\tmode\tor\tif\tthe\tthe\t--insecure\toption\tof\t'runserver'\tis\tused\")\n\tnormalized_path = posixpath.normpath(urllib.unquote(path)).lstrip('\/')\n\tabsolute_path = finders.find(normalized_path)\n\tif (not absolute_path):\n\t\tif (path.endswith('\/') or (path == '')):\n\t\t\traise Http404('Directory\tindexes\tare\tnot\tallowed\there.')\n\t\traise Http404((\"'%s'\tcould\tnot\tbe\tfound\" % path))\n\t(document_root, path) = os.path.split(absolute_path)\n\treturn static.serve(request, path, document_root=document_root, **kwargs)\n",["serve static files below a given point in the directory structure or from locations inferred from the staticfiles finders ."]]
["def create_connection(address, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None):\n\t(host, port) = address\n\terr = None\n\tfor res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):\n\t\t(af, socktype, proto, canonname, sa) = res\n\t\tsock = None\n\t\ttry:\n\t\t\tsock = socket.socket(af, socktype, proto)\n\t\t\tsock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n\t\t\tif (timeout is not socket._GLOBAL_DEFAULT_TIMEOUT):\n\t\t\t\tsock.settimeout(timeout)\n\t\t\tif source_address:\n\t\t\t\tsock.bind(source_address)\n\t\t\tsock.connect(sa)\n\t\t\treturn sock\n\t\texcept socket.error as _:\n\t\t\terr = _\n\t\t\tif (sock is not None):\n\t\t\t\tsock.close()\n\tif (err is not None):\n\t\traise err\n\telse:\n\t\traise socket.error('getaddrinfo\treturns\tan\tempty\tlist')\n",["connect to *address* and return the socket object ."]]
["def runWithWarningsSuppressed(suppressedWarnings, f, *a, **kw):\n\tfor (args, kwargs) in suppressedWarnings:\n\t\twarnings.filterwarnings(*args, **kwargs)\n\taddedFilters = warnings.filters[:len(suppressedWarnings)]\n\ttry:\n\t\tresult = f(*a, **kw)\n\texcept:\n\t\texc_info = sys.exc_info()\n\t\t_resetWarningFilters(None, addedFilters)\n\t\traise exc_info[0], exc_info[1], exc_info[2]\n\telse:\n\t\tif isinstance(result, defer.Deferred):\n\t\t\tresult.addBoth(_resetWarningFilters, addedFilters)\n\t\telse:\n\t\t\t_resetWarningFilters(None, addedFilters)\n\t\treturn result\n",["run the function c{f} ."]]
["def framework_find(fn, executable_path=None, env=None):\n\ttry:\n\t\treturn dyld_find(fn, executable_path=executable_path, env=env)\n\texcept ValueError:\n\t\tpass\n\tfmwk_index = fn.rfind('.framework')\n\tif (fmwk_index == (-1)):\n\t\tfmwk_index = len(fn)\n\t\tfn += '.framework'\n\tfn = os.path.join(fn, os.path.basename(fn[:fmwk_index]))\n\treturn dyld_find(fn, executable_path=executable_path, env=env)\n",["find a framework using dyld semantics in a very loose manner ."]]
["def init(mpstate):\n\treturn SerialModule(mpstate)\n",["connect mappings to the database ."]]
["def asynchronous(function):\n\targ_names = inspect.getargspec(function).args\n\tMessageData = collections.namedtuple(function.__name__, arg_names[1:])\n\t@functools.wraps(function)\n\tdef call_or_send(processor, *args, **kwargs):\n\t\tif ((len(args) == 1) and (not kwargs) and isinstance(args[0], MessageData)):\n\t\t\ttry:\n\t\t\t\treturn function(processor, **args[0]._asdict())\n\t\t\texcept Exception as exc:\n\t\t\t\tLOG.exception('[%s]\tException\tin\t\"%s\":\t%s', processor.name, function.__name__, exc)\n\t\t\t\traise\n\t\telse:\n\t\t\tdata = inspect.getcallargs(function, processor, *args, **kwargs)\n\t\t\tdata.pop(arg_names[0])\n\t\t\treturn processor.queue.send(function.__name__, MessageData(**data))\n\tcall_or_send.MessageData = MessageData\n\treturn call_or_send\n",["wrap request handler methods with this if they are asynchronous ."]]
["def assert_fingerprint(cert, fingerprint):\n\tfingerprint = fingerprint.replace(':', '').lower()\n\tdigest_length = len(fingerprint)\n\thashfunc = HASHFUNC_MAP.get(digest_length)\n\tif (not hashfunc):\n\t\traise SSLError('Fingerprint\tof\tinvalid\tlength:\t{0}'.format(fingerprint))\n\tfingerprint_bytes = unhexlify(fingerprint.encode())\n\tcert_digest = hashfunc(cert).digest()\n\tif (cert_digest != fingerprint_bytes):\n\t\traise SSLError('Fingerprints\tdid\tnot\tmatch.\tExpected\t\"{0}\",\tgot\t\"{1}\".'.format(fingerprint, hexlify(cert_digest)))\n",["checks if given fingerprint matches the supplied certificate ."]]
["def get_disk_info(virt_type, instance, block_device_info=None, image_meta=None, rescue=False):\n\tdisk_bus = get_disk_bus_for_device_type(virt_type, image_meta, 'disk')\n\tcdrom_bus = get_disk_bus_for_device_type(virt_type, image_meta, 'cdrom')\n\tmapping = get_disk_mapping(virt_type, instance, disk_bus, cdrom_bus, block_device_info, image_meta, rescue)\n\treturn {'disk_bus': disk_bus, 'cdrom_bus': cdrom_bus, 'mapping': mapping}\n",["determine guest disk mapping info ."]]
["def extract_views_from_urlpatterns(urlpatterns, base=''):\n\tviews = []\n\tfor p in urlpatterns:\n\t\tif hasattr(p, '_get_callback'):\n\t\t\ttry:\n\t\t\t\tviews.append((p._get_callback(), (base + p.regex.pattern)))\n\t\t\texcept ViewDoesNotExist:\n\t\t\t\tcontinue\n\t\telif hasattr(p, '_get_url_patterns'):\n\t\t\ttry:\n\t\t\t\tpatterns = p.url_patterns\n\t\t\texcept ImportError:\n\t\t\t\tcontinue\n\t\t\tviews.extend(extract_views_from_urlpatterns(patterns, (base + p.regex.pattern)))\n\t\telse:\n\t\t\traise TypeError((_('%s\tdoes\tnot\tappear\tto\tbe\ta\turlpattern\tobject') % p))\n\treturn views\n",["return a list of views from a list of urlpatterns ."]]
["def normpath(path):\n\tif (path == ''):\n\t\treturn '.'\n\tinitial_slashes = path.startswith('\/')\n\tif (initial_slashes and path.startswith('\/\/') and (not path.startswith('\/\/\/'))):\n\t\tinitial_slashes = 2\n\tcomps = path.split('\/')\n\tnew_comps = []\n\tfor comp in comps:\n\t\tif (comp in ('', '.')):\n\t\t\tcontinue\n\t\tif ((comp != '..') or ((not initial_slashes) and (not new_comps)) or (new_comps and (new_comps[(-1)] == '..'))):\n\t\t\tnew_comps.append(comp)\n\t\telif new_comps:\n\t\t\tnew_comps.pop()\n\tcomps = new_comps\n\tpath = '\/'.join(comps)\n\tif initial_slashes:\n\t\tpath = (('\/' * initial_slashes) + path)\n\treturn (path or '.')\n",["normalize path ."]]
["def test(HandlerClass=BaseHTTPRequestHandler, ServerClass=HTTPServer, protocol='HTTP\/1.0', port=8000, bind=''):\n\tserver_address = (bind, port)\n\tHandlerClass.protocol_version = protocol\n\thttpd = ServerClass(server_address, HandlerClass)\n\tsa = httpd.socket.getsockname()\n\tprint ('Serving\tHTTP\ton', sa[0], 'port', sa[1], '...')\n\ttry:\n\t\thttpd.serve_forever()\n\texcept KeyboardInterrupt:\n\t\tprint '\\nKeyboard\tinterrupt\treceived,\texiting.'\n\t\thttpd.server_close()\n\t\tsys.exit(0)\n",["test the http request handler class ."]]
["def set_query_params(url, param_dict):\n\t(scheme, netloc, path, query_string, fragment) = urlsplit(url)\n\tquery_params = parse_qs(query_string, keep_blank_values=True)\n\tfor (param_name, param_value) in param_dict.items():\n\t\tif (param_value is None):\n\t\t\tdel query_params[param_name]\n\t\telse:\n\t\t\tquery_params[param_name] = [param_value]\n\tnew_query_string = urlencode(query_params, doseq=True)\n\treturn urlunsplit((scheme, netloc, path, new_query_string, fragment))\n",["given a url ."]]
["def get_browsers():\n\thtml = get(settings.BROWSERS_STATS_PAGE)\n\thtml = html.decode(u'windows-1252')\n\thtml = html.split(u'<table\tclass=\"w3-table-all\tnotranslate\">')[1]\n\thtml = html.split(u'<\/table>')[0]\n\tbrowsers = re.findall(u'\\\\.asp\">(.+?)<', html, re.UNICODE)\n\tbrowsers = [settings.OVERRIDES.get(browser, browser) for browser in browsers]\n\tbrowsers_statistics = re.findall(u'td\\\\sclass=\"right\">(.+?)\\\\s', html, re.UNICODE)\n\treturn list(zip(browsers, browsers_statistics))\n",["very very hardcoded\/dirty re\/split stuff ."]]
["def constant_time_compare(val1, val2):\n\tif (len(val1) != len(val2)):\n\t\treturn False\n\tresult = 0\n\tif (six.PY3 and isinstance(val1, bytes) and isinstance(val2, bytes)):\n\t\tfor (x, y) in zip(val1, val2):\n\t\t\tresult |= (x ^ y)\n\telse:\n\t\tfor (x, y) in zip(val1, val2):\n\t\t\tresult |= (ord(x) ^ ord(y))\n\treturn (result == 0)\n",["returns true if the two strings are equal ."]]
["def writeOutput(fileName=''):\n\tfileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName)\n\tif (fileName != ''):\n\t\tskeinforge_craft.writeChainTextWithNounMessage(fileName, 'unpause')\n",["write a skeinisoed gcode file for a skeinforge gcode file ."]]
["def get_fun(fun):\n\tserv = _get_serv(ret=None)\n\tret = {}\n\tfor minion in serv.smembers('minions'):\n\t\tind_str = '{0}:{1}'.format(minion, fun)\n\t\ttry:\n\t\t\tjid = serv.get(ind_str)\n\t\texcept Exception:\n\t\t\tcontinue\n\t\tif (not jid):\n\t\t\tcontinue\n\t\tdata = serv.get('{0}:{1}'.format(minion, jid))\n\t\tif data:\n\t\t\tret[minion] = json.loads(data)\n\treturn ret\n",["return a dict of the last function called for all minions ."]]
["def get_cli_body_ssh(command, response, module):\n\tif ('^' == response[0]):\n\t\tbody = []\n\telif ('running' in command):\n\t\tbody = response\n\telse:\n\t\tif (command in response[0]):\n\t\t\tresponse = [response[0].split(command)[1]]\n\t\ttry:\n\t\t\tbody = [json.loads(response[0])]\n\t\texcept ValueError:\n\t\t\tmodule.fail_json(msg='Command\tdoes\tnot\tsupport\tJSON\toutput', command=command)\n\treturn body\n",["get response for when transport=cli ."]]
["def is_image_visible(context, image, status=None):\n\tif context.is_admin:\n\t\treturn True\n\tif (image['owner'] is None):\n\t\treturn True\n\tif (image['visibility'] in ['public', 'community']):\n\t\treturn True\n\tif (context.owner is not None):\n\t\tif (context.owner == image['owner']):\n\t\t\treturn True\n\t\tif ('shared' == image['visibility']):\n\t\t\tmembers = image_member_find(context, image_id=image['id'], member=context.owner, status=status)\n\t\t\tif members:\n\t\t\t\treturn True\n\treturn False\n",["return true if the image is visible in this context ."]]
["@then('the\tcommand\toutput\tshould\tcontain\tthe\tfollowing\tlog\trecords')\ndef step_command_output_should_contain_log_records(context):\n\tassert context.table, 'REQUIRE:\tcontext.table'\n\tcontext.table.require_columns(['category', 'level', 'message'])\n\tformat = getattr(context, 'log_record_format', context.config.logging_format)\n\tfor row in context.table.rows:\n\t\toutput = LogRecordTable.make_output_for_row(row, format)\n\t\tcontext.execute_steps(u'\\n\t\t\t\t\t\t\t\t\t\t\t\tThen\tthe\tcommand\toutput\tshould\tcontain:\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"\"\"\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{expected_output}\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"\"\"\\n\t\t\t\t\t\t\t\t\t\t\t\t'.format(expected_output=output))\n",["verifies that the command output contains the specified log records ."]]
["def _api_change_script(name, output, kwargs):\n\tvalue = kwargs.get('value')\n\tvalue2 = kwargs.get('value2')\n\tif (value and value2):\n\t\tnzo_id = value\n\t\tscript = value2\n\t\tif (script.lower() == 'none'):\n\t\t\tscript = None\n\t\tresult = NzbQueue.do.change_script(nzo_id, script)\n\t\treturn report(output, keyword='status', data=bool((result > 0)))\n\telse:\n\t\treturn report(output, _MSG_NO_VALUE)\n",["api: accepts output ."]]
["@register.tag('extends')\ndef do_extends(parser, token):\n\tbits = token.split_contents()\n\tif (len(bits) != 2):\n\t\traise TemplateSyntaxError((\"'%s'\ttakes\tone\targument\" % bits[0]))\n\tparent_name = parser.compile_filter(bits[1])\n\tnodelist = parser.parse()\n\tif nodelist.get_nodes_by_type(ExtendsNode):\n\t\traise TemplateSyntaxError((\"'%s'\tcannot\tappear\tmore\tthan\tonce\tin\tthe\tsame\ttemplate\" % bits[0]))\n\treturn ExtendsNode(nodelist, parent_name)\n",["signal that this template extends a parent template ."]]
["def getTransformElementNode(coords, transformName):\n\ttransformElementNode = coords.getFirstChildByLocalName(transformName)\n\tif (len(transformElementNode.attributes) < 16):\n\t\tif ('bf:ref' in transformElementNode.attributes):\n\t\t\tidReference = transformElementNode.attributes['bf:ref']\n\t\t\treturn coords.getDocumentElement().getSubChildWithID(idReference)\n\treturn transformElementNode\n",["get the transform attributes ."]]
["def delete_org(orgid, profile='grafana'):\n\tif isinstance(profile, string_types):\n\t\tprofile = __salt__['config.option'](profile)\n\tresponse = requests.delete('{0}\/api\/orgs\/{1}'.format(profile['grafana_url'], orgid), auth=_get_auth(profile), headers=_get_headers(profile), timeout=profile.get('grafana_timeout', 3))\n\tif (response.status_code >= 400):\n\t\tresponse.raise_for_status()\n\treturn response.json()\n",["delete an organization ."]]
["def url_path_join(*pieces):\n\tinitial = pieces[0].startswith('\/')\n\tfinal = pieces[(-1)].endswith('\/')\n\tstripped = [s.strip('\/') for s in pieces]\n\tresult = '\/'.join((s for s in stripped if s))\n\tif initial:\n\t\tresult = ('\/' + result)\n\tif final:\n\t\tresult = (result + '\/')\n\tif (result == '\/\/'):\n\t\tresult = '\/'\n\treturn result\n",["join components of url into a relative url use to prevent double slash when joining subpath ."]]
["def to_locale(language):\n\tp = language.find('-')\n\tif (p >= 0):\n\t\tif (len(language[(p + 1):]) > 2):\n\t\t\treturn (((language[:p].lower() + '_') + language[(p + 1)].upper()) + language[(p + 2):].lower())\n\t\treturn ((language[:p].lower() + '_') + language[(p + 1):].upper())\n\telse:\n\t\treturn language.lower()\n",["turns a language name into a locale name ."]]
["def splitnport(host, defport=(-1)):\n\tglobal _nportprog\n\tif (_nportprog is None):\n\t\t_nportprog = re.compile('^(.*):(.*)$')\n\tmatch = _nportprog.match(host)\n\tif match:\n\t\t(host, port) = match.group(1, 2)\n\t\tif port:\n\t\t\ttry:\n\t\t\t\tnport = int(port)\n\t\t\texcept ValueError:\n\t\t\t\tnport = None\n\t\t\treturn (host, nport)\n\treturn (host, defport)\n",["split host and port ."]]
["def hardlinkFile(srcFile, destFile):\n\ttry:\n\t\tlink(srcFile, destFile)\n\t\tfixSetGroupID(destFile)\n\texcept Exception as e:\n\t\tsickrage.srCore.srLogger.warning((u'Failed\tto\tcreate\thardlink\tof\t%s\tat\t%s.\tError:\t%r.\tCopying\tinstead' % (srcFile, destFile, e)))\n\t\tcopyFile(srcFile, destFile)\n",["create a hard-link between source and destination ."]]
["def test_nearmiss_init():\n\tratio = 1.0\n\tnm3 = NearMiss(ratio=ratio, random_state=RND_SEED, version=VERSION_NEARMISS)\n\tassert_equal(nm3.version, VERSION_NEARMISS)\n\tassert_equal(nm3.n_neighbors, 3)\n\tassert_equal(nm3.ratio, ratio)\n\tassert_equal(nm3.random_state, RND_SEED)\n",["test the initialisation of the object ."]]
["def dbcheck(exprstr, globals=None, locals=None):\n\tdef decorate(func):\n\t\texpr = compile(exprstr, ('dbcheck-%s' % func.__name__), 'eval')\n\t\tdef check(*args, **kwds):\n\t\t\tif (not eval(expr, globals, locals)):\n\t\t\t\traise DbcheckError(exprstr, func, args, kwds)\n\t\t\treturn func(*args, **kwds)\n\t\treturn check\n\treturn decorate\n",["decorator to implement debugging assertions ."]]
["@with_setup(prepare_stdout)\ndef test_output_when_could_not_find_features_colorless():\n\tpath = fs.relpath(join(abspath(dirname(__file__)), 'no_features', 'unexistent-folder'))\n\trunner = Runner(path, verbosity=3, no_color=True)\n\trunner.run()\n\tassert_stdout_lines(('Oops!\\ncould\tnot\tfind\tfeatures\tat\t.\/%s\\n' % path))\n",["testing the colorful output of many successful features colorless ."]]
["def flatten_blocks(lines, num_indents=(-1)):\n\tINDENTATION = ('\t' * 4)\n\tif (not lines):\n\t\treturn ''\n\tif isinstance(lines, six.string_types):\n\t\treturn ((INDENTATION * num_indents) + lines)\n\treturn '\\n'.join([flatten_blocks(line, (num_indents + 1)) for line in lines])\n",["takes a list or string and flattens it into a string with indentation ."]]
["def setup_test_environment():\n\tTemplate.original_render = Template.render\n\tTemplate.render = instrumented_test_render\n",["perform any global pre-test setup ."]]
["def trailing_whitespace(physical_line):\n\tphysical_line = physical_line.rstrip('\\n')\n\tphysical_line = physical_line.rstrip('\\r')\n\tphysical_line = physical_line.rstrip('\\x0c')\n\tstripped = physical_line.rstrip('\t DCTB \\x0b')\n\tif (physical_line != stripped):\n\t\tif stripped:\n\t\t\treturn (len(stripped), 'W291\ttrailing\twhitespace')\n\t\telse:\n\t\t\treturn (0, 'W293\tblank\tline\tcontains\twhitespace')\n",["jcr: trailing whitespace is superfluous ."]]
["def b64decode(s, altchars=None):\n\tif (altchars is not None):\n\t\ts = _translate(s, {altchars[0]: '+', altchars[1]: '\/'})\n\ttry:\n\t\treturn binascii.a2b_base64(s)\n\texcept binascii.Error as msg:\n\t\traise TypeError(msg)\n",["decode a base64 encoded string ."]]
["def register(func, msg_type=None, dispatcher_name=None, active_by_default=True):\n\tif (msg_type and (msg_type not in MSG_TYPE_MAP)):\n\t\traise InvalidHandlerType(('Invalid\tmessage\ttype\t[%s]:\ttype\tshould\tbe\tin\t%s' % (msg_type, str(MSG_TYPES))))\n\thandler = Handler(func=func, name=dispatcher_name)\n\tif (msg_type is None):\n\t\t_registry[RAW_TYPE].append(handler)\n\telse:\n\t\t_registry[msg_type].append(handler)\n\tif active_by_default:\n\t\t_active.add(dispatcher_name)\n",["register language support with the manager ."]]
["def libvlc_media_player_new(p_libvlc_instance):\n\tf = (_Cfunctions.get('libvlc_media_player_new', None) or _Cfunction('libvlc_media_player_new', ((1,),), class_result(MediaPlayer), ctypes.c_void_p, Instance))\n\treturn f(p_libvlc_instance)\n",["create an empty media player object ."]]
["def _get_hash(filename, block_size=(2 ** 20), hash=hashlib.md5):\n\tf = open(filename, 'rb')\n\thash_ = hash()\n\twhile True:\n\t\tdata = f.read(block_size)\n\t\tif (not data):\n\t\t\tbreak\n\t\thash_.update(data)\n\treturn hash_.hexdigest()\n",["returns an md5 hash for a filename ."]]
["def truncate_name(name, length=None, hash_len=4):\n\tif ((length is None) or (len(name) <= length)):\n\t\treturn name\n\thash = md5_constructor(name).hexdigest()[:hash_len]\n\treturn ('%s%s' % (name[:(length - hash_len)], hash))\n",["shortens a string to a repeatable mangled version with the given length ."]]
["def mkstemp(suffix=None, prefix=None, dir=None, text=False):\n\t(prefix, suffix, dir, output_type) = _sanitize_params(prefix, suffix, dir)\n\tif text:\n\t\tflags = _text_openflags\n\telse:\n\t\tflags = _bin_openflags\n\treturn _mkstemp_inner(dir, prefix, suffix, flags, output_type)\n",["user-callable function to create and return a unique temporary file ."]]
["def libvlc_video_get_spu_description(p_mi):\n\tf = (_Cfunctions.get('libvlc_video_get_spu_description', None) or _Cfunction('libvlc_video_get_spu_description', ((1,),), None, ctypes.POINTER(TrackDescription), MediaPlayer))\n\treturn f(p_mi)\n",["get the description of available video subtitles ."]]
["def new_figure_manager(num, *args, **kwargs):\n\t_focus = windowing.FocusManager()\n\tFigureClass = kwargs.pop('FigureClass', Figure)\n\tfigure = FigureClass(*args, **kwargs)\n\twindow = Tk.Tk()\n\tcanvas = FigureCanvasTkAgg(figure, master=window)\n\tfigManager = FigureManagerTkAgg(canvas, num, window)\n\tif matplotlib.is_interactive():\n\t\tfigManager.show()\n\treturn figManager\n",["create a new figure manager instance ."]]
["def buildTagMap(default, *args):\n\tbuilt = {}\n\tfor portion in args:\n\t\tif hasattr(portion, 'items'):\n\t\t\tfor (k, v) in portion.items():\n\t\t\t\tbuilt[k] = v\n\t\telif (isList(portion) and (not isString(portion))):\n\t\t\tfor k in portion:\n\t\t\t\tbuilt[k] = default\n\t\telse:\n\t\t\tbuilt[portion] = default\n\treturn built\n",["turns a list of maps ."]]
["def notify(conf, context, topic, msg, envelope):\n\ttopic = topic.replace('.', '-')\n\tcast(conf, context, topic, msg, envelope=envelope)\n",["notifies the recipient of the desired event given the model ."]]
["def test_saving_state_exclude_domain_include_entity(hass_recorder):\n\thass = hass_recorder({'include': {'entities': 'test.recorder'}, 'exclude': {'domains': 'test'}})\n\tstates = _add_entities(hass, ['test.recorder', 'test2.recorder'])\n\tassert (len(states) == 2)\n",["test saving and restoring a state ."]]
["def test_cnn_fit_single_class():\n\tcnn = CondensedNearestNeighbour(random_state=RND_SEED)\n\ty_single_class = np.zeros((X.shape[0],))\n\tassert_warns(UserWarning, cnn.fit, X, y_single_class)\n",["test either if an error when there is a single class ."]]
["def register(func, msg_type=None, dispatcher_name=None, active_by_default=True):\n\tif (msg_type and (msg_type not in MSG_TYPE_MAP)):\n\t\traise InvalidHandlerType(('Invalid\tmessage\ttype\t[%s]:\ttype\tshould\tbe\tin\t%s' % (msg_type, str(MSG_TYPES))))\n\thandler = Handler(func=func, name=dispatcher_name)\n\tif (msg_type is None):\n\t\t_registry[RAW_TYPE].append(handler)\n\telse:\n\t\t_registry[msg_type].append(handler)\n\tif active_by_default:\n\t\t_active.add(dispatcher_name)\n",["register language support with the manager ."]]
["def enforce(context, action, target):\n\tinit()\n\tmatch_list = (('rule:%s' % action),)\n\tcredentials = context.to_dict()\n\tpolicy.enforce(match_list, target, credentials, exception.PolicyNotAuthorized, action=action)\n",["verifies that the action is valid on the target in this context ."]]
["@FileSystem.in_directory(current_directory, 'django', 'brocolis')\ndef test_harvest_with_debug_mode_disabled():\n\t(status, out) = run_scenario('leaves', 'disabled')\n\tassert_equals(status, 0, out)\n",["python manage ."]]
["def test_continuous_error():\n\ty = np.linspace(0, 1, 20)\n\tcnn = CondensedNearestNeighbour(random_state=RND_SEED)\n\tassert_warns(UserWarning, cnn.fit, X, y)\n",["test either if an error is raised when the target are continuous type ."]]
["@register.simple_tag\ndef get_advertisement_text_mail():\n\tadvertisement = Advertisement.objects.get_advertisement(Advertisement.PLACEMENT_MAIL_TEXT)\n\tif (advertisement is None):\n\t\treturn u''\n\treturn advertisement.text\n",["returns advertisement text ."]]
["def get_auth_from_url(url):\n\tparsed = urlparse(url)\n\ttry:\n\t\tauth = (unquote(parsed.username), unquote(parsed.password))\n\texcept (AttributeError, TypeError):\n\t\tauth = ('', '')\n\treturn auth\n",["given a url with authentication components ."]]
["def quota_get(context, project_id, resource, user_id=None):\n\treturn IMPL.quota_get(context, project_id, resource, user_id=user_id)\n",["retrieve a quota or raise if it does not exist ."]]
["def quota_class_get_all_by_name(context, class_name):\n\treturn IMPL.quota_class_get_all_by_name(context, class_name)\n",["retrieve all quotas associated with a given quota class ."]]
["def import_stages():\n\tstages = []\n\tfor plugin in find_plugins():\n\t\tif hasattr(plugin, 'import_stages'):\n\t\t\tstages += plugin.import_stages\n\treturn stages\n",["get a list of import stage functions defined by plugins ."]]
["def recipr0(X):\n\ttest = np.equal(np.asarray(X), 0)\n\treturn np.where(test, 0, (1.0 \/ X))\n",["return the reciprocal of an array ."]]
["def warning(msg, *args, **kwargs):\n\tif (len(root.handlers) == 0):\n\t\tbasicConfig()\n\troot.warning(msg, *args, **kwargs)\n",["log a message with severity warning on the root logger ."]]
["def heapify(x):\n\tn = len(x)\n\tfor i in reversed(xrange((n \/\/ 2))):\n\t\t_siftup(x, i)\n",["transform list into a heap ."]]
["def processXMLElement(xmlElement):\n\tgroup.processShape(Difference, xmlElement)\n",["process the xml element ."]]
["def get_command_from_state(state):\n\tcommand = None\n\tif (state == 'present'):\n\t\tcommand = 'vrouter-loopback-interface-add'\n\tif (state == 'absent'):\n\t\tcommand = 'vrouter-loopback-interface-remove'\n\treturn command\n",["this method gets appropriate command name for the state specified ."]]
["@loader_option()\ndef undefer(loadopt, key):\n\treturn loadopt.set_column_strategy((key,), {'deferred': False, 'instrument': True})\n",["indicate that the given column-oriented attribute should be undeferred ."]]
["def get_day_names(width='wide', context='format', locale=LC_TIME):\n\treturn Locale.parse(locale).days[context][width]\n",["return the day names used by the locale for the specified format ."]]
["def Pool(processes=None, initializer=None, initargs=(), maxtasksperchild=None):\n\tfrom multiprocessing.pool import Pool\n\treturn Pool(processes, initializer, initargs, maxtasksperchild)\n",["returns a process pool object ."]]
["@then(u'we\tsee\tdatabase\tconnected')\ndef step_see_db_connected(context):\n\t_expect_exact(context, u'You\tare\tnow\tconnected\tto\tdatabase', timeout=2)\n",["wait to see drop database output ."]]
["def listen(opts):\n\tevent = salt.utils.event.get_event(opts['node'], sock_dir=opts['sock_dir'], transport=opts['transport'], opts=opts, listen=True)\n\tcheck_access_and_print_warning(opts['sock_dir'])\n\tprint(event.puburi)\n\tjid_counter = 0\n\tfound_minions = []\n\twhile True:\n\t\tret = event.get_event(full=True)\n\t\tif (ret is None):\n\t\t\tcontinue\n\t\tif opts['func_count']:\n\t\t\tdata = ret.get('data', False)\n\t\t\tif data:\n\t\t\t\tif (('id' in six.iterkeys(data)) and (data.get('id', False) not in found_minions)):\n\t\t\t\t\tif (data['fun'] == opts['func_count']):\n\t\t\t\t\t\tjid_counter += 1\n\t\t\t\t\t\tfound_minions.append(data['id'])\n\t\t\t\t\t\tprint('Reply\treceived\tfrom\t[{0}].\tTotal\treplies\tnow:\t[{1}].'.format(ret['data']['id'], jid_counter))\n\t\t\t\t\tcontinue\n\t\telse:\n\t\t\tprint('Event\tfired\tat\t{0}'.format(time.asctime()))\n\t\t\tprint(('*' * 25))\n\t\t\tprint('Tag:\t{0}'.format(ret['tag']))\n\t\t\tprint('Data:')\n\t\t\tpprint.pprint(ret['data'])\n",["register a listener function for the given target ."]]
["def authenticate(request):\n\toauthlib_core = get_oauthlib_core()\n\t(valid, r) = oauthlib_core.verify_request(request, scopes=[])\n\tif valid:\n\t\treturn (r.user, r.access_token)\n\telse:\n\t\treturn (None, None)\n",["validate a username\/password combo ."]]
["def catalog():\n\tglobal _default\n\tt = getattr(_active, u'value', None)\n\tif (t is not None):\n\t\treturn t\n\tif (_default is None):\n\t\tfrom django.conf import settings\n\t\t_default = translation(settings.LANGUAGE_CODE)\n\treturn _default\n",["restful crud controller ."]]
["def _string_concat(*strings):\n\treturn u''.join([force_unicode(s) for s in strings])\n",["lazy variant of string concatenation ."]]
["def fnmatchcase(name, pat):\n\tmatch = _compile_pattern(pat)\n\treturn (match(name) is not None)\n",["test whether filename matches pattern ."]]
["def add_never_cache_headers(response):\n\tpatch_response_headers(response, cache_timeout=(-1))\n",["adds headers to a response to indicate that a page should never be cached ."]]
["def _delete_ntp_servers(servers):\n\treturn __salt__['ntp.delete_servers'](commit=False, *servers)\n",["calls ntp ."]]
["def expose_api_raw(func):\n\treturn expose_api(func, to_json=False)\n",["expose this function via the api but dont dump the results to json ."]]
["def format_number(number, locale=LC_NUMERIC):\n\treturn format_decimal(number, locale=locale)\n",["see :meth:i18n ."]]
["def divisibleby(value, arg):\n\treturn ((int(value) % int(arg)) == 0)\n",["returns true if the value is devisible by the argument ."]]
["def getNewDerivation(elementNode):\n\treturn evaluate.EmptyObject(elementNode)\n",["get new derivation ."]]
["def new(rsa_key):\n\treturn PKCS115_SigScheme(rsa_key)\n",["create a new blowfish cipher ."]]
["@register.filter('escapejs')\n@stringfilter\ndef escapejs_filter(value):\n\treturn escapejs(value)\n",["hex encodes characters for use in javascript strings ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["check to make sure requests and xml are installed and requests ."]]
["def R(seqn):\n\tfor i in seqn:\n\t\t(yield i)\n",["regular generator ."]]
["@with_device\ndef interactive(**kw):\n\treturn shell(**kw).interactive()\n",["set interactive mode to boolean b ."]]
["def list_nodes_full(call=None):\n\tif (call == 'action'):\n\t\traise SaltCloudSystemExit('The\tlist_nodes_full\tfunction\tmust\tbe\tcalled\twith\t-f\tor\t--function.')\n\tret = {}\n\titems = query(action='ve')\n\tfor item in items:\n\t\tname = item.attrib['name']\n\t\tnode = show_instance(name, call='action')\n\t\tret[name] = node\n\t\tret[name]['image'] = node['platform']['template-info']['name']\n\t\tif ('private-ip' in node['network']):\n\t\t\tret[name]['private_ips'] = [node['network']['private-ip']['address']]\n\t\tif ('public-ip' in node['network']):\n\t\t\tret[name]['public_ips'] = [node['network']['public-ip']['address']]\n\treturn ret\n",["because this module is not specific to any cloud providers ."]]
["@pytest.mark.parametrize('api_version', API_VERSIONS)\ndef test_thread_label_updates(db, api_client, default_account, api_version, custom_label):\n\theaders = dict()\n\theaders['Api-Version'] = api_version\n\tgmail_thread = add_fake_thread(db.session, default_account.namespace.id)\n\tgmail_message = add_fake_message(db.session, default_account.namespace.id, gmail_thread)\n\tresp_data = api_client.get_data('\/threads\/{}'.format(gmail_thread.public_id), headers=headers)\n\tassert (resp_data['labels'] == [])\n\tcategory = custom_label.category\n\tupdate = dict(labels=[category.public_id])\n\tresp = api_client.put_data('\/threads\/{}'.format(gmail_thread.public_id), update, headers=headers)\n\tresp_data = json.loads(resp.data)\n\tif (api_version == API_VERSIONS[0]):\n\t\tassert (len(resp_data['labels']) == 1)\n\t\tassert (resp_data['labels'][0]['id'] == category.public_id)\n\t\tresp_data = api_client.get_data('\/messages\/{}'.format(gmail_message.public_id), headers=headers)\n\t\tassert (len(resp_data['labels']) == 1)\n\t\tassert (resp_data['labels'][0]['id'] == category.public_id)\n\telse:\n\t\tassert (resp_data['labels'] == [])\n",["check that you can update a message ."]]
["def fitness_and_quality_parsed(mime_type, parsed_ranges):\n\t(best_fitness, best_fit_q) = ((-1), 0)\n\t(target_type, target_subtype, target_params) = parse_media_range(mime_type)\n\tfor (type, subtype, params) in parsed_ranges:\n\t\tif (((type == target_type) or (type == '*') or (target_type == '*')) and ((subtype == target_subtype) or (subtype == '*') or (target_subtype == '*'))):\n\t\t\tfitness = 0\n\t\t\tif (type == target_type):\n\t\t\t\tfitness += 100\n\t\t\tif (subtype == target_subtype):\n\t\t\t\tfitness += 10\n\t\t\tfor key in target_params:\n\t\t\t\tif ((key != 'q') and (key in params)):\n\t\t\t\t\tif (params[key] == target_params[key]):\n\t\t\t\t\t\tfitness += 1\n\t\t\tif (fitness > best_fitness):\n\t\t\t\tbest_fitness = fitness\n\t\t\t\tbest_fit_q = params['q']\n\treturn (best_fitness, float(best_fit_q))\n",["find the best match for a mime-type amongst parsed media-ranges ."]]
["def destroy(name, call=None):\n\tlog.info('Attempting\tto\tdelete\tinstance\t%s', name)\n\tif (not vb_machine_exists(name)):\n\t\treturn \"{0}\tdoesn't\texist\tand\tcan't\tbe\tdeleted\".format(name)\n\tcloud.fire_event('event', 'destroying\tinstance', 'salt\/cloud\/{0}\/destroying'.format(name), args={'name': name}, sock_dir=__opts__['sock_dir'], transport=__opts__['transport'])\n\tvb_destroy_machine(name)\n\tcloud.fire_event('event', 'destroyed\tinstance', 'salt\/cloud\/{0}\/destroyed'.format(name), args={'name': name}, sock_dir=__opts__['sock_dir'], transport=__opts__['transport'])\n",["destroy a node ."]]
["def concatenate(tensor_list, axis=0):\n\tconcat_size = sum((tt.shape[axis] for tt in tensor_list))\n\toutput_shape = ()\n\tfor k in range(axis):\n\t\toutput_shape += (tensor_list[0].shape[k],)\n\toutput_shape += (concat_size,)\n\tfor k in range((axis + 1), tensor_list[0].ndim):\n\t\toutput_shape += (tensor_list[0].shape[k],)\n\tout = tensor.zeros(output_shape)\n\toffset = 0\n\tfor tt in tensor_list:\n\t\tindices = ()\n\t\tfor k in range(axis):\n\t\t\tindices += (slice(None),)\n\t\tindices += (slice(offset, (offset + tt.shape[axis])),)\n\t\tfor k in range((axis + 1), tensor_list[0].ndim):\n\t\t\tindices += (slice(None),)\n\t\tout = tensor.set_subtensor(out[indices], tt)\n\t\toffset += tt.shape[axis]\n\treturn out\n",["alternative implementation of theano ."]]
["def encode_multipart_formdata(fields, boundary=None):\n\tbody = BytesIO()\n\tif (boundary is None):\n\t\tboundary = choose_boundary()\n\tfor (fieldname, value) in iter_fields(fields):\n\t\tbody.write(b(('--%s\\r\\n' % boundary)))\n\t\tif isinstance(value, tuple):\n\t\t\t(filename, data) = value\n\t\t\twriter(body).write(('Content-Disposition:\tform-data;\tname=\"%s\";\tfilename=\"%s\"\\r\\n' % (fieldname, filename)))\n\t\t\tbody.write(b(('Content-Type:\t%s\\r\\n\\r\\n' % get_content_type(filename))))\n\t\telse:\n\t\t\tdata = value\n\t\t\twriter(body).write(('Content-Disposition:\tform-data;\tname=\"%s\"\\r\\n' % fieldname))\n\t\t\tbody.write('Content-Type:\ttext\/plain\\r\\n\\r\\n')\n\t\tif isinstance(data, int):\n\t\t\tdata = str(data)\n\t\tif isinstance(data, six.text_type):\n\t\t\twriter(body).write(data)\n\t\telse:\n\t\t\tbody.write(data)\n\t\tbody.write('\\r\\n')\n\tbody.write(b(('--%s--\\r\\n' % boundary)))\n\tcontent_type = b(('multipart\/form-data;\tboundary=%s' % boundary))\n\treturn (body.getvalue(), content_type)\n",["fields is a sequence of elements for regular form fields ."]]
["def getPathByPrefix(path, prefix, xmlElement):\n\tif (len(path) < 2):\n\t\tprint 'Warning,\tbug,\tpath\tis\ttoo\tsmall\tin\tevaluate\tin\tsetPathByPrefix.'\n\t\treturn\n\tpathByKey = getPathByKey((prefix + 'path'), xmlElement)\n\tif (len(pathByKey) < len(path)):\n\t\tfor pointIndex in xrange(len(pathByKey)):\n\t\t\tpath[pointIndex] = pathByKey[pointIndex]\n\telse:\n\t\tpath = pathByKey\n\tpath[0] = getVector3ByPrefix(path[0], (prefix + 'pathStart'), xmlElement)\n\tpath[(-1)] = getVector3ByPrefix(path[(-1)], (prefix + 'pathEnd'), xmlElement)\n\treturn path\n",["get path from prefix and xml element ."]]
["@utils.no_4byte_params\ndef metadef_tag_update(context, namespace_name, id, tag_dict, session=None):\n\tsession = (session or get_session())\n\treturn metadef_tag_api.update(context, namespace_name, id, tag_dict, session)\n",["update a metadef tag ."]]
["def find_referenced_templates(ast):\n\tfor node in ast.find_all((nodes.Extends, nodes.FromImport, nodes.Import, nodes.Include)):\n\t\tif (isinstance(node.template, nodes.Const) and isinstance(node.template.value, basestring)):\n\t\t\t(yield node.template.value)\n\t\telse:\n\t\t\t(yield None)\n",["finds all the referenced templates from the ast ."]]
["def clean_html(buf):\n\tbuf = buf.strip()\n\tif (not buf):\n\t\treturn buf\n\thtml_parser = html5lib.HTMLParser(tree=treebuilders.getTreeBuilder('dom'), tokenizer=HTMLSanitizer)\n\tdom_tree = html_parser.parseFragment(buf)\n\twalker = treewalkers.getTreeWalker('dom')\n\tstream = walker(dom_tree)\n\ts = serializer.htmlserializer.HTMLSerializer(omit_optional_tags=False, quote_attr_values=True)\n\toutput = s.render(stream, 'utf-8')\n\twhile ('toberemoved' in output):\n\t\toldoutput = output\n\t\tmatches = re.findall('&lt;toberemoved.*?&gt;.*?&lt;\/toberemoved&gt;', output, re.DOTALL)\n\t\tfor s in matches:\n\t\t\toutput = output.replace(s, '')\n\t\tmatches = re.findall('&lt;\/toberemoved&gt;', output, re.DOTALL)\n\t\tfor s in matches:\n\t\t\toutput = output.replace(s, '')\n\t\tmatches = re.findall('&lt;toberemoved.*?&gt;', output, re.DOTALL)\n\t\tfor s in matches:\n\t\t\toutput = output.replace(s, '')\n\t\tif (output == oldoutput):\n\t\t\tbreak\n\treturn output\n",["clean the given html ."]]
["def _get_review_request_id_to_commits_map(payload, server_url, repository):\n\treview_request_id_to_commits_map = defaultdict(list)\n\tcommits = payload.get(u'commits', [])\n\tfor commit in commits:\n\t\tcommit_hash = commit.get(u'raw_node')\n\t\tcommit_message = commit.get(u'message')\n\t\tbranch_name = commit.get(u'branch')\n\t\tif branch_name:\n\t\t\treview_request_id = get_review_request_id(commit_message, server_url, commit_hash, repository)\n\t\t\treview_request_id_to_commits_map[review_request_id].append((u'%s\t(%s)' % (branch_name, commit_hash[:7])))\n\treturn review_request_id_to_commits_map\n",["returns a dictionary ."]]
["@FileSystem.in_directory(current_directory, 'django', 'grocery')\ndef test_django_admin_media_serving_on_django_13():\n\tos.environ['PYTHONPATH'] = ('%s:%s' % (FileSystem.join(lib_directory, 'Django-1.3'), OLD_PYTHONPATH))\n\t(status, out) = commands.getstatusoutput('python\tmanage.py\tharvest\t--verbosity=2\t.\/features\/')\n\tassert_equals(status, 0, out)\n\tlines = out.splitlines()\n\tassert (u\"Preparing\tto\tserve\tdjango's\tadmin\tsite\tstatic\tfiles...\" in lines)\n\tassert (u'Running\ton\tport\t7000\t...\tOK' in lines)\n\tassert (u'Fetching\tadmin\tmedia\t...\tOK' in lines)\n\tassert (u'Fetching\tstatic\tfiles\t...\tOK' in lines)\n\tassert (u'Fetching\tCSS\tfiles:\t...\tOK' in lines)\n\tassert (u'Fetching\tjavascript\tfiles:\t...\tOK' in lines)\n\tassert (u\"Django's\tbuiltin\tserver\tis\trunning\tat\t0.0.0.0:7000\" in lines)\n",["lettuce should serve admin static files properly on django 1 ."]]
["def get_images_table(meta):\n\t(get_images_table,) = from_migration_import('008_add_image_members_table', ['get_images_table'])\n\timages = get_images_table(meta)\n\treturn images\n",["returns the table object for the images table that corresponds to the images table definition of this version ."]]
["def get_version(package):\n\tinit_py = open(os.path.join(package, '__init__.py')).read()\n\treturn re.search('__version__\t=\t[\\'\"]([^\\'\"]+)[\\'\"]', init_py).group(1)\n",["returns a pep 386-compliant version number from version ."]]
["def parse(json_string):\n\ttry:\n\t\tjson_data = json.loads(json_string)\n\texcept:\n\t\traise SchemaParseException(('Error\tparsing\tJSON:\t%s' % json_string))\n\tnames = Names()\n\treturn make_avsc_object(json_data, names)\n",["parse metadata of the given video ."]]
["def get_data_files():\n\tdata_files = []\n\tntrim = len((here + os.path.sep))\n\tfor (d, dirs, filenames) in os.walk(share_jupyter):\n\t\tdata_files.append((d[ntrim:], [pjoin(d, f) for f in filenames]))\n\treturn data_files\n",["get all the data files that should be included in this distutils project ."]]
["def _configure(changes):\n\tcfgred = True\n\treasons = []\n\tfun = 'update_config'\n\tfor key in ['added', 'updated', 'removed']:\n\t\t_updated_changes = changes.get(key, {})\n\t\tif (not _updated_changes):\n\t\t\tcontinue\n\t\t_location = _updated_changes.get('location', '')\n\t\t_contact = _updated_changes.get('contact', '')\n\t\t_community = _updated_changes.get('community', {})\n\t\t_chassis_id = _updated_changes.get('chassis_id', '')\n\t\tif (key == 'removed'):\n\t\t\tfun = 'remove_config'\n\t\t_ret = __salt__['snmp.{fun}'.format(fun=fun)](location=_location, contact=_contact, community=_community, chassis_id=_chassis_id, commit=False)\n\t\tcfgred = (cfgred and _ret.get('result'))\n\t\tif ((not _ret.get('result')) and _ret.get('comment')):\n\t\t\treasons.append(_ret.get('comment'))\n\treturn {'result': cfgred, 'comment': ('\\n'.join(reasons) if reasons else '')}\n",["amend the global configuration object with command line options ."]]
["def file_dict(*packages):\n\terrors = []\n\tret = {}\n\tpkgs = {}\n\tcmd = 'dpkg\t-l\t{0}'.format('\t'.join(packages))\n\tout = __salt__['cmd.run_all'](cmd, python_shell=False)\n\tif (out['retcode'] != 0):\n\t\tmsg = ('Error:\t\t' + out['stderr'])\n\t\tlog.error(msg)\n\t\treturn msg\n\tout = out['stdout']\n\tfor line in out.splitlines():\n\t\tif line.startswith('ii\t'):\n\t\t\tcomps = line.split()\n\t\t\tpkgs[comps[1]] = {'version': comps[2], 'description': '\t'.join(comps[3:])}\n\t\tif ('No\tpackages\tfound' in line):\n\t\t\terrors.append(line)\n\tfor pkg in pkgs:\n\t\tfiles = []\n\t\tcmd = 'dpkg\t-L\t{0}'.format(pkg)\n\t\tfor line in __salt__['cmd.run'](cmd, python_shell=False).splitlines():\n\t\t\tfiles.append(line)\n\t\tret[pkg] = files\n\treturn {'errors': errors, 'packages': ret}\n",["list the files that belong to a package ."]]
["def test_enn_fit_sample():\n\tenn = EditedNearestNeighbours(random_state=RND_SEED)\n\t(X_resampled, y_resampled) = enn.fit_sample(X, Y)\n\tX_gt = np.array([[(-0.10903849), (-0.12085181)], [0.01936241, 0.17799828], [2.59928271, 0.93323465], [1.92365863, 0.82718767], [0.25738379, 0.95564169], [0.78318102, 2.59153329], [0.52726792, (-0.38735648)]])\n\ty_gt = np.array([0, 0, 1, 1, 2, 2, 2])\n\tassert_array_equal(X_resampled, X_gt)\n\tassert_array_equal(y_resampled, y_gt)\n",["test the fit sample routine ."]]
["def patch_response_headers(response, cache_timeout=None):\n\tif (cache_timeout is None):\n\t\tcache_timeout = settings.CACHE_MIDDLEWARE_SECONDS\n\tnow = datetime.datetime.utcnow()\n\tif (not response.has_header('ETag')):\n\t\tresponse['ETag'] = md5.new(response.content).hexdigest()\n\tif (not response.has_header('Last-Modified')):\n\t\tresponse['Last-Modified'] = now.strftime('%a,\t%d\t%b\t%Y\t%H:%M:%S\tGMT')\n\tif (not response.has_header('Expires')):\n\t\texpires = (now + datetime.timedelta(0, cache_timeout))\n\t\tresponse['Expires'] = expires.strftime('%a,\t%d\t%b\t%Y\t%H:%M:%S\tGMT')\n\tif (cache_timeout < 0):\n\t\tcache_timeout = 0\n\tpatch_cache_control(response, max_age=cache_timeout)\n",["adds some useful headers to the given httpresponse object: etag ."]]
["def prune_dirs(path, root=None, clutter=('.DS_Store', 'Thumbs.db')):\n\tpath = normpath(path)\n\tif (root is not None):\n\t\troot = normpath(root)\n\tancestors = ancestry(path)\n\tif (root is None):\n\t\tancestors = []\n\telif (root in ancestors):\n\t\tancestors = ancestors[(ancestors.index(root) + 1):]\n\telse:\n\t\treturn\n\tancestors.append(path)\n\tancestors.reverse()\n\tfor directory in ancestors:\n\t\tdirectory = syspath(directory)\n\t\tif (not os.path.exists(directory)):\n\t\t\tcontinue\n\t\tclutter = [bytestring_path(c) for c in clutter]\n\t\tmatch_paths = [bytestring_path(d) for d in os.listdir(directory)]\n\t\tif fnmatch_all(match_paths, clutter):\n\t\t\ttry:\n\t\t\t\tshutil.rmtree(directory)\n\t\t\texcept OSError:\n\t\t\t\tbreak\n\t\telse:\n\t\t\tbreak\n",["if path is an empty directory ."]]
["def find_scene_absolute_numbering(indexer_id, indexer, absolute_number):\n\tif ((indexer_id is None) or (absolute_number is None)):\n\t\treturn absolute_number\n\tindexer_id = int(indexer_id)\n\tindexer = int(indexer)\n\tmain_db_con = db.DBConnection()\n\trows = main_db_con.select('SELECT\tscene_absolute_number\tFROM\tscene_numbering\tWHERE\tindexer\t=\t?\tand\tindexer_id\t=\t?\tand\tabsolute_number\t=\t?\tand\tscene_absolute_number\t!=\t0', [indexer, indexer_id, absolute_number])\n\tif rows:\n\t\treturn int(rows[0]['scene_absolute_number'])\n",["same as get_scene_numbering() ."]]
["@register.filter(is_safe=True, needs_autoescape=True)\n@stringfilter\ndef linenumbers(value, autoescape=True):\n\tlines = value.split('\\n')\n\twidth = str(len(str(len(lines))))\n\tif ((not autoescape) or isinstance(value, SafeData)):\n\t\tfor (i, line) in enumerate(lines):\n\t\t\tlines[i] = ((('%0' + width) + 'd.\t%s') % ((i + 1), line))\n\telse:\n\t\tfor (i, line) in enumerate(lines):\n\t\t\tlines[i] = ((('%0' + width) + 'd.\t%s') % ((i + 1), escape(line)))\n\treturn mark_safe('\\n'.join(lines))\n",["displays text with line numbers ."]]
["def health_checks(consul_url=None, service=None, **kwargs):\n\tret = {}\n\tquery_params = {}\n\tif (not consul_url):\n\t\tconsul_url = _get_config()\n\t\tif (not consul_url):\n\t\t\tlog.error('No\tConsul\tURL\tfound.')\n\t\t\tret['message'] = 'No\tConsul\tURL\tfound.'\n\t\t\tret['res'] = False\n\t\t\treturn ret\n\tif (not service):\n\t\traise SaltInvocationError('Required\targument\t\"service\"\tis\tmissing.')\n\tif ('dc' in kwargs):\n\t\tquery_params['dc'] = kwargs['dc']\n\tfunction = 'health\/checks\/{0}'.format(service)\n\tret = _query(consul_url=consul_url, function=function, query_params=query_params)\n\treturn ret\n",["health information about the registered service ."]]
["def get_delta(name):\n\t[curr_metrics, last_metrics] = get_metrics()\n\tmetric_name_list = name.split('_')[1:]\n\tmetric_name = '_'.join(metric_name_list)\n\ttry:\n\t\tdelta = ((float(curr_metrics['data'][metric_name]) - float(last_metrics['data'][metric_name])) \/ (curr_metrics['time'] - last_metrics['time']))\n\t\tif (delta < 0):\n\t\t\tif Debug:\n\t\t\t\tprint (name + '\tis\tless\t0.\tSetting\tvalue\tto\t0.')\n\t\t\tdelta = 0\n\texcept KeyError:\n\t\tif Debug:\n\t\t\tprint (('Key\t' + name) + \"\tcan't\tbe\tfound.\")\n\t\tdelta = 0.0\n\treturn delta\n",["return change over time for the requested metric ."]]
["def reverse(endpoint, args=None, kwargs=None, is_dashboard_endpoint=True):\n\tis_endpoint_declared = ((endpoint in INSTRUCTOR_GET_ENDPOINTS) or (endpoint in INSTRUCTOR_POST_ENDPOINTS))\n\tif (is_dashboard_endpoint and (is_endpoint_declared is False)):\n\t\traise ValueError('The\tendpoint\t{}\tmust\tbe\tdeclared\tin\tENDPOINTS\tbefore\tuse.'.format(endpoint))\n\treturn django_reverse(endpoint, args=args, kwargs=kwargs)\n",["wraps djangos reverse to prepend the correct locale ."]]
["def check_valid_naming(pattern=None, multi=None, anime_type=None):\n\tif (pattern is None):\n\t\tpattern = sickbeard.NAMING_PATTERN\n\tif (anime_type is None):\n\t\tanime_type = sickbeard.NAMING_ANIME\n\tlogger.log(((u'Checking\twhether\tthe\tpattern\t' + pattern) + u'\tis\tvalid\tfor\ta\tsingle\tepisode'), logger.DEBUG)\n\tvalid = validate_name(pattern, None, anime_type)\n\tif (multi is not None):\n\t\tlogger.log(((u'Checking\twhether\tthe\tpattern\t' + pattern) + u'\tis\tvalid\tfor\ta\tmulti\tepisode'), logger.DEBUG)\n\t\tvalid = (valid and validate_name(pattern, multi, anime_type))\n\treturn valid\n",["checks if the name is can be parsed back to its original form for both single and multi episodes ."]]
["def download_setuptools(version=DEFAULT_VERSION, download_base=DEFAULT_URL, to_dir=os.curdir, delay=15):\n\timport urllib2, shutil\n\tegg_name = ('setuptools-%s-py%s.egg' % (version, sys.version[:3]))\n\turl = (download_base + egg_name)\n\tsaveto = os.path.join(to_dir, egg_name)\n\tsrc = dst = None\n\tif (not os.path.exists(saveto)):\n\t\ttry:\n\t\t\tfrom distutils import log\n\t\t\tif delay:\n\t\t\t\tlog.warn('\\n---------------------------------------------------------------------------\\nThis\tscript\trequires\tsetuptools\tversion\t%s\tto\trun\t(even\tto\tdisplay\\nhelp).\t\tI\twill\tattempt\tto\tdownload\tit\tfor\tyou\t(from\\n%s),\tbut\\nyou\tmay\tneed\tto\tenable\tfirewall\taccess\tfor\tthis\tscript\tfirst.\\nI\twill\tstart\tthe\tdownload\tin\t%d\tseconds.\\n\\n(Note:\tif\tthis\tmachine\tdoes\tnot\thave\tnetwork\taccess,\tplease\tobtain\tthe\tfile\\n\\n\t\t\t%s\\n\\nand\tplace\tit\tin\tthis\tdirectory\tbefore\trerunning\tthis\tscript.)\\n---------------------------------------------------------------------------', version, download_base, delay, url)\n\t\t\t\tfrom time import sleep\n\t\t\t\tsleep(delay)\n\t\t\tlog.warn('Downloading\t%s', url)\n\t\t\tsrc = urllib2.urlopen(url)\n\t\t\tdata = _validate_md5(egg_name, src.read())\n\t\t\tdst = open(saveto, 'wb')\n\t\t\tdst.write(data)\n\t\tfinally:\n\t\t\tif src:\n\t\t\t\tsrc.close()\n\t\t\tif dst:\n\t\t\t\tdst.close()\n\treturn os.path.realpath(saveto)\n",["download setuptools from a specified location and return its filename version should be a valid setuptools version number that is available as an egg for download under the"]]
["def list_nodes_full(call=None):\n\tif (call == 'action'):\n\t\traise SaltCloudSystemExit('The\tlist_nodes_full\tfunction\tmust\tbe\tcalled\twith\t-f\tor\t--function.')\n\tret = {}\n\titems = query(action='ve')\n\tfor item in items:\n\t\tname = item.attrib['name']\n\t\tnode = show_instance(name, call='action')\n\t\tret[name] = node\n\t\tret[name]['image'] = node['platform']['template-info']['name']\n\t\tif ('private-ip' in node['network']):\n\t\t\tret[name]['private_ips'] = [node['network']['private-ip']['address']]\n\t\tif ('public-ip' in node['network']):\n\t\t\tret[name]['public_ips'] = [node['network']['public-ip']['address']]\n\treturn ret\n",["return a list of the vms that are on the provider ."]]
["def _i18n_cache_key_suffix(request, cache_key):\n\tif (settings.USE_I18N or settings.USE_L10N):\n\t\tcache_key += ('.%s' % getattr(request, 'LANGUAGE_CODE', get_language()))\n\tif settings.USE_TZ:\n\t\ttz_name = force_text(get_current_timezone_name(), errors='ignore')\n\t\tcache_key += ('.%s' % tz_name.encode('ascii', 'ignore').decode('ascii').replace('\t', '_'))\n\treturn cache_key\n",["if necessary ."]]
["def apodize(data, ms=5, rate=44100):\n\thw_size = int(min((rate \/\/ (1000 \/ ms)), (len(data) \/\/ 15)))\n\thamming_window = np.hamming(((2 * hw_size) + 1))\n\tdata[:hw_size] *= hamming_window[:hw_size]\n\tdata[(- hw_size):] *= hamming_window[(- hw_size):]\n\treturn data\n",["apply a hamming window to reduce a sounds click onset \/ offset ."]]
["def run_sampler(dec, c, beam_width=1, stochastic=False, use_unk=False):\n\t(sample, score) = gen_sample(dec['tparams'], dec['f_init'], dec['f_next'], c.reshape(1, dec['options']['dimctx']), dec['options'], trng=dec['trng'], k=beam_width, maxlen=1000, stochastic=stochastic, use_unk=use_unk)\n\ttext = []\n\tif stochastic:\n\t\tsample = [sample]\n\tfor c in sample:\n\t\ttext.append('\t'.join([dec['word_idict'][w] for w in c[:(-1)]]))\n\treturn text\n",["generate text conditioned on c ."]]
["def resource(*args, **kwargs):\n\treturn _get_default_session().resource(*args, **kwargs)\n",["restful crud controller ."]]
["def extract_params(raw):\n\tif (isinstance(raw, bytes_type) or isinstance(raw, unicode_type)):\n\t\ttry:\n\t\t\tparams = urldecode(raw)\n\t\texcept ValueError:\n\t\t\tparams = None\n\telif hasattr(raw, u'__iter__'):\n\t\ttry:\n\t\t\tdict(raw)\n\t\texcept ValueError:\n\t\t\tparams = None\n\t\texcept TypeError:\n\t\t\tparams = None\n\t\telse:\n\t\t\tparams = list((raw.items() if isinstance(raw, dict) else raw))\n\t\t\tparams = decode_params_utf8(params)\n\telse:\n\t\tparams = None\n\treturn params\n",["extract parameters and return them as a list of 2-tuples ."]]
["@task(help={'args': 'Command\tline\targs\tfor\tbehave', 'format': 'Formatter\tto\tuse'})\ndef behave_test(ctx, args='', format=''):\n\tformat = (format or ctx.behave_test.format)\n\toptions = (ctx.behave_test.options or '')\n\targs = (args or ctx.behave_test.args)\n\tbehave = '{python}\tbin\/behave'.format(python=sys.executable)\n\tctx.run('{behave}\t-f\t{format}\t{options}\t{args}'.format(behave=behave, format=format, options=options, args=args), pty=USE_PTY)\n",["run behave tests ."]]
["def remove_invalid_options(context, search_options, allowed_search_options):\n\tif context.is_admin:\n\t\tfor key in ('sort_key', 'sort_dir', 'limit', 'marker'):\n\t\t\tsearch_options.pop(key, None)\n\t\treturn\n\tunknown_options = [opt for opt in search_options if (opt not in allowed_search_options)]\n\tif unknown_options:\n\t\tLOG.debug(\"Removing\toptions\t'%s'\tfrom\tquery\", ',\t'.join(unknown_options))\n\t\tfor opt in unknown_options:\n\t\t\tsearch_options.pop(opt, None)\n",["remove search options that are not valid for non-admin api\/context ."]]
["def _parse_optional(fh):\n\toptional = {'StartKernData': _parse_kern_pairs, 'StartComposites': _parse_composites}\n\td = {'StartKernData': {}, 'StartComposites': {}}\n\twhile 1:\n\t\tline = fh.readline()\n\t\tif (not line):\n\t\t\tbreak\n\t\tline = line.rstrip()\n\t\tif (len(line) == 0):\n\t\t\tcontinue\n\t\tkey = line.split()[0]\n\t\tif (key in optional):\n\t\t\td[key] = optional[key](fh)\n\tl = (d['StartKernData'], d['StartComposites'])\n\treturn l\n",["parse the optional fields for kern pair data and composites return value is a which are the return values from :func:_parse_kern_pairs ."]]
["def _create_gcm_cipher(factory, **kwargs):\n\ttry:\n\t\tkey = kwargs.pop('key')\n\texcept KeyError as e:\n\t\traise TypeError(('Missing\tparameter:' + str(e)))\n\tnonce = kwargs.pop('nonce', None)\n\tif (nonce is None):\n\t\tnonce = get_random_bytes(16)\n\tmac_len = kwargs.pop('mac_len', 16)\n\treturn GcmMode(factory, key, nonce, mac_len, kwargs)\n",["create a new block cipher ."]]
["def notify(conf, context, topic, msg, envelope):\n\ttopic = topic.replace('.', '-')\n\tcast(conf, context, topic, msg, envelope=envelope)\n",["sends a notification via rpc ."]]
["def getLoopLayers(archivableObjects, importRadius, layerHeight, maximumZ, shouldPrintWarning, z, zoneArrangement):\n\tloopLayers = []\n\twhile (z <= maximumZ):\n\t\ttriangle_mesh.getLoopLayerAppend(loopLayers, z).loops = getEmptyZLoops(archivableObjects, importRadius, True, z, zoneArrangement)\n\t\tz += layerHeight\n\treturn loopLayers\n",["get loop layers ."]]
["def import_object(import_str, *args, **kwargs):\n\treturn import_class(import_str)(*args, **kwargs)\n",["imports an object by name ."]]
["def new(rsa_key):\n\treturn PKCS115_SigScheme(rsa_key)\n",["return a fresh instance of the hash object ."]]
["def url_to_path(url):\n\tassert url.startswith('file:'), ('You\tcan\tonly\tturn\tfile:\turls\tinto\tfilenames\t(not\t%r)' % url)\n\t(_, netloc, path, _, _) = urllib_parse.urlsplit(url)\n\tif netloc:\n\t\tnetloc = ('\\\\\\\\' + netloc)\n\tpath = urllib_request.url2pathname((netloc + path))\n\treturn path\n",["convert a file: url to a path ."]]
["def start_remote_debugger(rpcclt, pyshell):\n\tglobal idb_adap_oid\n\tidb_adap_oid = rpcclt.remotecall('exec', 'start_the_debugger', (gui_adap_oid,), {})\n\tidb_proxy = IdbProxy(rpcclt, pyshell, idb_adap_oid)\n\tgui = debugger.Debugger(pyshell, idb_proxy)\n\tgui_adap = GUIAdapter(rpcclt, gui)\n\trpcclt.register(gui_adap_oid, gui_adap)\n\treturn gui\n",["start the subprocess debugger ."]]
["def getOutput(gcodeText, binary16ByteRepository=None):\n\tif (gcodeText == ''):\n\t\treturn ''\n\tif (binary16ByteRepository == None):\n\t\tbinary16ByteRepository = Binary16ByteRepository()\n\t\tsettings.getReadRepository(binary16ByteRepository)\n\treturn Binary16ByteSkein().getCraftedGcode(gcodeText, binary16ByteRepository)\n",["get the exported version of a gcode file ."]]
["def win32FontDirectory():\n\ttry:\n\t\tfrom six.moves import winreg\n\texcept ImportError:\n\t\tpass\n\telse:\n\t\ttry:\n\t\t\tuser = winreg.OpenKey(winreg.HKEY_CURRENT_USER, MSFolders)\n\t\t\ttry:\n\t\t\t\treturn winreg.QueryValueEx(user, u'Fonts')[0]\n\t\t\texcept OSError:\n\t\t\t\tpass\n\t\t\tfinally:\n\t\t\t\twinreg.CloseKey(user)\n\t\texcept OSError:\n\t\t\tpass\n\treturn os.path.join(os.environ[u'WINDIR'], u'Fonts')\n",["return the user-specified font directory for win32 ."]]
["def view(tpl_name, **defaults):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tif isinstance(result, (dict, DictMixin)):\n\t\t\t\ttplvars = defaults.copy()\n\t\t\t\ttplvars.update(result)\n\t\t\t\treturn template(tpl_name, **tplvars)\n\t\t\telif (result is None):\n\t\t\t\treturn template(tpl_name, defaults)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n",["decorator: renders a template for a handler ."]]
["def _CheckDocument(document):\n\tno_repeat_date_names = set()\n\tno_repeat_number_names = set()\n\tfor field in document.fields:\n\t\tif isinstance(field, NumberField):\n\t\t\tif (field.name in no_repeat_number_names):\n\t\t\t\traise ValueError(('Invalid\tdocument\t%s:\tfield\t%s\twith\ttype\tdate\tor\tnumber\tmay\tnot\tbe\trepeated.' % (document.doc_id, field.name)))\n\t\t\tno_repeat_number_names.add(field.name)\n\t\telif isinstance(field, DateField):\n\t\t\tif (field.name in no_repeat_date_names):\n\t\t\t\traise ValueError(('Invalid\tdocument\t%s:\tfield\t%s\twith\ttype\tdate\tor\tnumber\tmay\tnot\tbe\trepeated.' % (document.doc_id, field.name)))\n\t\t\tno_repeat_date_names.add(field.name)\n",["check that the document is valid ."]]
["def report():\n\tt = Twitter(auth=authen())\n\tscreen_name = g['stuff'].split()[0]\n\tif screen_name.startswith('@'):\n\t\tt.users.report_spam(screen_name=screen_name[1:])\n\t\tprintNicely(green((('You\treported\t' + screen_name) + '.')))\n\telse:\n\t\tprintNicely(red(\"Sorry\tI\tcan't\tunderstand.\"))\n",["summary report about a list of issues ."]]
["def get_session(args):\n\tengine_uri = args.engine_uri\n\tgot_from = 'command\tline'\n\tif (engine_uri is None):\n\t\t(engine_uri, got_from) = defaults.get_default_db_uri_with_origin()\n\tsession = pokedex.db.connect(engine_uri)\n\tif args.verbose:\n\t\tprint(('Connected\tto\tdatabase\t%(engine)s\t(from\t%(got_from)s)' % dict(engine=session.bind.url, got_from=got_from)))\n\treturn session\n",["return a sqlalchemy session ."]]
["def versions_from_parentdir(parentdir_prefix, root, verbose):\n\tdirname = os.path.basename(root)\n\tif (not dirname.startswith(parentdir_prefix)):\n\t\tif verbose:\n\t\t\tprint((\"guessing\trootdir\tis\t'%s',\tbut\t'%s'\tdoesn't\tstart\twith\tprefix\t'%s'\" % (root, dirname, parentdir_prefix)))\n\t\traise NotThisMethod(\"rootdir\tdoesn't\tstart\twith\tparentdir_prefix\")\n\treturn {'version': dirname[len(parentdir_prefix):], 'full-revisionid': None, 'dirty': False, 'error': None}\n",["try to determine the version from the parent directory name ."]]
["def libvlc_media_new_as_node(p_instance, psz_name):\n\tf = (_Cfunctions.get('libvlc_media_new_as_node', None) or _Cfunction('libvlc_media_new_as_node', ((1,), (1,)), class_result(Media), ctypes.c_void_p, Instance, ctypes.c_char_p))\n\treturn f(p_instance, psz_name)\n",["create a media as an empty node with a given name ."]]
["def Deserializer(stream_or_string, **options):\n\tif isinstance(stream_or_string, basestring):\n\t\tstream = StringIO(stream_or_string)\n\telse:\n\t\tstream = stream_or_string\n\tfor obj in PythonDeserializer(yaml.load(stream), **options):\n\t\t(yield obj)\n",["deserialize a stream or string of json data ."]]
["def getCharacterIntegerString(character, offset, splitLine, step):\n\tfloatValue = getFloatFromCharacterSplitLine(character, splitLine)\n\tif (floatValue == None):\n\t\treturn None\n\tfloatValue += offset\n\tintegerValue = int(round(float((floatValue \/ step))))\n\treturn (character + str(integerValue))\n",["get a character and integer string ."]]
["def _parse_version(text):\n\t(major, major2, minor) = VERSION_RE.search(text).groups()\n\ttry:\n\t\treturn (int(major), int(major2), int(minor))\n\texcept (ValueError, TypeError):\n\t\treturn (int(major), int(major2), None)\n",["internal parsing method ."]]
["def _py_convert_agg_to_wx_image(agg, bbox):\n\timage = wx.EmptyImage(int(agg.width), int(agg.height))\n\timage.SetData(agg.tostring_rgb())\n\tif (bbox is None):\n\t\treturn image\n\telse:\n\t\treturn wx.ImageFromBitmap(_clipped_image_as_bitmap(image, bbox))\n",["convert the region of the agg buffer bounded by bbox to a wx ."]]
["def has_purchased(f):\n\t@functools.wraps(f)\n\tdef wrapper(request, addon, *args, **kw):\n\t\tif (addon.is_premium() and (not addon.has_purchased(request.user))):\n\t\t\tlog.info(('Not\tpurchased:\t%d' % addon.pk))\n\t\t\traise PermissionDenied\n\t\treturn f(request, addon, *args, **kw)\n\treturn wrapper\n",["if the addon is premium ."]]
["def set_clean(using=None):\n\tget_connection(using).set_clean()\n",["resets a dirty flag for the current thread and code streak ."]]
["def get_all_qoss(tenant_id):\n\tLOG.debug(_('get_all_qoss()\tcalled'))\n\tsession = db.get_session()\n\ttry:\n\t\tqoss = session.query(l2network_models.QoS).filter_by(tenant_id=tenant_id).all()\n\t\treturn qoss\n\texcept exc.NoResultFound:\n\t\treturn []\n",["lists all the qos to tenant associations ."]]
["@dec.skip_if_not_win32\ndef test_arg_split_win32():\n\ttests = [['hi', ['hi']], [u'hi', [u'hi']], ['hello\tthere', ['hello', 'there']], [u'h\\u01cello', [u'h\\u01cello']], ['something\t\"with\tquotes\"', ['something', 'with\tquotes']]]\n\tfor (argstr, argv) in tests:\n\t\tnt.assert_equal(arg_split(argstr), argv)\n",["ensure that argument lines are correctly split like in a shell ."]]
["def test_ioerror_if_replay_dir_creation_fails(mock_ensure_failure, replay_test_dir):\n\twith pytest.raises(IOError):\n\t\treplay.dump(replay_test_dir, 'foo', {'cookiecutter': {'hello': 'world'}})\n\tmock_ensure_failure.assert_called_once_with(replay_test_dir)\n",["test that replay ."]]
["def getOutput(gcodeText, binary16ByteRepository=None):\n\tif (gcodeText == ''):\n\t\treturn ''\n\tif (binary16ByteRepository == None):\n\t\tbinary16ByteRepository = Binary16ByteRepository()\n\t\tsettings.getReadRepository(binary16ByteRepository)\n\treturn Binary16ByteSkein().getCraftedGcode(gcodeText, binary16ByteRepository)\n",["get the exported version of a gcode file ."]]
["def umount(name, device=None, user=None, util='mount'):\n\tif (util != 'mount'):\n\t\tif ('qemu_nbd.clear' in __salt__):\n\t\t\tif ('img.mnt_{0}'.format(name) in __context__):\n\t\t\t\t__salt__['qemu_nbd.clear'](__context__['img.mnt_{0}'.format(name)])\n\t\t\t\treturn\n\tmnts = active()\n\tif (name not in mnts):\n\t\treturn '{0}\tdoes\tnot\thave\tanything\tmounted'.format(name)\n\tif (not device):\n\t\tcmd = 'umount\t{0}'.format(name)\n\telse:\n\t\tcmd = 'umount\t{0}'.format(device)\n\tout = __salt__['cmd.run_all'](cmd, runas=user, python_shell=False)\n\tif out['retcode']:\n\t\treturn out['stderr']\n\treturn True\n",["unmount a path ."]]
["def splittype(url):\n\tglobal _typeprog\n\tif (_typeprog is None):\n\t\t_typeprog = re.compile('([^\/:]+):(.*)', re.DOTALL)\n\tmatch = _typeprog.match(url)\n\tif match:\n\t\t(scheme, data) = match.groups()\n\t\treturn (scheme.lower(), data)\n\treturn (None, url)\n",["splittype --> type ."]]
["def getAroundsFromPath(path, radius, thresholdRatio=0.9):\n\treturn getAroundsFromPoints(getPointsFromPath(path, (1.01 * abs(radius)), thresholdRatio), radius)\n",["get the arounds from the path ."]]
["def resolve():\n\tfilename = '\/'.join(request.args)\n\tpath = apath(filename, r=request)\n\ta = safe_read(path).split('\\n')\n\ttry:\n\t\tb = safe_read((path + '.1')).split('\\n')\n\texcept IOError:\n\t\tsession.flash = 'Other\tfile,\tno\tlonger\tthere'\n\t\tredirect(URL('edit', args=request.args))\n\td = difflib.ndiff(a, b)\n\tdef leading(line):\n\t\t'\t\t'\n\t\tz = ''\n\t\tfor (k, c) in enumerate(line):\n\t\t\tif (c == '\t'):\n\t\t\t\tz += '&nbsp;'\n\t\t\telif (c == '\t DCTB '):\n\t\t\t\tz += '&nbsp;'\n\t\t\telif ((k == 0) and (c == '?')):\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tbreak\n\t\treturn XML(z)\n\tdef getclass(item):\n\t\t'\tDetermine\titem\tclass\t'\n\t\toperators = {'\t': 'normal', '+': 'plus', '-': 'minus'}\n\t\treturn operators[item[0]]\n\tif request.vars:\n\t\tc = '\\n'.join([item[2:].rstrip() for (i, item) in enumerate(d) if ((item[0] == '\t') or (('line%i' % i) in request.vars))])\n\t\tsafe_write(path, c)\n\t\tsession.flash = 'files\tmerged'\n\t\tredirect(URL('edit', args=request.args))\n\telse:\n\t\tgen_data = (lambda index, item: (((not (item[:1] in ['+', '-'])) and '') or INPUT(_type='checkbox', _name=('line%i' % index), value=(item[0] == '+'))))\n\t\tdiff = TABLE(*[TR(TD(gen_data(i, item)), TD(item[0]), TD(leading(item[2:]), TT(item[2:].rstrip())), _class=getclass(item)) for (i, item) in enumerate(d) if (item[0] != '?')])\n\treturn dict(diff=diff, filename=filename)\n",["given an object or a path to an object ."]]
["def unnotify(thing, possible_recipients=None):\n\tfrom r2.lib import butler\n\terror_message = ('Unable\tto\tunnotify\tthing\tof\ttype:\t%r' % thing)\n\tnotification_handler(thing, notify_function=butler.remove_mention_notification, error_message=error_message, possible_recipients=possible_recipients)\n",["given a thing ."]]
["def main():\n\terrors = 0\n\tfits_files = handle_options(sys.argv[1:])\n\tsetup_logging()\n\tfor filename in fits_files:\n\t\terrors += process_file(filename)\n\tif errors:\n\t\tlog.warning('{}\terrors'.format(errors))\n\treturn int(bool(errors))\n",["this function returns a pyramid wsgi application ."]]
["def is_ip_addr(ip):\n\ttry:\n\t\tsocket.inet_aton(ip)\n\t\treturn ip\n\texcept:\n\t\traise ArgumentTypeError(('%r\tis\tnot\tan\tIP\taddress!' % ip))\n",["check that the supplied value is an internet protocol address ."]]
["def next_char(input_iter):\n\tfor ch in input_iter:\n\t\tif (ch != u'\\\\'):\n\t\t\t(yield (ch, False))\n\t\t\tcontinue\n\t\tch = next(input_iter)\n\t\trepresentative = ESCAPE_MAPPINGS.get(ch, ch)\n\t\tif (representative is None):\n\t\t\tcontinue\n\t\t(yield (representative, True))\n",["an iterator that yields the next character from \"pattern_iter\" ."]]
["def safe_repr(o):\n\ttry:\n\t\treturn repr(o)\n\texcept:\n\t\treturn _safeFormat(repr, o)\n",["copied from python2 ."]]
["def writeOutput(fileName=''):\n\tfileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName)\n\tif (fileName != ''):\n\t\tskeinforge_craft.writeChainTextWithNounMessage(fileName, 'unpause')\n",["outset the carving of a gcode file ."]]
["@register.inclusion_tag(get_template('inclusion.html'), takes_context=False)\ndef inclusion_explicit_no_context_from_template(arg):\n\treturn {'result': ('inclusion_explicit_no_context_from_template\t-\tExpected\tresult:\t%s' % arg)}\n",["expected inclusion_explicit_no_context_from_template __doc__ ."]]
["def user_follower_list(context, data_dict):\n\t_check_access('user_follower_list', context, data_dict)\n\treturn _follower_list(context, data_dict, ckan.logic.schema.default_follow_user_schema(), context['model'].UserFollowingUser)\n",["return the list of users that are following the given user ."]]
["def sslwrap_simple(sock, keyfile=None, certfile=None):\n\tif hasattr(sock, '_sock'):\n\t\tsock = sock._sock\n\tssl_sock = _ssl.sslwrap(sock, 0, keyfile, certfile, CERT_NONE, PROTOCOL_SSLv23, None)\n\ttry:\n\t\tsock.getpeername()\n\texcept:\n\t\tpass\n\telse:\n\t\tssl_sock.do_handshake()\n\treturn ssl_sock\n",["a replacement for the old socket ."]]
["def _reraise_translated_image_exception(image_id):\n\t(_exc_type, exc_value, exc_trace) = sys.exc_info()\n\tnew_exc = _translate_image_exception(image_id, exc_value)\n\tsix.reraise(type(new_exc), new_exc, exc_trace)\n",["transform the exception for the image but keep its traceback intact ."]]
["def _parsems(value):\n\tif ('.' not in value):\n\t\treturn (int(value), 0)\n\telse:\n\t\t(i, f) = value.split('.')\n\t\treturn (int(i), int(f.ljust(6, '0')[:6]))\n",["parse a i[ ."]]
["def filter_factory(global_conf, **local_conf):\n\tconf = global_conf.copy()\n\tconf.update(local_conf)\n\tregister_swift_info('formpost')\n\treturn (lambda app: FormPost(app, conf))\n",["returns a static web wsgi filter for use with paste ."]]
["def _totuple(x):\n\tif isinstance(x, basestring):\n\t\tout = (x,)\n\telif isinstance(x, (int, float)):\n\t\tout = (str(x),)\n\telif (x is None):\n\t\tout = (None,)\n\telse:\n\t\tout = tuple(x)\n\treturn out\n",["utility stuff to convert string ."]]
["def _rshift_nearest(x, shift):\n\t(b, q) = ((1L << shift), (x >> shift))\n\treturn (q + (((2 * (x & (b - 1))) + (q & 1)) > b))\n",["given an integer x and a nonnegative integer shift ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only works on windows systems ."]]
["def convert_DateTimeProperty(model, prop, kwargs):\n\tif (prop.auto_now or prop.auto_now_add):\n\t\treturn None\n\tkwargs.setdefault('format', '%Y-%m-%d\t%H:%M:%S')\n\treturn f.DateTimeField(**kwargs)\n",["returns a form field for a db ."]]
["def open_if_exists(filename, mode='rb'):\n\ttry:\n\t\treturn open(filename, mode)\n\texcept IOError as e:\n\t\tif (e.errno not in (errno.ENOENT, errno.EISDIR)):\n\t\t\traise\n",["returns a file descriptor for the filename if that file exists ."]]
["def get_course_organizations(course_id):\n\tif (not organizations_enabled()):\n\t\treturn []\n\tfrom organizations import api as organizations_api\n\treturn organizations_api.get_course_organizations(course_id)\n",["client api operation adapter\/wrapper ."]]
["def connect_to_region(region_name, **kw_params):\n\tfor region in regions():\n\t\tif (region.name == region_name):\n\t\t\treturn region.connect(**kw_params)\n\treturn None\n",["given a valid region name ."]]
["def getManipulatedGeometryOutput(elementNode, geometryOutput, prefix):\n\tflippedGeometryOutput = triangle_mesh.getGeometryOutputCopy(geometryOutput)\n\tflip.flipPoints(elementNode, matrix.getVertexes(flippedGeometryOutput), prefix)\n\tif flip.getShouldReverse(elementNode, prefix):\n\t\tflippedFaces = face.getFaces(flippedGeometryOutput)\n\t\tfor flippedFace in flippedFaces:\n\t\t\tflippedFace.vertexIndexes.reverse()\n\treturn {'union': {'shapes': [flippedGeometryOutput, geometryOutput]}}\n",["get equated geometryoutput ."]]
["def _popen(cmd):\n\tp = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE, close_fds=(os.name != 'nt'), universal_newlines=True)\n\treturn p.communicate()\n",["friendly wrapper around popen for windows ."]]
["@require_admin_context\ndef volume_include_in_cluster(context, cluster, partial_rename=True, **filters):\n\treturn _include_in_cluster(context, cluster, models.Volume, partial_rename, filters)\n",["include all volumes matching the filters into a cluster ."]]
["def drange(v0, v1, d):\n\tassert (v0 < v1)\n\treturn xrange((int(v0) \/\/ d), (int((v1 + d)) \/\/ d))\n",["returns a discrete range ."]]
["def decade_down(x, base=10):\n\tlx = math.floor((math.log(x) \/ math.log(base)))\n\treturn (base ** lx)\n",["floor x to the nearest lower decade ."]]
["def hash_user_password(user):\n\tpassword = user.get('password')\n\tif (password is None):\n\t\treturn user\n\treturn dict(user, password=hash_password(password))\n",["hash a user dicts password without modifying the passed-in dict ."]]
["def block_device_mapping_update(context, bdm_id, values, legacy=True):\n\treturn IMPL.block_device_mapping_update(context, bdm_id, values, legacy)\n",["update an entry of block device mapping ."]]
["def processElementNode(elementNode):\n\tpath.convertElementNode(elementNode, getGeometryOutput(None, elementNode))\n",["process the xml element ."]]
["@deprecated(u'1.0', message=DEPRECATION_MESSAGE)\ndef join(left, right, keys=None, join_type=u'inner', uniq_col_name=u'{col_name}_{table_name}', table_names=[u'1', u'2'], col_name_map=None):\n\t_col_name_map = col_name_map\n\tif (join_type not in (u'inner', u'outer', u'left', u'right')):\n\t\traise ValueError(u\"The\t'join_type'\targument\tshould\tbe\tin\t'inner',\t'outer',\t'left'\tor\t'right'\t(got\t'{0}'\tinstead)\".format(join_type))\n\tif (keys is None):\n\t\tkeys = tuple((name for name in left.dtype.names if (name in right.dtype.names)))\n\t\tif (len(keys) == 0):\n\t\t\traise TableMergeError(u'No\tkeys\tin\tcommon\tbetween\tleft\tand\tright\ttables')\n\telif isinstance(keys, six.string_types):\n\t\tkeys = (keys,)\n\tfor (arr, arr_label) in ((left, u'Left'), (right, u'Right')):\n\t\tfor name in keys:\n\t\t\tif (name not in arr.dtype.names):\n\t\t\t\traise TableMergeError(u'{0}\ttable\tdoes\tnot\thave\tkey\tcolumn\t{1!r}'.format(arr_label, name))\n\t\t\tif (hasattr(arr[name], u'mask') and np.any(arr[name].mask)):\n\t\t\t\traise TableMergeError(u'{0}\tkey\tcolumn\t{1!r}\thas\tmissing\tvalues'.format(arr_label, name))\n\tleft = left.ravel()\n\tright = right.ravel()\n\t(len_left, len_right) = (len(left), len(right))\n\t(left_names, right_names) = (left.dtype.names, right.dtype.names)\n\tcol_name_map = get_col_name_map([left, right], keys, uniq_col_name, table_names)\n\tout_descrs = get_descrs([left, right], col_name_map)\n\tout_keys_dtype = [descr for descr in out_descrs if (descr[0] in keys)]\n\tout_keys = np.empty((len_left + len_right), dtype=out_keys_dtype)\n\tfor key in keys:\n\t\tout_keys[key][:len_left] = left[key]\n\t\tout_keys[key][len_left:] = right[key]\n\tidx_sort = out_keys.argsort(order=keys)\n\tout_keys = out_keys[idx_sort]\n\tdiffs = np.concatenate(([True], (out_keys[1:] != out_keys[:(-1)]), [True]))\n\tidxs = np.flatnonzero(diffs)\n\tint_join_type = {u'inner': 0, u'outer': 1, u'left': 2, u'right': 3}[join_type]\n\t(masked, n_out, left_out, left_mask, right_out, right_mask) = _np_utils.join_inner(idxs, idx_sort, len_left, int_join_type)\n\tif any((isinstance(array, ma.MaskedArray) for array in (left, right))):\n\t\tmasked = True\n\tif masked:\n\t\tout = ma.empty(n_out, dtype=out_descrs)\n\telse:\n\t\tout = np.empty(n_out, dtype=out_descrs)\n\tif (len(left) == 0):\n\t\tleft = left.__class__(1, dtype=left.dtype)\n\tif (len(right) == 0):\n\t\tright = right.__class__(1, dtype=right.dtype)\n\tfor (out_name, left_right_names) in six.iteritems(col_name_map):\n\t\t(left_name, right_name) = left_right_names\n\t\tif (left_name and right_name):\n\t\t\tout[out_name] = np.where(right_mask, left[left_name].take(left_out), right[right_name].take(right_out))\n\t\t\tcontinue\n\t\telif left_name:\n\t\t\t(name, array, array_out, array_mask) = (left_name, left, left_out, left_mask)\n\t\telif right_name:\n\t\t\t(name, array, array_out, array_mask) = (right_name, right, right_out, right_mask)\n\t\telse:\n\t\t\traise TableMergeError(u'Unexpected\tcolumn\tnames\t(maybe\tone\tis\t\"\"?)')\n\t\tout[out_name] = array[name].take(array_out, axis=0)\n\t\tif masked:\n\t\t\tif isinstance(array, ma.MaskedArray):\n\t\t\t\tarray_mask = (array_mask | array[name].mask.take(array_out))\n\t\t\tout[out_name].mask = array_mask\n\tif isinstance(_col_name_map, collections.Mapping):\n\t\t_col_name_map.update(col_name_map)\n\treturn out\n",["produce an inner join between left and right clauses ."]]
["def localize(value):\n\treturn force_unicode(formats.localize(value, use_l10n=True))\n",["forces a value to be rendered as a localized value ."]]
["def _hmacedString(key, string):\n\thash = hmac.HMAC(key, digestmod=sha1)\n\tif isinstance(string, unicode):\n\t\tstring = string.encode('utf-8')\n\thash.update(string)\n\treturn hash.digest()\n",["return the sha-1 hmac hash of the given key and string ."]]
["def validate_int(s):\n\ttry:\n\t\treturn int(s)\n\texcept ValueError:\n\t\traise ValueError(('Could\tnot\tconvert\t\"%s\"\tto\tint' % s))\n",["convert s to int or raise ."]]
["def cosine_transform(f, x, k, **hints):\n\treturn CosineTransform(f, x, k).doit(**hints)\n",["compute the unitary ."]]
["def convert_StringListProperty(model, prop, kwargs):\n\treturn StringListPropertyField(**kwargs)\n",["returns a form field for a db ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load if boto is available ."]]
["def _api_resume_pp(name, output, kwargs):\n\tPostProcessor.do.paused = False\n\treturn report(output)\n",["api: accepts output ."]]
["def convert_comments(text):\n\treturn re.sub('(?<=\\\\n)\\\\s*#[^#]', '##', text)\n",["preprocess old style comments ."]]
["def get_current_instance_id():\n\treturn os.environ.get('INSTANCE_ID', None)\n",["returns the id of the current instance ."]]
["def getNewDerivation(elementNode):\n\treturn evaluate.EmptyObject(elementNode)\n",["get new derivation ."]]
["def iteritems(d):\n\treturn getattr(d, _iteritems)()\n",["return an iterator over the pairs of a dictionary ."]]
["@pytest.fixture\ndef template_name():\n\treturn 'cookiedozer_load'\n",["fixture to return a valid template_name ."]]
["def ensure_default_manager(sender, **kwargs):\n\tcls = sender\n\tif cls._meta.abstract:\n\t\treturn\n\tif (not getattr(cls, '_default_manager', None)):\n\t\ttry:\n\t\t\tcls._meta.get_field('objects')\n\t\t\traise ValueError((\"Model\t%s\tmust\tspecify\ta\tcustom\tManager,\tbecause\tit\thas\ta\tfield\tnamed\t'objects'\" % cls.__name__))\n\t\texcept FieldDoesNotExist:\n\t\t\tpass\n\t\tcls.add_to_class('objects', Manager())\n\t\tcls._base_manager = cls.objects\n\telif (not getattr(cls, '_base_manager', None)):\n\t\tdefault_mgr = cls._default_manager.__class__\n\t\tif ((default_mgr is Manager) or getattr(default_mgr, 'use_for_related_fields', False)):\n\t\t\tcls._base_manager = cls._default_manager\n\t\telse:\n\t\t\tfor base_class in default_mgr.mro()[1:]:\n\t\t\t\tif ((base_class is Manager) or getattr(base_class, 'use_for_related_fields', False)):\n\t\t\t\t\tcls.add_to_class('_base_manager', base_class())\n\t\t\t\t\treturn\n\t\t\traise AssertionError('Should\tnever\tget\there.\tPlease\treport\ta\tbug,\tincluding\tyour\tmodel\tand\tmodel\tmanager\tsetup.')\n",["ensures that a model subclass contains a default manager and sets the _default_manager attribute on the class ."]]
["def _downloadResult(result):\n\tresProvider = result.provider\n\tnewResult = False\n\tif (resProvider is None):\n\t\tlogger.log(u'Invalid\tprovider\tname\t-\tthis\tis\ta\tcoding\terror,\treport\tit\tplease', logger.ERROR)\n\t\treturn False\n\tif (result.resultType == 'nzb'):\n\t\tnewResult = resProvider.downloadResult(result)\n\telif (result.resultType == 'nzbdata'):\n\t\tfileName = ek.ek(os.path.join, sickbeard.NZB_DIR, (result.name + '.nzb'))\n\t\tlogger.log((u'Saving\tNZB\tto\t' + fileName))\n\t\tnewResult = True\n\t\ttry:\n\t\t\twith ek.ek(open, fileName, 'w') as fileOut:\n\t\t\t\tfileOut.write(result.extraInfo[0])\n\t\t\thelpers.chmodAsParent(fileName)\n\t\texcept EnvironmentError as e:\n\t\t\tlogger.log((u'Error\ttrying\tto\tsave\tNZB\tto\tblack\thole:\t' + ex(e)), logger.ERROR)\n\t\t\tnewResult = False\n\telif (resProvider.providerType == 'torrent'):\n\t\tnewResult = resProvider.downloadResult(result)\n\telse:\n\t\tlogger.log(u'Invalid\tprovider\ttype\t-\tthis\tis\ta\tcoding\terror,\treport\tit\tplease', logger.ERROR)\n\t\treturn False\n\treturn newResult\n",["downloads a result to the appropriate black hole folder ."]]
["def isIntersectingLoopsPaths(loops, paths, pointBegin, pointEnd):\n\tnormalizedSegment = (pointEnd.dropAxis() - pointBegin.dropAxis())\n\tnormalizedSegmentLength = abs(normalizedSegment)\n\tif (normalizedSegmentLength == 0.0):\n\t\treturn False\n\tnormalizedSegment \/= normalizedSegmentLength\n\tsegmentYMirror = complex(normalizedSegment.real, (- normalizedSegment.imag))\n\tpointBeginRotated = euclidean.getRoundZAxisByPlaneAngle(segmentYMirror, pointBegin)\n\tpointEndRotated = euclidean.getRoundZAxisByPlaneAngle(segmentYMirror, pointEnd)\n\tif euclidean.isLoopListIntersectingInsideXSegment(loops, pointBeginRotated.real, pointEndRotated.real, segmentYMirror, pointBeginRotated.imag):\n\t\treturn True\n\treturn euclidean.isXSegmentIntersectingPaths(paths, pointBeginRotated.real, pointEndRotated.real, segmentYMirror, pointBeginRotated.imag)\n",["determine if the segment between the first and second point is intersecting the loop list ."]]
["def _create_base_cipher(dict_parameters):\n\tuse_aesni = dict_parameters.pop('use_aesni', True)\n\ttry:\n\t\tkey = dict_parameters.pop('key')\n\texcept KeyError:\n\t\traise TypeError(\"Missing\t'key'\tparameter\")\n\texpect_byte_string(key)\n\tif (len(key) not in key_size):\n\t\traise ValueError(('Incorrect\tAES\tkey\tlength\t(%d\tbytes)' % len(key)))\n\tif (use_aesni and _raw_aesni_lib):\n\t\tstart_operation = _raw_aesni_lib.AESNI_start_operation\n\t\tstop_operation = _raw_aesni_lib.AESNI_stop_operation\n\telse:\n\t\tstart_operation = _raw_aes_lib.AES_start_operation\n\t\tstop_operation = _raw_aes_lib.AES_stop_operation\n\tcipher = VoidPointer()\n\tresult = start_operation(key, c_size_t(len(key)), cipher.address_of())\n\tif result:\n\t\traise ValueError(('Error\t%X\twhile\tinstantiating\tthe\tAES\tcipher' % result))\n\treturn SmartPointer(cipher.get(), stop_operation)\n",["this method instantiates and returns a handle to a low-level base cipher ."]]
["def processSVGElementellipse(svgReader, xmlElement):\n\tattributeDictionary = xmlElement.attributeDictionary\n\tcenter = euclidean.getComplexDefaultByDictionaryKeys(complex(), attributeDictionary, 'cx', 'cy')\n\tradius = euclidean.getComplexDefaultByDictionaryKeys(complex(), attributeDictionary, 'rx', 'ry')\n\tif ((radius.real == 0.0) or (radius.imag == 0.0)):\n\t\tprint 'Warning,\tin\tprocessSVGElementellipse\tin\tsvgReader\tradius\tis\tzero\tin:'\n\t\tprint attributeDictionary\n\t\treturn\n\tglobal globalNumberOfCirclePoints\n\tglobal globalSideAngle\n\tloop = []\n\trotatedLoopLayer = svgReader.getRotatedLoopLayer()\n\tfor side in xrange(globalNumberOfCirclePoints):\n\t\tunitPolar = euclidean.getWiddershinsUnitPolar((float(side) * globalSideAngle))\n\t\tloop.append((center + complex((unitPolar.real * radius.real), (unitPolar.imag * radius.imag))))\n\trotatedLoopLayer.loops += getTransformedFillOutline(loop, xmlElement, svgReader.yAxisPointingUpward)\n",["process elementnode by svgreader ."]]
["def dump(object_, file_, parameters=None, use_cpickle=False, protocol=DEFAULT_PROTOCOL, **kwargs):\n\tif use_cpickle:\n\t\tpickler = cPickle.Pickler\n\telse:\n\t\tpickler = _PicklerWithWarning\n\twith closing(tarfile.TarFile(fileobj=file_, mode='w')) as tar_file:\n\t\texternal_objects = {}\n\t\tdef _save_parameters(f):\n\t\t\trenamer = _Renamer()\n\t\t\tnamed_parameters = {renamer(p): p for p in parameters}\n\t\t\tnumpy.savez(f, **{n: p.get_value() for (n, p) in named_parameters.items()})\n\t\t\tfor (name, p) in named_parameters.items():\n\t\t\t\tarray_ = p.container.storage[0]\n\t\t\t\texternal_objects[id(array_)] = _mangle_parameter_name(p, name)\n\t\tif parameters:\n\t\t\t_taradd(_save_parameters, tar_file, '_parameters')\n\t\tif (object_ is not None):\n\t\t\tsave_object = _SaveObject(pickler, object_, external_objects, protocol, **kwargs)\n\t\t\t_taradd(save_object, tar_file, '_pkl')\n",["serialize obj as a json formatted stream to fp (a ."]]
["def _parse_date_perforce(aDateString):\n\t_my_date_pattern = re.compile(u'(\\\\w{,3}),\t(\\\\d{,4})\/(\\\\d{,2})\/(\\\\d{2})\t(\\\\d{,2}):(\\\\d{2}):(\\\\d{2})\t(\\\\w{,3})')\n\tm = _my_date_pattern.search(aDateString)\n\tif (m is None):\n\t\treturn None\n\t(dow, year, month, day, hour, minute, second, tz) = m.groups()\n\tmonths = [u'Jan', u'Feb', u'Mar', u'Apr', u'May', u'Jun', u'Jul', u'Aug', u'Sep', u'Oct', u'Nov', u'Dec']\n\tdateString = (u'%s,\t%s\t%s\t%s\t%s:%s:%s\t%s' % (dow, day, months[(int(month) - 1)], year, hour, minute, second, tz))\n\ttm = rfc822.parsedate_tz(dateString)\n\tif tm:\n\t\treturn time.gmtime(rfc822.mktime_tz(tm))\n",["parse a date in yyyy\/mm\/dd hh:mm:ss ttt format ."]]
["def dumps(obj, *args, **kwargs):\n\tjson_options = kwargs.pop('json_options', DEFAULT_JSON_OPTIONS)\n\treturn json.dumps(_json_convert(obj, json_options), *args, **kwargs)\n",["serialize obj to a json formatted str ."]]
["def getRadioPluginsAddPluginGroupFrame(directoryPath, importantFileNames, names, repository):\n\trepository.pluginGroupFrame = settings.PluginGroupFrame()\n\tradioPlugins = []\n\tfor name in names:\n\t\tradioPlugin = settings.RadioPlugin().getFromRadio((name in importantFileNames), repository.pluginGroupFrame.latentStringVar, name, repository, (name == importantFileNames[0]))\n\t\tradioPlugin.updateFunction = repository.pluginGroupFrame.update\n\t\tradioPlugins.append(radioPlugin)\n\tdefaultRadioButton = settings.getSelectedRadioPlugin((importantFileNames + [radioPlugins[0].name]), radioPlugins)\n\trepository.pluginGroupFrame.getFromPath(defaultRadioButton, directoryPath, repository)\n\treturn radioPlugins\n",["get the radio plugins and add the plugin frame ."]]
["def dedent(text):\n\tmargin = None\n\ttext = _whitespace_only_re.sub('', text)\n\tindents = _leading_whitespace_re.findall(text)\n\tfor indent in indents:\n\t\tif (margin is None):\n\t\t\tmargin = indent\n\t\telif indent.startswith(margin):\n\t\t\tpass\n\t\telif margin.startswith(indent):\n\t\t\tmargin = indent\n\t\telse:\n\t\t\tmargin = ''\n\t\t\tbreak\n\tif (0 and margin):\n\t\tfor line in text.split('\\n'):\n\t\t\tassert ((not line) or line.startswith(margin)), ('line\t=\t%r,\tmargin\t=\t%r' % (line, margin))\n\tif margin:\n\t\ttext = re.sub(('(?m)^' + margin), '', text)\n\treturn text\n",["remove leading indent from a block of text ."]]
["def generate_jwt():\n\tnow = int(time.time())\n\theader_json = json.dumps({'typ': 'JWT', 'alg': 'RS256'})\n\tpayload_json = json.dumps({'iat': now, 'exp': (now + 3600), 'iss': DEFAUTL_SERVICE_ACCOUNT, 'scope': TARGET_AUD, 'aud': 'https:\/\/www.googleapis.com\/oauth2\/v4\/token'})\n\theaderAndPayload = '{}.{}'.format(base64.urlsafe_b64encode(header_json), base64.urlsafe_b64encode(payload_json))\n\t(key_name, signature) = app_identity.sign_blob(headerAndPayload)\n\tsigned_jwt = '{}.{}'.format(headerAndPayload, base64.urlsafe_b64encode(signature))\n\treturn signed_jwt\n",["generates a signed json web token using the google app engine default service account ."]]
["def localize(value):\n\treturn force_unicode(formats.localize(value, use_l10n=True))\n",["checks if value is a localizable type and returns it formatted as a string using current locale format ."]]
["def get_exploration_metadata_dicts(exploration_ids, editor_user_id=None):\n\texploration_summaries = exp_services.get_exploration_summaries_matching_ids(exploration_ids)\n\tfiltered_exploration_summaries = []\n\tfor exploration_summary in exploration_summaries:\n\t\tif (exploration_summary is None):\n\t\t\tcontinue\n\t\tif (exploration_summary.status == rights_manager.ACTIVITY_STATUS_PRIVATE):\n\t\t\tif (editor_user_id is None):\n\t\t\t\tcontinue\n\t\t\tif (not rights_manager.Actor(editor_user_id).can_edit(feconf.ACTIVITY_TYPE_EXPLORATION, exploration_summary.id)):\n\t\t\t\tcontinue\n\t\tfiltered_exploration_summaries.append(exploration_summary)\n\treturn [summary.to_metadata_dict() for summary in filtered_exploration_summaries]\n",["given a list of exploration ids ."]]
["def get_declared_fields(bases, attrs, with_base_fields=True):\n\tfields = [(field_name, attrs.pop(field_name)) for (field_name, obj) in list(six.iteritems(attrs)) if isinstance(obj, Field)]\n\tfields.sort(key=(lambda x: x[1].creation_counter))\n\tif with_base_fields:\n\t\tfor base in bases[::(-1)]:\n\t\t\tif hasattr(base, u'base_fields'):\n\t\t\t\tfields = (list(six.iteritems(base.base_fields)) + fields)\n\telse:\n\t\tfor base in bases[::(-1)]:\n\t\t\tif hasattr(base, u'declared_fields'):\n\t\t\t\tfields = (list(six.iteritems(base.declared_fields)) + fields)\n\treturn SortedDict(fields)\n",["create a list of form field instances from the passed in attrs ."]]
["def determine_format(request, serializer, default_format=u'application\/json'):\n\tformat = request.GET.get(u'format')\n\tif format:\n\t\tif (format in serializer.formats):\n\t\t\treturn serializer.get_mime_for_format(format)\n\tif ((u'callback' in request.GET) and (u'jsonp' in serializer.formats)):\n\t\treturn serializer.get_mime_for_format(u'jsonp')\n\taccept = request.META.get(u'HTTP_ACCEPT', u'*\/*')\n\tif (accept != u'*\/*'):\n\t\ttry:\n\t\t\tbest_format = mimeparse.best_match(serializer.supported_formats_reversed, accept)\n\t\texcept ValueError:\n\t\t\traise BadRequest(u'Invalid\tAccept\theader')\n\t\tif best_format:\n\t\t\treturn best_format\n\treturn default_format\n",["tries to \"smartly\" determine which output format is desired ."]]
["def sanitize_open(filename, open_mode):\n\ttry:\n\t\tif (filename == u'-'):\n\t\t\tif (sys.platform == u'win32'):\n\t\t\t\timport msvcrt\n\t\t\t\tmsvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)\n\t\t\treturn ((sys.stdout.buffer if hasattr(sys.stdout, u'buffer') else sys.stdout), filename)\n\t\tstream = open(encodeFilename(filename), open_mode)\n\t\treturn (stream, filename)\n\texcept (IOError, OSError) as err:\n\t\tif (err.errno in (errno.EACCES,)):\n\t\t\traise\n\t\talt_filename = sanitize_path(filename)\n\t\tif (alt_filename == filename):\n\t\t\traise\n\t\telse:\n\t\t\tstream = open(encodeFilename(alt_filename), open_mode)\n\t\t\treturn (stream, alt_filename)\n",["try to open the given filename ."]]
["def _run_quiet(cmd, cwd=None, stdin=None, runas=None, shell=DEFAULT_SHELL, python_shell=False, env=None, template=None, umask=None, timeout=None, reset_system_locale=True, saltenv='base', pillarenv=None, pillar_override=None):\n\treturn _run(cmd, runas=runas, cwd=cwd, stdin=stdin, stderr=subprocess.STDOUT, output_loglevel='quiet', log_callback=None, shell=shell, python_shell=python_shell, env=env, template=template, umask=umask, timeout=timeout, reset_system_locale=reset_system_locale, saltenv=saltenv, pillarenv=pillarenv, pillar_override=pillar_override)['stdout']\n",["helper for running commands quietly for minion startup ."]]
["def in6_ctop(addr):\n\tif ((len(addr) != 20) or (not reduce((lambda x, y: (x and y)), map((lambda x: (x in _rfc1924map)), addr)))):\n\t\treturn None\n\ti = 0\n\tfor c in addr:\n\t\tj = _rfc1924map.index(c)\n\t\ti = ((85 * i) + j)\n\tres = []\n\tfor j in xrange(4):\n\t\tres.append(struct.pack('!I', (i % (2 ** 32))))\n\t\ti = (i \/ (2 ** 32))\n\tres.reverse()\n\treturn inet_ntop(socket.AF_INET6, ''.join(res))\n",["convert an ipv6 address in compact representation notation to printable representation ;-) returns none on error ."]]
["def preserve_value(namespace, name):\n\tdef decorator(func):\n\t\tdef resetter_attr(saved_value_internal):\n\t\t\treturn setattr(namespace, name, saved_value_internal)\n\t\tdef resetter_no_attr(saved_value_internal):\n\t\t\tdel saved_value_internal\n\t\t\treturn delattr(namespace, name)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tsaved_value = None\n\t\t\ttry:\n\t\t\t\tsaved_value = getattr(namespace, name)\n\t\t\t\tresetter = resetter_attr\n\t\t\texcept AttributeError:\n\t\t\t\tresetter = resetter_no_attr\n\t\t\ttry:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\tfinally:\n\t\t\t\tresetter(saved_value)\n\t\twrapper.__name__ = func.__name__\n\t\twrapper.__doc__ = func.__doc__\n\t\treturn wrapper\n\treturn decorator\n",["function decorator to wrap a function that sets a namespace item ."]]
["def ValidFilename(filename):\n\tif (_file_path_positive_re.match(filename) is None):\n\t\treturn ('Invalid\tcharacter\tin\tfilename:\t%s' % filename)\n\tif (_file_path_negative_1_re.search(filename) is not None):\n\t\treturn ('Filename\tcannot\tcontain\t\".\"\tor\t\"..\"\tor\tstart\twith\t\"-\"\tor\t\"_ah\/\":\t%s' % filename)\n\tif (_file_path_negative_2_re.search(filename) is not None):\n\t\treturn ('Filename\tcannot\thave\ttrailing\t\/\tor\tcontain\t\/\/:\t%s' % filename)\n\tif (_file_path_negative_3_re.search(filename) is not None):\n\t\treturn ('Any\tspaces\tmust\tbe\tin\tthe\tmiddle\tof\ta\tfilename:\t%s' % filename)\n\treturn ''\n",["determines if filename is valid ."]]
["def _setlabel(object_alias, index):\n\tfinder = _getfinder()\n\targs = {}\n\tattrs = {}\n\t_code = 'core'\n\t_subcode = 'setd'\n\taeobj_0 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='alis', seld=object_alias, fr=None)\n\taeobj_1 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('labi'), fr=aeobj_0)\n\targs['----'] = aeobj_1\n\targs['data'] = index\n\t(_reply, args, attrs) = finder.send(_code, _subcode, args, attrs)\n\tif ('errn' in args):\n\t\traise Error, aetools.decodeerror(args)\n\treturn index\n",["label: set the label for the object ."]]
["def copy_image(module, ec2):\n\tsource_region = module.params.get('source_region')\n\tsource_image_id = module.params.get('source_image_id')\n\tname = module.params.get('name')\n\tdescription = module.params.get('description')\n\tencrypted = module.params.get('encrypted')\n\tkms_key_id = module.params.get('kms_key_id')\n\ttags = module.params.get('tags')\n\twait_timeout = int(module.params.get('wait_timeout'))\n\twait = module.params.get('wait')\n\ttry:\n\t\tparams = {'source_region': source_region, 'source_image_id': source_image_id, 'name': name, 'description': description, 'encrypted': encrypted, 'kms_key_id': kms_key_id}\n\t\timage_id = ec2.copy_image(**params).image_id\n\texcept boto.exception.BotoServerError as e:\n\t\tmodule.fail_json(msg=('%s:\t%s' % (e.error_code, e.error_message)))\n\timg = wait_until_image_is_recognized(module, ec2, wait_timeout, image_id, wait)\n\timg = wait_until_image_is_copied(module, ec2, wait_timeout, img, image_id, wait)\n\tregister_tags_if_any(module, ec2, tags, image_id)\n\tmodule.exit_json(msg='AMI\tcopy\toperation\tcomplete', image_id=image_id, state=img.state, changed=True)\n",["copy a disk image to an existing directory ."]]
["def _run_file(file_path, globals_, script_dir=_SCRIPT_DIR):\n\tscript_name = os.path.basename(file_path)\n\tsys.path = (_SYS_PATH_ADDITIONS[script_name] + sys.path)\n\tif ('google' in sys.modules):\n\t\tdel sys.modules['google']\n\tscript_dir = _SCRIPT_TO_DIR.get(script_name, script_dir)\n\tscript_name = _BOOTSTAP_NAME_TO_REAL_NAME.get(script_name, script_name)\n\tscript_path = os.path.join(script_dir, script_name)\n\texecfile(script_path, globals_)\n\texit(0)\n",["execute the file at the specified path with the passed-in globals ."]]
["def get_metrics():\n\tglobal METRICS, LAST_METRICS\n\tif ((time.time() - METRICS['time']) > METRICS_CACHE_TTL):\n\t\tmetrics = {}\n\t\tfor status_type in PARAMS.keys():\n\t\t\tio = os.popen(PARAMS[status_type])\n\t\t\tmetrics_str = ''.join(io.readlines()).strip()\n\t\t\tmetrics_str = re.sub('\\\\w+\\\\((.*)\\\\)', '\\\\1', metrics_str)\n\t\t\ttry:\n\t\t\t\tif (status_type == 'server_status'):\n\t\t\t\t\tmetrics.update(flatten(json.loads(metrics_str)))\n\t\t\t\telse:\n\t\t\t\t\tmetrics.update(flatten(json.loads(metrics_str), pre=('%s_' % status_type)))\n\t\t\texcept ValueError:\n\t\t\t\tmetrics = {}\n\t\tLAST_METRICS = copy.deepcopy(METRICS)\n\t\tMETRICS = {'time': time.time(), 'data': metrics}\n\treturn [METRICS, LAST_METRICS]\n",["return all metrics ."]]
["def import_module(module_path):\n\ttry:\n\t\tmod = importlib.import_module(module_path)\n\t\treturn mod\n\texcept:\n\t\traise ImportError('Error\twhen\timporting\tobject\t{}.'.format(module_path))\n",["imports and returns a module ."]]
["def _mountpoint_to_number(mountpoint):\n\tif mountpoint.startswith('\/dev\/'):\n\t\tmountpoint = mountpoint[5:]\n\tif re.match('^[hs]d[a-p]$', mountpoint):\n\t\treturn (ord(mountpoint[2:3]) - ord('a'))\n\telif re.match('^x?vd[a-p]$', mountpoint):\n\t\treturn (ord(mountpoint[(-1)]) - ord('a'))\n\telif re.match('^[0-9]+$', mountpoint):\n\t\treturn int(mountpoint, 10)\n\telse:\n\t\tLOG.warning(_LW('Mountpoint\tcannot\tbe\ttranslated:\t%s'), mountpoint)\n\t\treturn (-1)\n",["translate a mountpoint like \/dev\/sdc into a numeric ."]]
["@register.filter(is_safe=False)\ndef yesno(value, arg=None):\n\tif (arg is None):\n\t\targ = ugettext(u'yes,no,maybe')\n\tbits = arg.split(u',')\n\tif (len(bits) < 2):\n\t\treturn value\n\ttry:\n\t\t(yes, no, maybe) = bits\n\texcept ValueError:\n\t\t(yes, no, maybe) = (bits[0], bits[1], bits[1])\n\tif (value is None):\n\t\treturn maybe\n\tif value:\n\t\treturn yes\n\treturn no\n",["given a string mapping values for true ."]]
["def a2dismod(mod):\n\tret = {}\n\tcommand = ['a2dismod', mod]\n\ttry:\n\t\tstatus = __salt__['cmd.retcode'](command, python_shell=False)\n\texcept Exception as e:\n\t\treturn e\n\tret['Name'] = 'Apache2\tDisable\tMod'\n\tret['Mod'] = mod\n\tif (status == 256):\n\t\tret['Status'] = 'Mod\t{0}\tNot\tfound'.format(mod)\n\telif (status == 0):\n\t\tret['Status'] = 'Mod\t{0}\tdisabled'.format(mod)\n\telse:\n\t\tret['Status'] = status\n\treturn ret\n",["runs a2dismod for the given mod ."]]
["def get_cli_body_ssh(command, response, module):\n\tif ('^' == response[0]):\n\t\tbody = []\n\telif ('running' in command):\n\t\tbody = response\n\telse:\n\t\tif (command in response[0]):\n\t\t\tresponse = [response[0].split(command)[1]]\n\t\ttry:\n\t\t\tbody = [json.loads(response[0])]\n\t\texcept ValueError:\n\t\t\tmodule.fail_json(msg='Command\tdoes\tnot\tsupport\tJSON\toutput', command=command)\n\treturn body\n",["get response for when transport=cli ."]]
["@checker('.rst', severity=2)\ndef check_suspicious_constructs(fn, lines):\n\tinprod = False\n\tfor (lno, line) in enumerate(lines):\n\t\tif seems_directive_re.match(line):\n\t\t\t(yield ((lno + 1), 'comment\tseems\tto\tbe\tintended\tas\ta\tdirective'))\n\t\tif ('..\tproductionlist::' in line):\n\t\t\tinprod = True\n\t\telif ((not inprod) and default_role_re.search(line)):\n\t\t\t(yield ((lno + 1), 'default\trole\tused'))\n\t\telif (inprod and (not line.strip())):\n\t\t\tinprod = False\n",["check for suspicious rest constructs ."]]
["def clientFromString(reactor, description):\n\t(args, kwargs) = _parse(description)\n\taname = args.pop(0)\n\tname = aname.upper()\n\tif (name not in _clientParsers):\n\t\tplugin = _matchPluginToPrefix(getPlugins(IStreamClientEndpointStringParserWithReactor), name)\n\t\treturn plugin.parseStreamClient(reactor, *args, **kwargs)\n\tkwargs = _clientParsers[name](*args, **kwargs)\n\treturn _endpointClientFactories[name](reactor, **kwargs)\n",["construct a client endpoint from a description string ."]]
["def execute_return_success(cmd):\n\tret = _run_all(cmd)\n\tif ((ret['retcode'] != 0) or ('not\tsupported' in ret['stdout'].lower())):\n\t\tmsg = 'Command\tFailed:\t{0}\\n'.format(cmd)\n\t\tmsg += 'Return\tCode:\t{0}\\n'.format(ret['retcode'])\n\t\tmsg += 'Output:\t{0}\\n'.format(ret['stdout'])\n\t\tmsg += 'Error:\t{0}\\n'.format(ret['stderr'])\n\t\traise CommandExecutionError(msg)\n\treturn True\n",["executes the passed command ."]]
["def move_in_stack(move_up):\n\tframe = Frame.get_selected_python_frame()\n\twhile frame:\n\t\tif move_up:\n\t\t\titer_frame = frame.older()\n\t\telse:\n\t\t\titer_frame = frame.newer()\n\t\tif (not iter_frame):\n\t\t\tbreak\n\t\tif iter_frame.is_python_frame():\n\t\t\tif iter_frame.select():\n\t\t\t\titer_frame.print_summary()\n\t\t\treturn\n\t\tframe = iter_frame\n\tif move_up:\n\t\tprint('Unable\tto\tfind\tan\tolder\tpython\tframe')\n\telse:\n\t\tprint('Unable\tto\tfind\ta\tnewer\tpython\tframe')\n",["move up or down the stack ."]]
["def aliased(element, alias=None, name=None, flat=False, adapt_on_names=False):\n\tif isinstance(element, expression.FromClause):\n\t\tif adapt_on_names:\n\t\t\traise sa_exc.ArgumentError('adapt_on_names\tonly\tapplies\tto\tORM\telements')\n\t\treturn element.alias(name, flat=flat)\n\telse:\n\t\treturn AliasedClass(element, alias=alias, flat=flat, name=name, adapt_on_names=adapt_on_names)\n",["produce an alias of the given element ."]]
["def test_io_layout_lay():\n\ttempdir = _TempDir()\n\tlayout = read_layout('CTF151', scale=False)\n\tlayout.save(op.join(tempdir, 'foobar.lay'))\n\tlayout_read = read_layout(op.join(tempdir, 'foobar.lay'), path='.\/', scale=False)\n\tassert_array_almost_equal(layout.pos, layout_read.pos, decimal=2)\n\tassert_true(layout.names, layout_read.names)\n",["test io with ."]]
["def getnode():\n\tglobal _node\n\tif (_node is not None):\n\t\treturn _node\n\timport sys\n\tif (sys.platform == 'win32'):\n\t\tgetters = [_windll_getnode, _netbios_getnode, _ipconfig_getnode]\n\telse:\n\t\tgetters = [_unixdll_getnode, _ifconfig_getnode]\n\tfor getter in (getters + [_random_getnode]):\n\t\ttry:\n\t\t\t_node = getter()\n\t\texcept:\n\t\t\tcontinue\n\t\tif (_node is not None):\n\t\t\treturn _node\n",["get the hardware address as a 48-bit positive integer ."]]
["def getCraftedTextFromText(gcodeText, exportRepository=None):\n\tif gcodec.isProcedureDoneOrFileIsEmpty(gcodeText, 'export'):\n\t\treturn gcodeText\n\tif (exportRepository == None):\n\t\texportRepository = settings.getReadRepository(ExportRepository())\n\tif (not exportRepository.activateExport.value):\n\t\treturn gcodeText\n\treturn ExportSkein().getCraftedGcode(exportRepository, gcodeText)\n",["cool a gcode linear move text ."]]
["def getCraftedTextFromText(gcodeText, exportRepository=None):\n\tif gcodec.isProcedureDoneOrFileIsEmpty(gcodeText, 'export'):\n\t\treturn gcodeText\n\tif (exportRepository == None):\n\t\texportRepository = settings.getReadRepository(ExportRepository())\n\tif (not exportRepository.activateExport.value):\n\t\treturn gcodeText\n\treturn ExportSkein().getCraftedGcode(exportRepository, gcodeText)\n",["cool a gcode linear move text ."]]
["def detachRequest(GmmCause_presence=0):\n\ta = TpPd(pd=3)\n\tb = MessageType(mesType=5)\n\tc = DetachTypeAndForceToStandby()\n\tpacket = ((a \/ b) \/ c)\n\tif (GmmCause_presence is 1):\n\t\te = GmmCause(ieiGC=37)\n\t\tpacket = (packet \/ e)\n\treturn packet\n",["detach request section 9 ."]]
["def iterateInReactor(i):\n\tfrom twisted.internet import reactor\n\td = defer.Deferred()\n\tdef go(last):\n\t\ttry:\n\t\t\tr = next(i)\n\t\texcept StopIteration:\n\t\t\td.callback(last)\n\t\texcept:\n\t\t\td.errback()\n\t\telse:\n\t\t\tif isinstance(r, defer.Deferred):\n\t\t\t\tr.addCallback(go)\n\t\t\telse:\n\t\t\t\treactor.callLater(0, go, r)\n\tgo(None)\n\treturn d\n",["consume an interator at most a single iteration per reactor iteration ."]]
["def get_makefile_filename():\n\tif python_build:\n\t\treturn os.path.join(project_base, 'Makefile')\n\tlib_dir = get_python_lib(plat_specific=1, standard_lib=1)\n\treturn os.path.join(lib_dir, 'config', 'Makefile')\n",["return full pathname of installed makefile from the python build ."]]
["def _getopt_flags(options):\n\ts = []\n\tl = []\n\tfor o in options:\n\t\tif (o.prefix == '-'):\n\t\t\ts.append(o.name)\n\t\t\tif o.takes_argument:\n\t\t\t\ts.append(':')\n\t\telif o.takes_argument:\n\t\t\tl.append((o.name + '='))\n\t\telse:\n\t\t\tl.append(o.name)\n\treturn (''.join(s), l)\n",["convert the option list to a getopt flag string and long opt list ."]]
["def test_make_imbalance_4():\n\t(X_, y_) = make_imbalance(X, Y, ratio=0.01, min_c_=1)\n\tcounter = Counter(y_)\n\tassert_equal(counter[0], 500)\n\tassert_equal(counter[1], 5)\n\tassert_true(np.all([(X_i in X) for X_i in X_]))\n",["test make_imbalance ."]]
["def split_path(f):\n\tcomponents = []\n\t(drive, path) = os.path.splitdrive(f)\n\twhile True:\n\t\t(path, folder) = os.path.split(path)\n\t\tif folder:\n\t\t\tcomponents.append(folder)\n\t\telse:\n\t\t\tif path:\n\t\t\t\tcomponents.append(path)\n\t\t\tbreak\n\tif drive:\n\t\tcomponents.append(drive)\n\tcomponents.reverse()\n\treturn components\n",["split the requested path into ."]]
["def _getAccessibleAttribute(attributeName, elementNode):\n\tif (attributeName in globalGetAccessibleAttributeSet):\n\t\treturn getattr(Document(elementNode), attributeName, None)\n\treturn None\n",["get the accessible attribute ."]]
["def cookie_decode(data, key, digestmod=None):\n\tdepr(0, 13, 'cookie_decode()\twill\tbe\tremoved\tsoon.', 'Do\tnot\tuse\tthis\tAPI\tdirectly.')\n\tdata = tob(data)\n\tif cookie_is_encoded(data):\n\t\t(sig, msg) = data.split(tob('?'), 1)\n\t\tdigestmod = (digestmod or hashlib.sha256)\n\t\thashed = hmac.new(tob(key), msg, digestmod=digestmod).digest()\n\t\tif _lscmp(sig[1:], base64.b64encode(hashed)):\n\t\t\treturn pickle.loads(base64.b64decode(msg))\n\treturn None\n",["verify and decode an encoded string ."]]
["def inputhook_wx1(context):\n\ttry:\n\t\tapp = wx.GetApp()\n\t\tif (app is not None):\n\t\t\tassert wx.Thread_IsMain()\n\t\t\tevtloop = wx.EventLoop()\n\t\t\tea = wx.EventLoopActivator(evtloop)\n\t\t\twhile evtloop.Pending():\n\t\t\t\tevtloop.Dispatch()\n\t\t\tapp.ProcessIdle()\n\t\t\tdel ea\n\texcept KeyboardInterrupt:\n\t\tpass\n\treturn 0\n",["run the wx event loop by processing pending events only ."]]
["def setup_args(config_files=[]):\n\tglobal args\n\targlist = sys.argv[1:]\n\tfor config_file in filter(os.path.isfile, config_files):\n\t\targlist.insert(0, ('@' + config_file))\n\targs = parser.parse_args(arglist)\n\tif args.stream:\n\t\targs.stream = [stream.lower() for stream in args.stream]\n",["adds additional args to allow the vm uuid to be set ."]]
["@deprecated(u'1.0', message=DEPRECATION_MESSAGE)\ndef join(left, right, keys=None, join_type=u'inner', uniq_col_name=u'{col_name}_{table_name}', table_names=[u'1', u'2'], col_name_map=None):\n\t_col_name_map = col_name_map\n\tif (join_type not in (u'inner', u'outer', u'left', u'right')):\n\t\traise ValueError(u\"The\t'join_type'\targument\tshould\tbe\tin\t'inner',\t'outer',\t'left'\tor\t'right'\t(got\t'{0}'\tinstead)\".format(join_type))\n\tif (keys is None):\n\t\tkeys = tuple((name for name in left.dtype.names if (name in right.dtype.names)))\n\t\tif (len(keys) == 0):\n\t\t\traise TableMergeError(u'No\tkeys\tin\tcommon\tbetween\tleft\tand\tright\ttables')\n\telif isinstance(keys, six.string_types):\n\t\tkeys = (keys,)\n\tfor (arr, arr_label) in ((left, u'Left'), (right, u'Right')):\n\t\tfor name in keys:\n\t\t\tif (name not in arr.dtype.names):\n\t\t\t\traise TableMergeError(u'{0}\ttable\tdoes\tnot\thave\tkey\tcolumn\t{1!r}'.format(arr_label, name))\n\t\t\tif (hasattr(arr[name], u'mask') and np.any(arr[name].mask)):\n\t\t\t\traise TableMergeError(u'{0}\tkey\tcolumn\t{1!r}\thas\tmissing\tvalues'.format(arr_label, name))\n\tleft = left.ravel()\n\tright = right.ravel()\n\t(len_left, len_right) = (len(left), len(right))\n\t(left_names, right_names) = (left.dtype.names, right.dtype.names)\n\tcol_name_map = get_col_name_map([left, right], keys, uniq_col_name, table_names)\n\tout_descrs = get_descrs([left, right], col_name_map)\n\tout_keys_dtype = [descr for descr in out_descrs if (descr[0] in keys)]\n\tout_keys = np.empty((len_left + len_right), dtype=out_keys_dtype)\n\tfor key in keys:\n\t\tout_keys[key][:len_left] = left[key]\n\t\tout_keys[key][len_left:] = right[key]\n\tidx_sort = out_keys.argsort(order=keys)\n\tout_keys = out_keys[idx_sort]\n\tdiffs = np.concatenate(([True], (out_keys[1:] != out_keys[:(-1)]), [True]))\n\tidxs = np.flatnonzero(diffs)\n\tint_join_type = {u'inner': 0, u'outer': 1, u'left': 2, u'right': 3}[join_type]\n\t(masked, n_out, left_out, left_mask, right_out, right_mask) = _np_utils.join_inner(idxs, idx_sort, len_left, int_join_type)\n\tif any((isinstance(array, ma.MaskedArray) for array in (left, right))):\n\t\tmasked = True\n\tif masked:\n\t\tout = ma.empty(n_out, dtype=out_descrs)\n\telse:\n\t\tout = np.empty(n_out, dtype=out_descrs)\n\tif (len(left) == 0):\n\t\tleft = left.__class__(1, dtype=left.dtype)\n\tif (len(right) == 0):\n\t\tright = right.__class__(1, dtype=right.dtype)\n\tfor (out_name, left_right_names) in six.iteritems(col_name_map):\n\t\t(left_name, right_name) = left_right_names\n\t\tif (left_name and right_name):\n\t\t\tout[out_name] = np.where(right_mask, left[left_name].take(left_out), right[right_name].take(right_out))\n\t\t\tcontinue\n\t\telif left_name:\n\t\t\t(name, array, array_out, array_mask) = (left_name, left, left_out, left_mask)\n\t\telif right_name:\n\t\t\t(name, array, array_out, array_mask) = (right_name, right, right_out, right_mask)\n\t\telse:\n\t\t\traise TableMergeError(u'Unexpected\tcolumn\tnames\t(maybe\tone\tis\t\"\"?)')\n\t\tout[out_name] = array[name].take(array_out, axis=0)\n\t\tif masked:\n\t\t\tif isinstance(array, ma.MaskedArray):\n\t\t\t\tarray_mask = (array_mask | array[name].mask.take(array_out))\n\t\t\tout[out_name].mask = array_mask\n\tif isinstance(_col_name_map, collections.Mapping):\n\t\t_col_name_map.update(col_name_map)\n\treturn out\n",["joins a list with a string ."]]
["def revoke_cert(project_id, file_name):\n\ttry:\n\t\tutils.execute('openssl', 'ca', '-config', '.\/openssl.cnf', '-revoke', file_name, cwd=ca_folder(project_id))\n\t\tutils.execute('openssl', 'ca', '-gencrl', '-config', '.\/openssl.cnf', '-out', CONF.crypto.crl_file, cwd=ca_folder(project_id))\n\texcept OSError:\n\t\traise exception.ProjectNotFound(project_id=project_id)\n\texcept processutils.ProcessExecutionError:\n\t\traise exception.RevokeCertFailure(project_id=project_id)\n",["revoke a cert by file name ."]]
["def getCarving(fileName):\n\tpluginModule = fabmetheus_interpret.getInterpretPlugin(fileName)\n\tif (pluginModule == None):\n\t\treturn None\n\treturn pluginModule.getCarving(fileName)\n",["get the triangle mesh for the obj file ."]]
["def _normalize_module(module, depth=2):\n\tif inspect.ismodule(module):\n\t\treturn module\n\telif isinstance(module, (str, unicode)):\n\t\treturn __import__(module, globals(), locals(), ['*'])\n\telif (module is None):\n\t\treturn sys.modules[sys._getframe(depth).f_globals['__name__']]\n\telse:\n\t\traise TypeError('Expected\ta\tmodule,\tstring,\tor\tNone')\n",["return the module specified by module ."]]
["def convert_password(context, password):\n\tpassword = (password or '')\n\tmeta = {}\n\tfor i in xrange(CHUNKS):\n\t\tmeta[('password_%d' % i)] = password[:CHUNK_LENGTH]\n\t\tpassword = password[CHUNK_LENGTH:]\n\treturn meta\n",["stores password as system_metadata items ."]]
["def get_metrics():\n\tglobal METRICS, LAST_METRICS\n\tif ((time.time() - METRICS['time']) > METRICS_CACHE_TTL):\n\t\tmetrics = {}\n\t\tfor status_type in PARAMS.keys():\n\t\t\tio = os.popen(PARAMS[status_type])\n\t\t\tmetrics_str = ''.join(io.readlines()).strip()\n\t\t\tmetrics_str = re.sub('\\\\w+\\\\((.*)\\\\)', '\\\\1', metrics_str)\n\t\t\ttry:\n\t\t\t\tif (status_type == 'server_status'):\n\t\t\t\t\tmetrics.update(flatten(json.loads(metrics_str)))\n\t\t\t\telse:\n\t\t\t\t\tmetrics.update(flatten(json.loads(metrics_str), pre=('%s_' % status_type)))\n\t\t\texcept ValueError:\n\t\t\t\tmetrics = {}\n\t\tLAST_METRICS = copy.deepcopy(METRICS)\n\t\tMETRICS = {'time': time.time(), 'data': metrics}\n\treturn [METRICS, LAST_METRICS]\n",["return all metrics ."]]
["def fetch_stream_from_url(url, config, data=None, handlers=None):\n\t(return_code, return_message, response) = open_url(url, config, data=data, handlers=handlers)\n\tif (return_code and (return_code == httplib.OK)):\n\t\treturn response\n\telse:\n\t\traise URLFetchError(return_message)\n",["returns data retrieved from a url ."]]
["def delete_comment(request, comment_id):\n\t(cc_comment, context) = _get_comment_and_context(request, comment_id)\n\tif can_delete(cc_comment, context):\n\t\tcc_comment.delete()\n\t\tcomment_deleted.send(sender=None, user=request.user, post=cc_comment)\n\telse:\n\t\traise PermissionDenied\n",["delete a comment ."]]
["def test_sample_wrong_X():\n\tada = ADASYN(random_state=RND_SEED)\n\tada.fit(X, Y)\n\tassert_raises(RuntimeError, ada.sample, np.random.random((100, 40)), np.array((([0] * 50) + ([1] * 50))))\n",["test either if an error is raised when x is different at fitting and sampling ."]]
["def word_count(documents):\n\tcollector = defaultdict(list)\n\tfor document in documents:\n\t\tfor (word, count) in wc_mapper(document):\n\t\t\tcollector[word].append(count)\n\treturn [output for (word, counts) in collector.items() for output in wc_reducer(word, counts)]\n",["count the words in the input documents using mapreduce ."]]
["def authenticationRequest():\n\ta = TpPd(pd=5)\n\tb = MessageType(mesType=18)\n\tc = CiphKeySeqNrAndSpareHalfOctets()\n\td = AuthenticationParameterRAND()\n\tpacket = (((a \/ b) \/ c) \/ d)\n\treturn packet\n",["authentication request section 9 ."]]
["def get_stylesheet_reference(settings, relative_to=None):\n\tif settings.stylesheet_path:\n\t\tassert (not settings.stylesheet), 'stylesheet\tand\tstylesheet_path\tare\tmutually\texclusive.'\n\t\tif (relative_to == None):\n\t\t\trelative_to = settings._destination\n\t\treturn relative_path(relative_to, settings.stylesheet_path)\n\telse:\n\t\treturn settings.stylesheet\n",["retrieve a stylesheet reference from the settings object ."]]
["def stop(name):\n\tcmd = '\/etc\/rc.d\/{0}\t-f\tstop'.format(name)\n\treturn (not __salt__['cmd.retcode'](cmd))\n",["stop the specified service cli example: ."]]
["@register.inclusion_tag(get_template('inclusion.html'))\ndef inclusion_only_unlimited_args_from_template(*args):\n\treturn {'result': ('inclusion_only_unlimited_args_from_template\t-\tExpected\tresult:\t%s' % ',\t'.join([six.text_type(arg) for arg in args]))}\n",["expected inclusion_only_unlimited_args_from_template __doc__ ."]]
["def security_group_rule_get_by_security_group(context, security_group_id):\n\treturn IMPL.security_group_rule_get_by_security_group(context, security_group_id)\n",["get all rules for a given security group ."]]
["def getWinDrives():\n\tassert (os.name == u'nt')\n\tfrom ctypes import windll\n\tdrives = []\n\tbitmask = windll.kernel32.GetLogicalDrives()\n\tfor letter in string.uppercase:\n\t\tif (bitmask & 1):\n\t\t\tdrives.append(letter)\n\t\tbitmask >>= 1\n\treturn drives\n",["return list of detected drives ."]]
["def is_archive_file(name):\n\tarchives = ('.zip', '.tar.gz', '.tar.bz2', '.tgz', '.tar', '.pybundle')\n\text = splitext(name)[1].lower()\n\tif (ext in archives):\n\t\treturn True\n\treturn False\n",["return true if name is a considered as an archive file ."]]
["def filter_oauth_params(params):\n\tis_oauth = (lambda kv: kv[0].startswith(u'oauth_'))\n\tif isinstance(params, dict):\n\t\treturn filter(is_oauth, params.items())\n\telse:\n\t\treturn filter(is_oauth, params)\n",["removes all non oauth parameters from a dict or a list of params ."]]
["@core_helper\ndef redirect_to(*args, **kw):\n\tif are_there_flash_messages():\n\t\tkw['__no_cache__'] = True\n\tuargs = map((lambda arg: (str(arg) if isinstance(arg, unicode) else arg)), args)\n\t_url = url_for(*uargs, **kw)\n\tif _url.startswith('\/'):\n\t\t_url = str((config['ckan.site_url'].rstrip('\/') + _url))\n\tif is_flask_request():\n\t\treturn _flask_redirect(_url)\n\telse:\n\t\treturn _routes_redirect_to(_url)\n",["redirect to a given url ."]]
["def addXMLFromVertexes(depth, output, vertexes):\n\tfor vertexIndex in xrange(len(vertexes)):\n\t\tvertex = vertexes[vertexIndex]\n\t\taddXMLFromXYZ((depth + 1), vertexIndex, output, vertex.x, vertex.y, vertex.z)\n",["add xml from loop ."]]
["def check_valid_abd_naming(pattern=None):\n\tif (pattern is None):\n\t\tpattern = sickbeard.NAMING_PATTERN\n\tlogger.log(((u'Checking\twhether\tthe\tpattern\t' + pattern) + u'\tis\tvalid\tfor\tan\tair-by-date\tepisode'), logger.DEBUG)\n\tvalid = validate_name(pattern, abd=True)\n\treturn valid\n",["checks if the name is can be parsed back to its original form for an air-by-date format ."]]
["def map_string2func(funcname, clss):\n\tif (funcname == 'fprop_roipooling'):\n\t\treturn _get_fprop_roipooling(clss)\n\tif (funcname == 'bprop_roipooling'):\n\t\treturn _get_bprop_roipooling(clss)\n\traise AttributeError(((\"kernel\ttype\t'\" + funcname) + \"'\tnot\tunderstood\"))\n",["helper function that converts string function names to function calls ."]]
["@register.simple_tag\ndef simple_unlimited_args(one, two='hi', *args):\n\treturn ('simple_unlimited_args\t-\tExpected\tresult:\t%s' % ',\t'.join([unicode(arg) for arg in ([one, two] + list(args))]))\n",["expected simple_unlimited_args __doc__ ."]]
["def get_minions():\n\tserv = _get_serv(ret=None)\n\tsql = 'select\tdistinct(id)\tfrom\treturns'\n\tdata = serv.query(sql)\n\tret = []\n\tif data:\n\t\tfor jid in data[0]['points']:\n\t\t\tret.append(jid[1])\n\treturn ret\n",["return a list of minions ."]]
["@register.inclusion_tag(engine.get_template('inclusion.html'))\ndef inclusion_one_default_from_template(one, two='hi'):\n\treturn {'result': ('inclusion_one_default_from_template\t-\tExpected\tresult:\t%s,\t%s' % (one, two))}\n",["expected inclusion_one_default_from_template __doc__ ."]]
["def format_exc(limit=None):\n\ttry:\n\t\t(etype, value, tb) = sys.exc_info()\n\t\treturn ''.join(traceback.format_exception(etype, value, tb, limit))\n\tfinally:\n\t\tetype = value = tb = None\n",["return exc ."]]
["def to_native_string(string, encoding='ascii'):\n\tif isinstance(string, builtin_str):\n\t\tout = string\n\telif is_py2:\n\t\tout = string.encode(encoding)\n\telse:\n\t\tout = string.decode(encoding)\n\treturn out\n",["given a string object ."]]
["def test_continuous_error():\n\ty = np.linspace(0, 1, 20)\n\tcnn = CondensedNearestNeighbour(random_state=RND_SEED)\n\tassert_warns(UserWarning, cnn.fit, X, y)\n",["test either if an error is raised when the target are continuous type ."]]
["def add(song):\n\tif (not g.userhist.get('history')):\n\t\tg.userhist['history'] = Playlist('history')\n\tg.userhist['history'].songs.append(song)\n\tsave()\n",["convenience method will create a task and add it to a queue ."]]
["def _get_default_context(request):\n\tfolders = Object.filter_by_request(request, Folder.objects, mode='r')\n\tmassform = MassActionForm(request.user.profile)\n\tcontext = {'folders': folders, 'massform': massform}\n\treturn context\n",["returns default context as a dict() ."]]
["def add_password_arg(cmd, psw, required=False):\n\tif (UNRAR_TOOL == ALT_TOOL):\n\t\treturn\n\tif (psw is not None):\n\t\tcmd.append(('-p' + psw))\n\telse:\n\t\tcmd.append('-p-')\n",["append password switch to commandline ."]]
["def set_data_value(datastore, path, data):\n\tif isinstance(path, six.string_types):\n\t\tpath = '\/'.split(path)\n\treturn _proxy_cmd('set_data_value', datastore, path, data)\n",["get a data entry in a datastore ."]]
["def re_subm(pat, repl, string):\n\tr = re_compile(pat)\n\tproxy = _re_subm_proxy()\n\tr.sub(proxy.__call__, string)\n\treturn (r.sub(repl, string), proxy.match)\n",["like re ."]]
["def commit_manually(using=None):\n\twarnings.warn('commit_manually\tis\tdeprecated\tin\tfavor\tof\tset_autocommit.', PendingDeprecationWarning, stacklevel=2)\n\tdef entering(using):\n\t\tenter_transaction_management(using=using)\n\tdef exiting(exc_type, using):\n\t\tleave_transaction_management(using=using)\n\treturn _transaction_func(entering, exiting, using)\n",["decorator that activates manual transaction control ."]]
["def load_module_from_name(dotted_name, path=None, use_sys=1):\n\treturn load_module_from_modpath(dotted_name.split('.'), path, use_sys)\n",["load a python module from its name ."]]
["def _generateX(random, bits):\n\twhile True:\n\t\tx = _getRandomNumber(random, bits)\n\t\tif (2 <= x <= ((2 ** bits) - 2)):\n\t\t\treturn x\n",["generate a new value for the private key x ."]]
["def test_continuous_error():\n\ty = np.linspace(0, 1, 20)\n\tcnn = CondensedNearestNeighbour(random_state=RND_SEED)\n\tassert_warns(UserWarning, cnn.fit, X, y)\n",["test either if an error is raised when the target are continuous type ."]]
["def basic_check_run_complete_f(f):\n\tfilepaths = [l.strip() for l in f]\n\tfor fp in filepaths:\n\t\tif (not exists(fp)):\n\t\t\treturn False\n\treturn True\n",["return true if all filepaths exist f: file containing list of filepaths example f: f1 ."]]
["def getManipulatedGeometryOutput(elementNode, geometryOutput, prefix):\n\tflippedGeometryOutput = triangle_mesh.getGeometryOutputCopy(geometryOutput)\n\tflip.flipPoints(elementNode, matrix.getVertexes(flippedGeometryOutput), prefix)\n\tif flip.getShouldReverse(elementNode, prefix):\n\t\tflippedFaces = face.getFaces(flippedGeometryOutput)\n\t\tfor flippedFace in flippedFaces:\n\t\t\tflippedFace.vertexIndexes.reverse()\n\treturn {'union': {'shapes': [flippedGeometryOutput, geometryOutput]}}\n",["get equated geometryoutput ."]]
["def getManipulatedGeometryOutput(elementNode, geometryOutput, prefix):\n\tflippedGeometryOutput = triangle_mesh.getGeometryOutputCopy(geometryOutput)\n\tflip.flipPoints(elementNode, matrix.getVertexes(flippedGeometryOutput), prefix)\n\tif flip.getShouldReverse(elementNode, prefix):\n\t\tflippedFaces = face.getFaces(flippedGeometryOutput)\n\t\tfor flippedFace in flippedFaces:\n\t\t\tflippedFace.vertexIndexes.reverse()\n\treturn {'union': {'shapes': [flippedGeometryOutput, geometryOutput]}}\n",["get equated geometryoutput ."]]
["def test_iht_sample_wt_fit():\n\tiht = InstanceHardnessThreshold(ESTIMATOR, random_state=RND_SEED)\n\tassert_raises(RuntimeError, iht.sample, X, Y)\n",["test either if an error is raised when sample is called before fitting ."]]
["def build_filter(class_name, *args):\n\tif (not hasattr(filters, class_name)):\n\t\treturn None\n\tfilterclass = getattr(filters, class_name)\n\treturn filterclass(*args)\n",["returns a filter object of class class_name ."]]
["def delete_if_exists(pathname):\n\ttry:\n\t\tos.unlink(pathname)\n\texcept OSError as e:\n\t\tif (e.errno == errno.ENOENT):\n\t\t\treturn\n\t\telse:\n\t\t\traise\n",["delete a file ."]]
["def default_key_func(key, key_prefix, version):\n\treturn ('%s:%s:%s' % (key_prefix, version, key))\n",["default function to generate keys ."]]
["def urlquote_plus(url, safe=''):\n\treturn force_unicode(urllib.quote_plus(smart_str(url), safe))\n",["a version of pythons urllib ."]]
["def rws(t):\n\tfor c in ['\\n', ' DCTB ', '\t']:\n\t\tt = t.replace(c, '')\n\treturn t\n",["remove white spaces ."]]
["@register.filter(is_safe=True)\n@stringfilter\ndef slugify(value):\n\treturn _slugify(value)\n",["converts to lowercase ."]]
["def get_system_username():\n\ttry:\n\t\treturn getpass.getuser().decode(locale.getdefaultlocale()[1])\n\texcept (ImportError, KeyError, UnicodeDecodeError):\n\t\treturn u''\n",["try to determine the current system users username ."]]
["def _api_pause(name, output, kwargs):\n\tscheduler.plan_resume(0)\n\tDownloader.do.pause()\n\treturn report(output)\n",["api: accepts output ."]]
["def vereq(a, b):\n\tif (not (a == b)):\n\t\traise TestFailed(('%r\t==\t%r' % (a, b)))\n",["raise testfailed if a == b is false ."]]
["def short_color(color):\n\tif (len(color) > 6):\n\t\treturn color[2:]\n\telse:\n\t\treturn color\n",["format a color to its short size ."]]
["def positive(s, threshold=0.1, **kwargs):\n\treturn (polarity(unicode(s), **kwargs) >= threshold)\n",["returns true if the given sentence has a positive sentiment ."]]
["def FakeUTime(path, times):\n\traise OSError(errno.EPERM, 'Operation\tnot\tpermitted', path)\n",["fake version of os ."]]
["def get_dependencies():\n\tdeps = {'netaddr': HAS_NETADDR, 'python-novaclient': nova.check_nova()}\n\treturn config.check_driver_dependencies(__virtualname__, deps)\n",["warn if dependencies arent met ."]]
["def default_connection_selector(connection, app_blame):\n\treturn handlers.BaseConnectionHandler\n",["returns a nogotofail ."]]
["def with_metaclass(meta, base=object):\n\treturn meta('NewBase', (base,), {})\n",["create a base class with a metaclass ."]]
["def test_ncr_sk_estimator():\n\tcheck_estimator(NeighbourhoodCleaningRule)\n",["test the sklearn estimator compatibility ."]]
["def cbSentMessage(result):\n\tprint('Message\tsent')\n\treactor.stop()\n",["called when the message has been sent ."]]
["def safe_load(stream):\n\treturn load(stream, SafeLoader)\n",["parse the first yaml document in a stream and produce the corresponding python object ."]]
["def getsize(p):\n\tst = os.stat(p)\n\treturn st[stat.ST_SIZE]\n",["return the size of a file ."]]
["def service():\n\tlane_stack.serviceAll()\n\twhile lane_stack.rxMsgs:\n\t\t(msg, sender) = lane_stack.rxMsgs.popleft()\n\t\trx_msgs[msg['route']['dst'][2]] = msg\n",["restful crud controller ."]]
["def comment(parser, token):\n\tparser.skip_past('endcomment')\n\treturn CommentNode()\n",["restful crud controller ."]]
["def getOverhangDirection(belowOutsetLoops, segmentBegin, segmentEnd):\n\tsegment = (segmentEnd - segmentBegin)\n\tnormalizedSegment = euclidean.getNormalized(complex(segment.real, segment.imag))\n\tsegmentYMirror = complex(normalizedSegment.real, (- normalizedSegment.imag))\n\tsegmentBegin = (segmentYMirror * segmentBegin)\n\tsegmentEnd = (segmentYMirror * segmentEnd)\n\tsolidXIntersectionList = []\n\ty = segmentBegin.imag\n\tsolidXIntersectionList.append(euclidean.XIntersectionIndex((-1.0), segmentBegin.real))\n\tsolidXIntersectionList.append(euclidean.XIntersectionIndex((-1.0), segmentEnd.real))\n\tfor belowLoopIndex in xrange(len(belowOutsetLoops)):\n\t\tbelowLoop = belowOutsetLoops[belowLoopIndex]\n\t\trotatedOutset = euclidean.getPointsRoundZAxis(segmentYMirror, belowLoop)\n\t\teuclidean.addXIntersectionIndexesFromLoopY(rotatedOutset, belowLoopIndex, solidXIntersectionList, y)\n\toverhangingSegments = euclidean.getSegmentsFromXIntersectionIndexes(solidXIntersectionList, y)\n\toverhangDirection = complex()\n\tfor overhangingSegment in overhangingSegments:\n\t\toverhangDirection += getDoubledRoundZ(overhangingSegment, normalizedSegment)\n\treturn overhangDirection\n",["add to span direction from the endpoint segments which overhang the layer below ."]]
["def isPointAddedAroundClosest(layerInfillWidth, paths, pixelTable, removedEndpointPoint, width):\n\tclosestDistanceSquared = 9.999999999876543e+17\n\tclosestPathIndex = None\n\tfor pathIndex in xrange(len(paths)):\n\t\tpath = paths[pathIndex]\n\t\tfor pointIndex in xrange(len(path)):\n\t\t\tpoint = path[pointIndex]\n\t\t\tdistanceSquared = abs((point - removedEndpointPoint))\n\t\t\tif (distanceSquared < closestDistanceSquared):\n\t\t\t\tclosestDistanceSquared = distanceSquared\n\t\t\t\tclosestPathIndex = pathIndex\n\tif (closestPathIndex == None):\n\t\treturn\n\tif (closestDistanceSquared < ((0.8 * layerInfillWidth) * layerInfillWidth)):\n\t\treturn\n\tclosestPath = paths[closestPathIndex]\n\tclosestPointIndex = getWithLeastLength(closestPath, removedEndpointPoint)\n\tif isAddedPointOnPathFree(closestPath, pixelTable, removedEndpointPoint, closestPointIndex, width):\n\t\taddPointOnPath(closestPath, closestPathIndex, pixelTable, removedEndpointPoint, closestPointIndex, width)\n\t\treturn True\n\treturn isSidePointAdded(pixelTable, closestPath, closestPathIndex, closestPointIndex, layerInfillWidth, removedEndpointPoint, width)\n",["add the closest removed endpoint to the path ."]]
["@sensitive_post_parameters()\n@csrf_protect\n@never_cache\ndef login(request, template_name='registration\/login.html', redirect_field_name=REDIRECT_FIELD_NAME, authentication_form=AuthenticationForm, current_app=None, extra_context=None):\n\tredirect_to = request.REQUEST.get(redirect_field_name, '')\n\tif (request.method == 'POST'):\n\t\tform = authentication_form(data=request.POST)\n\t\tif form.is_valid():\n\t\t\tif (not is_safe_url(url=redirect_to, host=request.get_host())):\n\t\t\t\tredirect_to = resolve_url(settings.LOGIN_REDIRECT_URL)\n\t\t\tauth_login(request, form.get_user())\n\t\t\tif request.session.test_cookie_worked():\n\t\t\t\trequest.session.delete_test_cookie()\n\t\t\treturn HttpResponseRedirect(redirect_to)\n\telse:\n\t\tform = authentication_form(request)\n\trequest.session.set_test_cookie()\n\tcurrent_site = get_current_site(request)\n\tcontext = {'form': form, redirect_field_name: redirect_to, 'site': current_site, 'site_name': current_site.name}\n\tif (extra_context is not None):\n\t\tcontext.update(extra_context)\n\treturn TemplateResponse(request, template_name, context, current_app=current_app)\n",["displays the login form and handles the login action ."]]
["def summarize_address_range(first, last):\n\tif (not (isinstance(first, _BaseAddress) and isinstance(last, _BaseAddress))):\n\t\traise TypeError(u'first\tand\tlast\tmust\tbe\tIP\taddresses,\tnot\tnetworks')\n\tif (first.version != last.version):\n\t\traise TypeError((u'%s\tand\t%s\tare\tnot\tof\tthe\tsame\tversion' % (first, last)))\n\tif (first > last):\n\t\traise ValueError(u'last\tIP\taddress\tmust\tbe\tgreater\tthan\tfirst')\n\tif (first.version == 4):\n\t\tip = IPv4Network\n\telif (first.version == 6):\n\t\tip = IPv6Network\n\telse:\n\t\traise ValueError(u'unknown\tIP\tversion')\n\tip_bits = first._max_prefixlen\n\tfirst_int = first._ip\n\tlast_int = last._ip\n\twhile (first_int <= last_int):\n\t\tnbits = min(_count_righthand_zero_bits(first_int, ip_bits), (_compat_bit_length(((last_int - first_int) + 1)) - 1))\n\t\tnet = ip((first_int, (ip_bits - nbits)))\n\t\t(yield net)\n\t\tfirst_int += (1 << nbits)\n\t\tif ((first_int - 1) == ip._ALL_ONES):\n\t\t\tbreak\n",["summarize a network range given the first and last ip addresses ."]]
["def test_dnn_tag():\n\tx = T.tensor4()\n\told = theano.config.on_opt_error\n\ttheano.config.on_opt_error = 'raise'\n\tsio = StringIO()\n\thandler = logging.StreamHandler(sio)\n\tlogging.getLogger('theano.compile.tests.test_dnn').addHandler(handler)\n\tlogging.getLogger('theano').removeHandler(theano.logging_default_handler)\n\traised = False\n\ttry:\n\t\tf = theano.function([x], pool_2d(x, ws=(2, 2), ignore_border=True), mode=mode_with_gpu.including('cudnn'))\n\texcept (AssertionError, RuntimeError):\n\t\tassert (not dnn.dnn_available(test_ctx_name))\n\t\traised = True\n\tfinally:\n\t\ttheano.config.on_opt_error = old\n\t\tlogging.getLogger('theano.compile.tests.test_dnn').removeHandler(handler)\n\t\tlogging.getLogger('theano').addHandler(theano.logging_default_handler)\n\tif (not raised):\n\t\tassert dnn.dnn_available(test_ctx_name)\n\t\tassert any([isinstance(n.op, dnn.GpuDnnPool) for n in f.maker.fgraph.toposort()])\n",["test that if cudnn isnt avail we crash and that if it is avail ."]]
["def FindPythonExe(exeAlias, possibleRealNames, searchPaths):\n\timport win32api, regutil, string, os, sys\n\tif (possibleRealNames is None):\n\t\tpossibleRealNames = exeAlias\n\tfound = os.path.join(sys.prefix, possibleRealNames)\n\tif (not FileExists(found)):\n\t\tfound = os.path.join(sys.prefix, 'PCBuild', possibleRealNames)\n\tif (not FileExists(found)):\n\t\tfound = LocateFileName(possibleRealNames, searchPaths)\n\tregistered_ok = 0\n\ttry:\n\t\tregistered = win32api.RegQueryValue(regutil.GetRootKey(), ((regutil.GetAppPathsKey() + '\\\\') + exeAlias))\n\t\tregistered_ok = (found == registered)\n\texcept win32api.error:\n\t\tpass\n\treturn (found, registered_ok)\n",["find an exe ."]]
["def ValidateProperty(name, values, read_only=False):\n\tValidateString(name, 'property\tname', datastore_errors.BadPropertyError)\n\tvalues_type = type(values)\n\tif (values_type is tuple):\n\t\traise datastore_errors.BadValueError(('May\tnot\tuse\ttuple\tproperty\tvalue;\tproperty\t%s\tis\t%s.' % (name, repr(values))))\n\tif (values_type is not list):\n\t\tvalues = [values]\n\tif (not values):\n\t\traise datastore_errors.BadValueError(('May\tnot\tuse\tthe\tempty\tlist\tas\ta\tproperty\tvalue;\tproperty\t%s\tis\t%s.' % (name, repr(values))))\n\ttry:\n\t\tfor v in values:\n\t\t\tprop_validator = _VALIDATE_PROPERTY_VALUES.get(v.__class__)\n\t\t\tif (prop_validator is None):\n\t\t\t\traise datastore_errors.BadValueError(('Unsupported\ttype\tfor\tproperty\t%s:\t%s' % (name, v.__class__)))\n\t\t\tprop_validator(name, v)\n\texcept (KeyError, ValueError, TypeError, IndexError, AttributeError) as msg:\n\t\traise datastore_errors.BadValueError(('Error\ttype\tchecking\tvalues\tfor\tproperty\t%s:\t%s' % (name, msg)))\n",["helper function for validating property values ."]]
["def force_unicode(s):\n\tif (s is None):\n\t\treturn None\n\telse:\n\t\treturn unicod(s)\n",["similar to smart_unicode ."]]
["def writePlistToResource(rootObject, path, restype='plst', resid=0):\n\twarnings.warnpy3k('In\t3.x,\twritePlistToResource\tis\tremoved.')\n\tfrom Carbon.File import FSRef, FSGetResourceForkName\n\tfrom Carbon.Files import fsRdWrPerm\n\tfrom Carbon import Res\n\tplistData = writePlistToString(rootObject)\n\tfsRef = FSRef(path)\n\tresNum = Res.FSOpenResourceFile(fsRef, FSGetResourceForkName(), fsRdWrPerm)\n\tRes.UseResFile(resNum)\n\ttry:\n\t\tRes.Get1Resource(restype, resid).RemoveResource()\n\texcept Res.Error:\n\t\tpass\n\tres = Res.Resource(plistData)\n\tres.AddResource(restype, resid, '')\n\tres.WriteResource()\n\tRes.CloseResFile(resNum)\n",["write rootobject as a plst resource to the resource fork of path ."]]
["def _buildArgs(f, self=None, kwargs={}):\n\targTuples = getArgumentDescriptions(f)\n\targTuples = argTuples[1:]\n\tinit = SPRegion.__init__\n\tourArgNames = [t[0] for t in getArgumentDescriptions(init)]\n\tourArgNames += ['numberOfCols']\n\tfor argTuple in argTuples[:]:\n\t\tif (argTuple[0] in ourArgNames):\n\t\t\targTuples.remove(argTuple)\n\tif self:\n\t\tfor argTuple in argTuples:\n\t\t\targName = argTuple[0]\n\t\t\tif (argName in kwargs):\n\t\t\t\targValue = kwargs.pop(argName)\n\t\t\telse:\n\t\t\t\tif (len(argTuple) == 2):\n\t\t\t\t\traise TypeError((\"Must\tprovide\tvalue\tfor\t'%s'\" % argName))\n\t\t\t\targValue = argTuple[2]\n\t\t\tsetattr(self, argName, argValue)\n\treturn argTuples\n",["get the default arguments from the function and assign as instance vars ."]]
["def should_bypass_proxies(url):\n\tget_proxy = (lambda k: (os.environ.get(k) or os.environ.get(k.upper())))\n\tno_proxy = get_proxy('no_proxy')\n\tnetloc = urlparse(url).netloc\n\tif no_proxy:\n\t\tno_proxy = no_proxy.replace('\t', '').split(',')\n\t\tip = netloc.split(':')[0]\n\t\tif is_ipv4_address(ip):\n\t\t\tfor proxy_ip in no_proxy:\n\t\t\t\tif is_valid_cidr(proxy_ip):\n\t\t\t\t\tif address_in_network(ip, proxy_ip):\n\t\t\t\t\t\treturn True\n\t\telse:\n\t\t\tfor host in no_proxy:\n\t\t\t\tif (netloc.endswith(host) or netloc.split(':')[0].endswith(host)):\n\t\t\t\t\treturn True\n\ttry:\n\t\tbypass = proxy_bypass(netloc)\n\texcept (TypeError, socket.gaierror):\n\t\tbypass = False\n\tif bypass:\n\t\treturn True\n\treturn False\n",["returns whether we should bypass proxies or not ."]]
["def _dump_date(d, delim):\n\tif (d is None):\n\t\td = gmtime()\n\telif isinstance(d, datetime):\n\t\td = d.utctimetuple()\n\telif isinstance(d, (integer_types, float)):\n\t\td = gmtime(d)\n\treturn ('%s,\t%02d%s%s%s%s\t%02d:%02d:%02d\tGMT' % (('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun')[d.tm_wday], d.tm_mday, delim, ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')[(d.tm_mon - 1)], delim, str(d.tm_year), d.tm_hour, d.tm_min, d.tm_sec))\n",["used for http_date and cookie_date ."]]
["def default_user_agent():\n\t_implementation = platform.python_implementation()\n\tif (_implementation == 'CPython'):\n\t\t_implementation_version = platform.python_version()\n\telif (_implementation == 'PyPy'):\n\t\t_implementation_version = ('%s.%s.%s' % (sys.pypy_version_info.major, sys.pypy_version_info.minor, sys.pypy_version_info.micro))\n\t\tif (sys.pypy_version_info.releaselevel != 'final'):\n\t\t\t_implementation_version = ''.join([_implementation_version, sys.pypy_version_info.releaselevel])\n\telif (_implementation == 'Jython'):\n\t\t_implementation_version = platform.python_version()\n\telif (_implementation == 'IronPython'):\n\t\t_implementation_version = platform.python_version()\n\telse:\n\t\t_implementation_version = 'Unknown'\n\ttry:\n\t\tp_system = platform.system()\n\t\tp_release = platform.release()\n\texcept IOError:\n\t\tp_system = 'Unknown'\n\t\tp_release = 'Unknown'\n\treturn '\t'.join([('python-requests\/%s' % __version__), ('%s\/%s' % (_implementation, _implementation_version)), ('%s\/%s' % (p_system, p_release))])\n",["return a string representing the default user agent ."]]
["def find_command(cmd, paths=None, pathext=None):\n\tif (paths is None):\n\t\tpaths = os.environ.get('PATH', '').split(os.pathsep)\n\tif isinstance(paths, string_types):\n\t\tpaths = [paths]\n\tif (pathext is None):\n\t\tpathext = get_pathext()\n\tpathext = [ext for ext in pathext.lower().split(os.pathsep) if len(ext)]\n\tif (os.path.splitext(cmd)[1].lower() in pathext):\n\t\tpathext = ['']\n\tfor path in paths:\n\t\tcmd_path = os.path.join(path, cmd)\n\t\tfor ext in pathext:\n\t\t\tcmd_path_ext = (cmd_path + ext)\n\t\t\tif os.path.isfile(cmd_path_ext):\n\t\t\t\treturn cmd_path_ext\n\t\tif os.path.isfile(cmd_path):\n\t\t\treturn cmd_path\n\traise BadCommand(('Cannot\tfind\tcommand\t%r' % cmd))\n",["searches the path for the given command and returns its path ."]]
["def _ipconfig_getnode():\n\timport os, re\n\tdirs = ['', 'c:\\\\windows\\\\system32', 'c:\\\\winnt\\\\system32']\n\ttry:\n\t\timport ctypes\n\t\tbuffer = ctypes.create_string_buffer(300)\n\t\tctypes.windll.kernel32.GetSystemDirectoryA(buffer, 300)\n\t\tdirs.insert(0, buffer.value.decode('mbcs'))\n\texcept:\n\t\tpass\n\tfor dir in dirs:\n\t\ttry:\n\t\t\tpipe = os.popen((os.path.join(dir, 'ipconfig') + '\t\/all'))\n\t\texcept IOError:\n\t\t\tcontinue\n\t\tfor line in pipe:\n\t\t\tvalue = line.split(':')[(-1)].strip().lower()\n\t\t\tif re.match('([0-9a-f][0-9a-f]-){5}[0-9a-f][0-9a-f]', value):\n\t\t\t\treturn int(value.replace('-', ''), 16)\n",["get the hardware address on windows by running ipconfig ."]]
["def _parse_date_greek(dateString):\n\tm = _greek_date_format_re.match(dateString)\n\tif (not m):\n\t\treturn\n\twday = _greek_wdays[m.group(1)]\n\tmonth = _greek_months[m.group(3)]\n\trfc822date = ('%(wday)s,\t%(day)s\t%(month)s\t%(year)s\t%(hour)s:%(minute)s:%(second)s\t%(zonediff)s' % {'wday': wday, 'day': m.group(2), 'month': month, 'year': m.group(4), 'hour': m.group(5), 'minute': m.group(6), 'second': m.group(7), 'zonediff': m.group(8)})\n\treturn _parse_date_rfc822(rfc822date)\n",["parse a string according to a greek 8-bit date format ."]]
["def logout(request, next_page=None, template_name='registration\/logged_out.html', redirect_field_name=REDIRECT_FIELD_NAME, current_app=None, extra_context=None):\n\tauth_logout(request)\n\tif (next_page is not None):\n\t\tnext_page = resolve_url(next_page)\n\tif (redirect_field_name in request.REQUEST):\n\t\tnext_page = request.REQUEST[redirect_field_name]\n\t\tif (not is_safe_url(url=next_page, host=request.get_host())):\n\t\t\tnext_page = request.path\n\tif next_page:\n\t\treturn HttpResponseRedirect(next_page)\n\tcurrent_site = get_current_site(request)\n\tcontext = {'site': current_site, 'site_name': current_site.name, 'title': _('Logged\tout')}\n\tif (extra_context is not None):\n\t\tcontext.update(extra_context)\n\treturn TemplateResponse(request, template_name, context, current_app=current_app)\n",["logs out the user and displays you are logged out message ."]]
["def show_model_changes(new, old=None, fields=None, always=False):\n\told = (old or new._db._get(type(new), new.id))\n\tchanges = []\n\tfor field in old:\n\t\tif ((field == 'mtime') or (fields and (field not in fields))):\n\t\t\tcontinue\n\t\tline = _field_diff(field, old, new)\n\t\tif line:\n\t\t\tchanges.append(u'\t\t{0}:\t{1}'.format(field, line))\n\tfor field in (set(new) - set(old)):\n\t\tif (fields and (field not in fields)):\n\t\t\tcontinue\n\t\tchanges.append(u'\t\t{0}:\t{1}'.format(field, colorize('red', new.formatted()[field])))\n\tif (changes or always):\n\t\tprint_obj(old, old._db)\n\tif changes:\n\t\tprint_(u'\\n'.join(changes))\n\treturn bool(changes)\n",["given a model object ."]]
["def filterwarnings(action, message='', category=Warning, module='', lineno=0, append=0):\n\timport re\n\traise ((action in ('error', 'ignore', 'always', 'default', 'module', 'once')) or AssertionError), ('invalid\taction:\t%r' % (action,))\n\traise (isinstance(message, basestring) or AssertionError), 'message\tmust\tbe\ta\tstring'\n\traise (isinstance(category, (type, types.ClassType)) or AssertionError), 'category\tmust\tbe\ta\tclass'\n\traise (issubclass(category, Warning) or AssertionError), 'category\tmust\tbe\ta\tWarning\tsubclass'\n\traise (isinstance(module, basestring) or AssertionError), 'module\tmust\tbe\ta\tstring'\n\tif (not (isinstance(lineno, int) and (lineno >= 0))):\n\t\traise AssertionError, 'lineno\tmust\tbe\tan\tint\t>=\t0'\n\t\titem = (action, re.compile(message, re.I), category, re.compile(module), lineno)\n\t\t(append and filters.append(item))\n\telse:\n\t\tfilters.insert(0, item)\n",["insert an entry into the list of warnings filters ."]]
["def getTreeWalker(treeType, implementation=None, **kwargs):\n\ttreeType = treeType.lower()\n\tif (treeType not in treeWalkerCache):\n\t\tif (treeType in ('dom', 'pulldom', 'simpletree')):\n\t\t\tmod = __import__(treeType, globals())\n\t\t\ttreeWalkerCache[treeType] = mod.TreeWalker\n\t\telif (treeType == 'genshi'):\n\t\t\timport genshistream\n\t\t\ttreeWalkerCache[treeType] = genshistream.TreeWalker\n\t\telif (treeType == 'beautifulsoup'):\n\t\t\timport soup\n\t\t\ttreeWalkerCache[treeType] = soup.TreeWalker\n\t\telif (treeType == 'lxml'):\n\t\t\timport lxmletree\n\t\t\ttreeWalkerCache[treeType] = lxmletree.TreeWalker\n\t\telif (treeType == 'etree'):\n\t\t\timport etree\n\t\t\treturn etree.getETreeModule(implementation, **kwargs).TreeWalker\n\treturn treeWalkerCache.get(treeType)\n",["get a treewalker class for various types of tree with built-in support treetype - the name of the tree type required ."]]
["def walk(top, func, arg):\n\twarnings.warnpy3k('In\t3.x,\tos.path.walk\tis\tremoved\tin\tfavor\tof\tos.walk.')\n\ttry:\n\t\tnames = os.listdir(top)\n\texcept os.error:\n\t\treturn\n\tfunc(arg, top, names)\n\tfor name in names:\n\t\tname = join(top, name)\n\t\tif isdir(name):\n\t\t\twalk(name, func, arg)\n",["directory tree generator ."]]
["def test_read_no_header_names_NoHeader():\n\ttable = '\\n|\t\tJohn\t\t|\t555-1234\t|192.168.1.10|\\n|\t\tMary\t\t|\t555-2134\t|192.168.1.12|\\n|\t\t\tBob\t\t|\t555-4527\t|\t192.168.1.9|\\n'\n\tdat = ascii.read(table, Reader=ascii.FixedWidthNoHeader, names=('Name', 'Phone', 'TCP'))\n\tassert_equal(tuple(dat.dtype.names), ('Name', 'Phone', 'TCP'))\n\tassert_equal(dat[1][0], 'Mary')\n\tassert_equal(dat[0][1], '555-1234')\n\tassert_equal(dat[2][2], '192.168.1.9')\n",["table with no header row and with col names provided ."]]
["def test_eager_does_upgrade_dependecies_when_no_longer_satisfied(script):\n\tscript.pip_install_local('simple==1.0', expect_error=True)\n\tresult = script.pip_install_local('--upgrade', '--upgrade-strategy=eager', 'require_simple', expect_error=True)\n\tassert (((script.site_packages \/ 'require_simple-1.0-py%s.egg-info') % pyversion) not in result.files_deleted), 'should\thave\tinstalled\trequire_simple==1.0'\n\tassert (((script.site_packages \/ 'simple-3.0-py%s.egg-info') % pyversion) in result.files_created), 'should\thave\tinstalled\tsimple==3.0'\n\tassert (((script.site_packages \/ 'simple-1.0-py%s.egg-info') % pyversion) in result.files_deleted), 'should\thave\tuninstalled\tsimple==1.0'\n",["it does upgrade a dependency if it no longer satisfies the requirements ."]]
["def get_default_username(distribution):\n\treturn 'root'\n",["try to determine the current system users username to use as a default ."]]
["def processCondition(xmlElement):\n\txmlProcessor = xmlElement.getXMLProcessor()\n\tif (xmlElement.object == None):\n\t\txmlElement.object = ModuleXMLElement(xmlElement)\n\tif (xmlElement.object.conditionSplitWords == None):\n\t\treturn\n\tif (len(xmlProcessor.functions) < 1):\n\t\tprint 'Warning,\t\"in\"\telement\tis\tnot\tin\ta\tfunction\tin\tprocessCondition\tin\tevaluate\tfor:'\n\t\tprint xmlElement\n\t\treturn\n\tif (int(getEvaluatedExpressionValueBySplitLine(xmlElement.object.conditionSplitWords, xmlElement)) > 0):\n\t\txmlProcessor.functions[(-1)].processChildren(xmlElement)\n\telse:\n\t\txmlElement.object.processElse(xmlElement)\n",["process the xml element condition ."]]
["def _run_finalizers(minpriority=None):\n\tif (minpriority is None):\n\t\tf = (lambda p: (p[0][0] is not None))\n\telse:\n\t\tf = (lambda p: ((p[0][0] is not None) and (p[0][0] >= minpriority)))\n\titems = [x for x in _finalizer_registry.items() if f(x)]\n\titems.sort(reverse=True)\n\tfor (key, finalizer) in items:\n\t\tsub_debug('calling\t%s', finalizer)\n\t\ttry:\n\t\t\tfinalizer()\n\t\texcept Exception:\n\t\t\timport traceback\n\t\t\ttraceback.print_exc()\n\tif (minpriority is None):\n\t\t_finalizer_registry.clear()\n",["run all finalizers whose exit priority is not none and at least minpriority finalizers with highest priority are called first; finalizers with the same priority will be called"]]
["def tunnel_patch(http):\n\trequest_orig = http.request\n\tdef new_request(uri, method='GET', body=None, headers=None, redirections=httplib2.DEFAULT_MAX_REDIRECTS, connection_type=None):\n\t\t'Modify\tthe\trequest\theaders\tto\tadd\tthe\tuser-agent.'\n\t\tif (headers is None):\n\t\t\theaders = {}\n\t\tif (method == 'PATCH'):\n\t\t\tif ('oauth_token' in headers.get('authorization', '')):\n\t\t\t\tlogging.warning('OAuth\t1.0\trequest\tmade\twith\tCredentials\tafter\ttunnel_patch.')\n\t\t\theaders['x-http-method-override'] = 'PATCH'\n\t\t\tmethod = 'POST'\n\t\t(resp, content) = request_orig(uri, method, body, headers, redirections, connection_type)\n\t\treturn (resp, content)\n\thttp.request = new_request\n\treturn http\n",["tunnel patch requests over post ."]]
["def _seticon(object_alias, icondata):\n\tfinder = _getfinder()\n\targs = {}\n\tattrs = {}\n\taeobj_00 = aetypes.ObjectSpecifier(want=aetypes.Type('cobj'), form='alis', seld=object_alias, fr=None)\n\taeobj_01 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('iimg'), fr=aeobj_00)\n\targs['----'] = aeobj_01\n\targs['data'] = icondata\n\t(_reply, args, attrs) = finder.send('core', 'setd', args, attrs)\n\tif args.has_key('errn'):\n\t\traise Error, aetools.decodeerror(args)\n\tif args.has_key('----'):\n\t\treturn args['----'].data\n",["set the icondata for object ."]]
["def prepare_roidb(imdb):\n\tsizes = [PIL.Image.open(imdb.image_path_at(i)).size for i in xrange(imdb.num_images)]\n\troidb = imdb.roidb\n\tfor i in xrange(len(imdb.image_index)):\n\t\troidb[i]['image'] = imdb.image_path_at(i)\n\t\troidb[i]['width'] = sizes[i][0]\n\t\troidb[i]['height'] = sizes[i][1]\n\t\tgt_overlaps = roidb[i]['gt_overlaps'].toarray()\n\t\tmax_overlaps = gt_overlaps.max(axis=1)\n\t\tmax_classes = gt_overlaps.argmax(axis=1)\n\t\troidb[i]['max_classes'] = max_classes\n\t\troidb[i]['max_overlaps'] = max_overlaps\n\t\tzero_inds = np.where((max_overlaps == 0))[0]\n\t\tassert all((max_classes[zero_inds] == 0))\n\t\tnonzero_inds = np.where((max_overlaps > 0))[0]\n\t\tassert all((max_classes[nonzero_inds] != 0))\n",["enrich the imdbs roidb by adding some derived quantities that are useful for training ."]]
["@with_setup(prepare_stdout)\ndef test_output_with_success_colorless():\n\trunner = Runner(join_path('pt-br', 'success', 'dumb.feature'), verbosity=3, no_color=True)\n\trunner.run()\n\tassert_stdout_lines(u'\\nFuncionalidade:\tfeature\tburra\t\t\t\t\t\t\t#\ttests\/functional\/language_specific_features\/pt-br\/success\/dumb.feature:3\\n\t\tComo\tum\tprogramador\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\ttests\/functional\/language_specific_features\/pt-br\/success\/dumb.feature:4\\n\t\tEu\tquero\tque\teste\tteste\tpasse\t\t\t\t\t#\ttests\/functional\/language_specific_features\/pt-br\/success\/dumb.feature:5\\n\t\tPara\ttestar\tum\tcen\\xe1rio\tde\tsucesso\t#\ttests\/functional\/language_specific_features\/pt-br\/success\/dumb.feature:6\\n\\n\t\tCen\\xe1rio:\tFazer\tnada\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\ttests\/functional\/language_specific_features\/pt-br\/success\/dumb.feature:8\\n\t\t\t\tDado\tque\teu\tfa\\xe7o\tnada\t\t\t\t\t\t\t\t\t\t\t#\ttests\/functional\/language_specific_features\/pt-br\/success\/dumb_steps.py:6\\n\\n1\tfeature\t(1\tpassed)\\n1\tscenario\t(1\tpassed)\\n1\tstep\t(1\tpassed)\\n')\n",["language: fr -> sucess colorless ."]]
["def test_feature_ptbr_from_string():\n\tptbr = Language('pt-br')\n\tfeature = Feature.from_string(FEATURE, language=ptbr)\n\tassert_equals(feature.name, u'Pesquisar\talunos\tcom\tmatr\\xedcula\tvencida')\n\tassert_equals(feature.description, u'Como\tgerente\tfinanceiro\\nEu\tquero\tpesquisar\talunos\tcom\tmatr\\xedcula\tvencida\\nPara\tpropor\tum\tfinanciamento')\n\t(scenario,) = feature.scenarios\n\tassert_equals(scenario.name, 'Pesquisar\tpor\tnome\tdo\tcurso')\n\tassert_equals(scenario.steps[(-1)].hashes, [{'nome': u'Jo\\xe3o', u'valor\tdevido': 'R$\t512,66'}, {'nome': u'Maria', u'valor\tdevido': 'R$\t998,41'}, {'nome': u'Ana', u'valor\tdevido': 'R$\t231,00'}])\n",["language: pt-br -> feature ."]]
["def _find_executable(executable, path=None):\n\tif (path is None):\n\t\tpath = os.environ['PATH']\n\tpaths = path.split(os.pathsep)\n\t(base, ext) = os.path.splitext(executable)\n\tif (((sys.platform == 'win32') or (os.name == 'os2')) and (ext != '.exe')):\n\t\texecutable = (executable + '.exe')\n\tif (not os.path.isfile(executable)):\n\t\tfor p in paths:\n\t\t\tf = os.path.join(p, executable)\n\t\t\tif os.path.isfile(f):\n\t\t\t\treturn f\n\t\treturn None\n\telse:\n\t\treturn executable\n",["tries to find executable in the directories listed in path ."]]
["def url_params_from_lookup_dict(lookups):\n\tparams = {}\n\tif (lookups and hasattr(lookups, u'items')):\n\t\titems = []\n\t\tfor (k, v) in lookups.items():\n\t\t\tif callable(v):\n\t\t\t\tv = v()\n\t\t\tif isinstance(v, (tuple, list)):\n\t\t\t\tv = u','.join([str(x) for x in v])\n\t\t\telif isinstance(v, bool):\n\t\t\t\tv = (u'0', u'1')[v]\n\t\t\telse:\n\t\t\t\tv = six.text_type(v)\n\t\t\titems.append((k, v))\n\t\tparams.update(dict(items))\n\treturn params\n",["converts the type of lookups specified in a foreignkey limit_choices_to attribute to a dictionary of query parameters ."]]
["def colored(text, color=None, on_color=None, attrs=None):\n\tif (os.getenv('ANSI_COLORS_DISABLED') is None):\n\t\tfmt_str = '\\x1b[%dm%s'\n\t\tif (color is not None):\n\t\t\ttext = (fmt_str % (COLORS[color], text))\n\t\tif (on_color is not None):\n\t\t\ttext = (fmt_str % (HIGHLIGHTS[on_color], text))\n\t\tif (attrs is not None):\n\t\t\tfor attr in attrs:\n\t\t\t\ttext = (fmt_str % (ATTRIBUTES[attr], text))\n\t\ttext += RESET\n\treturn text\n",["colorize text ."]]
["def application_uri(environ):\n\turl = (environ['wsgi.url_scheme'] + ':\/\/')\n\tfrom urllib.parse import quote\n\tif environ.get('HTTP_HOST'):\n\t\turl += environ['HTTP_HOST']\n\telse:\n\t\turl += environ['SERVER_NAME']\n\t\tif (environ['wsgi.url_scheme'] == 'https'):\n\t\t\tif (environ['SERVER_PORT'] != '443'):\n\t\t\t\turl += (':' + environ['SERVER_PORT'])\n\t\telif (environ['SERVER_PORT'] != '80'):\n\t\t\turl += (':' + environ['SERVER_PORT'])\n\turl += quote((environ.get('SCRIPT_NAME') or '\/'), encoding='latin1')\n\treturn url\n",["return the applications base uri ."]]
["def _filterargs(source):\n\targsregex = \"}\\\\('(.*)',\t*(\\\\d+),\t*(\\\\d+),\t*'(.*)'\\\\.split\\\\('\\\\|'\\\\),\t*(\\\\d+),\t*(.*)\\\\)\\\\)\"\n\targs = re.search(argsregex, source, re.DOTALL).groups()\n\ttry:\n\t\treturn (args[0], args[3].split('|'), int(args[1]), int(args[2]))\n\texcept ValueError:\n\t\traise UnpackingError('Corrupted\tp.a.c.k.e.r.\tdata.')\n",["juice from a source file the four args needed by decoder ."]]
["def _check_diff(cat_name, base_path):\n\tpo_path = '{path}\/en\/LC_MESSAGES\/django.po'.format(path=base_path)\n\tp = Popen(\"git\tdiff\t-U0\t{0}\t|\tegrep\t'^[-+]msgid'\t|\twc\t-l\".format(po_path), stdout=PIPE, stderr=PIPE, shell=True)\n\t(output, errors) = p.communicate()\n\tnum_changes = int(output.strip())\n\tprint \"{0}\tchanged\/added\tmessages\tin\t'{1}'\tcatalog.\".format(num_changes, cat_name)\n",["output the approximate number of changed\/added strings in the en catalog ."]]
["@register.tag\ndef ssi(parser, token):\n\tbits = token.split_contents()\n\tparsed = False\n\tif (len(bits) not in (2, 3)):\n\t\traise TemplateSyntaxError(u\"'ssi'\ttag\ttakes\tone\targument:\tthe\tpath\tto\tthe\tfile\tto\tbe\tincluded\")\n\tif (len(bits) == 3):\n\t\tif (bits[2] == u'parsed'):\n\t\t\tparsed = True\n\t\telse:\n\t\t\traise TemplateSyntaxError((u\"Second\t(optional)\targument\tto\t%s\ttag\tmust\tbe\t'parsed'\" % bits[0]))\n\tfilepath = parser.compile_filter(bits[1])\n\treturn SsiNode(filepath, parsed)\n",["outputs the contents of a given file into the page ."]]
["@app.route('\/account\/<subscription_id>\/resourcegroups\/<resource_group_name>\/vms\/<vm_name>')\n@auth.require_login\ndef vm_view(subscription_id, resource_group_name, vm_name):\n\tcreds = _get_credentials()\n\tmodel = models.get_vm_details(subscription_id, creds, resource_group_name, vm_name)\n\treturn render_template('vm.html', title=vm_name, year=datetime.now().year, subscription_id=subscription_id, resource_group_name=resource_group_name, model=model)\n",["renders the vm details ."]]
["def getCraftedTextFromText(gcodeText, exportRepository=None):\n\tif gcodec.isProcedureDoneOrFileIsEmpty(gcodeText, 'export'):\n\t\treturn gcodeText\n\tif (exportRepository == None):\n\t\texportRepository = settings.getReadRepository(ExportRepository())\n\tif (not exportRepository.activateExport.value):\n\t\treturn gcodeText\n\treturn ExportSkein().getCraftedGcode(exportRepository, gcodeText)\n",["flow a gcode linear move text ."]]
["def get_user_from_cookie(cookies, app_id, app_secret):\n\tcookie = cookies.get(('fbsr_%s' % app_id), None)\n\tif (not cookie):\n\t\treturn None\n\tparsed_request = parse_signed_request(cookie, app_secret)\n\tif (not parsed_request):\n\t\treturn None\n\ttry:\n\t\tresult = get_access_token_from_code(parsed_request['code'], '', app_id, app_secret)\n\texcept GraphAPIError:\n\t\treturn None\n\tresult['uid'] = parsed_request['user_id']\n\treturn result\n",["parses the cookie set by the official facebook javascript sdk ."]]
["def advanced_indexing_op(input, index):\n\tbatch_size = tf.shape(input)[0]\n\tmax_length = int(input.get_shape()[1])\n\tdim_size = int(input.get_shape()[2])\n\tindex = ((tf.range(0, batch_size) * max_length) + (index - 1))\n\tflat = tf.reshape(input, [(-1), dim_size])\n\trelevant = tf.gather(flat, index)\n\treturn relevant\n",["advanced indexing for sequences ."]]
["@treeio_login_required\ndef ajax_user_lookup(request, response_format='html'):\n\tusers = []\n\tif (request.GET and ('term' in request.GET)):\n\t\tusers = User.objects.filter((Q(name__icontains=request.GET['term']) | Q(contact__name__icontains=request.GET['term'])))\n\treturn render_to_response('identities\/ajax_user_lookup', {'users': users}, context_instance=RequestContext(request), response_format=response_format)\n",["returns a list of matching users ."]]
["def process_parallel(callbacks, input, *a, **kw):\n\tdfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n\td = defer.DeferredList(dfds, fireOnOneErrback=1, consumeErrors=1)\n\td.addCallbacks((lambda r: [x[1] for x in r]), (lambda f: f.value.subFailure))\n\treturn d\n",["return a deferred with the output of all successful calls to the given callbacks ."]]
["def send_mass_mail(datatuple, fail_silently=False, auth_user=None, auth_password=None, connection=None):\n\tconnection = (connection or get_connection(username=auth_user, password=auth_password, fail_silently=fail_silently))\n\tmessages = [EmailMessage(subject, message, sender, recipient, connection=connection) for (subject, message, sender, recipient) in datatuple]\n\treturn connection.send_messages(messages)\n",["given a datatuple of ."]]
["def _write_file(folder, filename, data):\n\tpath = os.path.join(folder, filename)\n\tif (not os.path.exists(folder)):\n\t\tmsg = '{0}\tcannot\tbe\twritten.\t{1}\tdoes\tnot\texist'\n\t\tmsg = msg.format(filename, folder)\n\t\tlog.error(msg)\n\t\traise AttributeError(msg)\n\twith salt.utils.fopen(path, 'w') as fp_:\n\t\tfp_.write(data)\n\treturn 0\n",["writes a file to disk ."]]
["def get_templatetags_modules():\n\tglobal templatetags_modules\n\tif (not templatetags_modules):\n\t\t_templatetags_modules = []\n\t\tfor app_module in (['google.appengine._internal.django'] + list(settings.INSTALLED_APPS)):\n\t\t\ttry:\n\t\t\t\ttemplatetag_module = ('%s.templatetags' % app_module)\n\t\t\t\timport_module(templatetag_module)\n\t\t\t\t_templatetags_modules.append(templatetag_module)\n\t\t\texcept ImportError:\n\t\t\t\tcontinue\n\t\ttemplatetags_modules = _templatetags_modules\n\treturn templatetags_modules\n",["return the list of all available template tag modules ."]]
["def uuid4():\n\tif _uuid_generate_random:\n\t\t_buffer = ctypes.create_string_buffer(16)\n\t\t_uuid_generate_random(_buffer)\n\t\treturn UUID(bytes=_buffer.raw)\n\ttry:\n\t\timport os\n\t\treturn UUID(bytes=os.urandom(16), version=4)\n\texcept:\n\t\timport random\n\t\tbytes = [chr(random.randrange(256)) for i in range(16)]\n\t\treturn UUID(bytes=bytes, version=4)\n",["generate a random uuid ."]]
["def _escape(value):\n\tif isinstance(value, (list, tuple)):\n\t\tvalue = u','.join(value)\n\telif isinstance(value, (date, datetime)):\n\t\tvalue = value.isoformat()\n\telif isinstance(value, bool):\n\t\tvalue = str(value).lower()\n\tif isinstance(value, string_types):\n\t\ttry:\n\t\t\treturn value.encode(u'utf-8')\n\t\texcept UnicodeDecodeError:\n\t\t\tpass\n\treturn str(value)\n",["escape & ."]]
["def test_suggested_column_names_from_visible_table(completer, complete_event):\n\ttext = u'SELECT\t\tfrom\tusers'\n\tposition = len(u'SELECT\t')\n\tresult = set(completer.get_completions(Document(text=text, cursor_position=position), complete_event))\n\tassert (set(result) == set(((testdata.columns(u'users') + testdata.functions()) + list((testdata.builtin_functions() + testdata.keywords())))))\n",["suggest column and function names when selecting from table ."]]
["@register.tag('cache')\ndef do_cache(parser, token):\n\tnodelist = parser.parse(('endcache',))\n\tparser.delete_first_token()\n\ttokens = token.split_contents()\n\tif (len(tokens) < 3):\n\t\traise TemplateSyntaxError((\"'%r'\ttag\trequires\tat\tleast\t2\targuments.\" % tokens[0]))\n\tif ((len(tokens) > 3) and tokens[(-1)].startswith('using=')):\n\t\tcache_name = parser.compile_filter(tokens[(-1)][len('using='):])\n\t\ttokens = tokens[:(-1)]\n\telse:\n\t\tcache_name = None\n\treturn CacheNode(nodelist, parser.compile_filter(tokens[1]), tokens[2], [parser.compile_filter(t) for t in tokens[3:]], cache_name)\n",["this will cache the contents of a template fragment for a given amount of time ."]]
["def get_jid(jid):\n\twith _get_serv(ret=None, commit=True) as cur:\n\t\tsql = 'SELECT\tid,\tfull_ret\tFROM\tsalt_returns\\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tWHERE\tjid\t=\t%s'\n\t\tcur.execute(sql, (jid,))\n\t\tdata = cur.fetchall()\n\t\tret = {}\n\t\tif data:\n\t\t\tfor (minion, full_ret) in data:\n\t\t\t\tret[minion] = full_ret\n\t\treturn ret\n",["return the information returned when the specified job id was executed ."]]
["def register(func, msg_type=None, dispatcher_name=None, active_by_default=True):\n\tif (msg_type and (msg_type not in MSG_TYPE_MAP)):\n\t\traise InvalidHandlerType(('Invalid\tmessage\ttype\t[%s]:\ttype\tshould\tbe\tin\t%s' % (msg_type, str(MSG_TYPES))))\n\thandler = Handler(func=func, name=dispatcher_name)\n\tif (msg_type is None):\n\t\t_registry[RAW_TYPE].append(handler)\n\telse:\n\t\t_registry[msg_type].append(handler)\n\tif active_by_default:\n\t\t_active.add(dispatcher_name)\n",["register language support with the manager ."]]
["def skip(reason):\n\treturn skipif(True, reason=reason)\n",["unconditionally skip a test ."]]
["def get_mapreduce_yaml(parse=parse_mapreduce_yaml):\n\tmr_yaml_path = find_mapreduce_yaml()\n\tif (not mr_yaml_path):\n\t\traise MissingYamlError()\n\tmr_yaml_file = open(mr_yaml_path)\n\ttry:\n\t\treturn parse(mr_yaml_file.read())\n\tfinally:\n\t\tmr_yaml_file.close()\n",["locates mapreduce ."]]
["def cmd(name, fun=None, arg=(), **kwargs):\n\tret = {'name': name, 'changes': {}, 'comment': '', 'result': True}\n\tif (fun is None):\n\t\tfun = name\n\tclient = salt.runner.RunnerClient(__opts__)\n\tlow = {'fun': fun, 'arg': arg, 'kwargs': kwargs}\n\tclient.cmd_async(low)\n\treturn ret\n",["run commands from __proxy__ :mod:salt ."]]
["def generate(bits, randfunc=None, e=65537):\n\tif (bits < 1024):\n\t\traise ValueError('RSA\tmodulus\tlength\tmust\tbe\t>=\t1024')\n\tif (((e % 2) == 0) or (e < 3)):\n\t\traise ValueError('RSA\tpublic\texponent\tmust\tbe\ta\tpositive,\todd\tinteger\tlarger\tthan\t2.')\n\tif (randfunc is None):\n\t\trandfunc = Random.get_random_bytes\n\td = n = Integer(1)\n\te = Integer(e)\n\twhile ((n.size_in_bits() != bits) and (d < (1 << (bits \/\/ 2)))):\n\t\tsize_q = (bits \/\/ 2)\n\t\tsize_p = (bits - size_q)\n\t\tmin_p = min_q = (Integer(1) << ((2 * size_q) - 1)).sqrt()\n\t\tif (size_q != size_p):\n\t\t\tmin_p = (Integer(1) << ((2 * size_p) - 1)).sqrt()\n\t\tdef filter_p(candidate):\n\t\t\treturn ((candidate > min_p) and ((candidate - 1).gcd(e) == 1))\n\t\tp = generate_probable_prime(exact_bits=size_p, randfunc=randfunc, prime_filter=filter_p)\n\t\tmin_distance = (Integer(1) << ((bits \/\/ 2) - 100))\n\t\tdef filter_q(candidate):\n\t\t\treturn ((candidate > min_q) and ((candidate - 1).gcd(e) == 1) and (abs((candidate - p)) > min_distance))\n\t\tq = generate_probable_prime(exact_bits=size_q, randfunc=randfunc, prime_filter=filter_q)\n\t\tn = (p * q)\n\t\tlcm = (p - 1).lcm((q - 1))\n\t\td = e.inverse(lcm)\n\tif (p > q):\n\t\t(p, q) = (q, p)\n\tu = p.inverse(q)\n\treturn RsaKey(n=n, e=e, d=d, p=p, q=q, u=u)\n",["generate the python source for a node tree ."]]
["def register_treebuilders_from(module):\n\tthis_module = sys.modules['bs4.builder']\n\tfor name in module.__all__:\n\t\tobj = getattr(module, name)\n\t\tif issubclass(obj, TreeBuilder):\n\t\t\tsetattr(this_module, name, obj)\n\t\t\tthis_module.__all__.append(name)\n\t\t\tthis_module.builder_registry.register(obj)\n",["copy treebuilders from the given module into this module ."]]
["def normalize_excludes(rootpath, excludes):\n\treturn [path.abspath(exclude) for exclude in excludes]\n",["normalize the excluded directory list: * must be either an absolute path or start with rootpath ."]]
["@conf.commands.register\ndef srflood(x, promisc=None, filter=None, iface=None, nofilter=None, *args, **kargs):\n\ts = conf.L3socket(promisc=promisc, filter=filter, iface=iface, nofilter=nofilter)\n\tr = sndrcvflood(s, x, *args, **kargs)\n\ts.close()\n\treturn r\n",["flood and receive packets at layer 3 prn: function applied to packets received ."]]
["def dumps(obj, *args, **kwargs):\n\tjson_options = kwargs.pop('json_options', DEFAULT_JSON_OPTIONS)\n\treturn json.dumps(_json_convert(obj, json_options), *args, **kwargs)\n",["serialize obj to a json formatted str by using the applications configured encoder if there is an application on the stack ."]]
["def matchSetStrength(match_set, target_set):\n\tsum = 0.0\n\tfor t in target_set:\n\t\tsum += max((matchStrength(m, t) for m in match_set))\n\treturn ((sum \/ len(target_set)),)\n",["compute the match strength of a set of strings on the target set of strings ."]]
["def list_nodes_select(call=None):\n\tif (call == 'action'):\n\t\traise SaltCloudSystemExit('The\tlist_nodes_select\tfunction\tmust\tbe\tcalled\twith\t-f\tor\t--function.')\n\tret = {}\n\tvm_properties = []\n\tselection = __opts__.get('query.selection')\n\tif (not selection):\n\t\traise SaltCloudSystemExit('query.selection\tnot\tfound\tin\t\/etc\/salt\/cloud')\n\tif ('id' in selection):\n\t\tvm_properties.append('name')\n\tif ('image' in selection):\n\t\tvm_properties.append('config.guestFullName')\n\tif ('size' in selection):\n\t\tvm_properties.extend(['config.hardware.numCPU', 'config.hardware.memoryMB'])\n\tif ('state' in selection):\n\t\tvm_properties.append('summary.runtime.powerState')\n\tif (('private_ips' in selection) or ('networks' in selection)):\n\t\tvm_properties.append('guest.net')\n\tif (('devices' in selection) or ('mac_address' in selection) or ('mac_addresses' in selection)):\n\t\tvm_properties.append('config.hardware.device')\n\tif ('storage' in selection):\n\t\tvm_properties.extend(['config.hardware.device', 'summary.storage.committed', 'summary.storage.uncommitted', 'summary.storage.unshared'])\n\tif ('files' in selection):\n\t\tvm_properties.append('layoutEx.file')\n\tif ('guest_id' in selection):\n\t\tvm_properties.append('config.guestId')\n\tif ('hostname' in selection):\n\t\tvm_properties.append('guest.hostName')\n\tif ('path' in selection):\n\t\tvm_properties.append('config.files.vmPathName')\n\tif ('tools_status' in selection):\n\t\tvm_properties.append('guest.toolsStatus')\n\tif (not vm_properties):\n\t\treturn {}\n\telif ('name' not in vm_properties):\n\t\tvm_properties.append('name')\n\tvm_list = salt.utils.vmware.get_mors_with_properties(_get_si(), vim.VirtualMachine, vm_properties)\n\tfor vm in vm_list:\n\t\tret[vm['name']] = _format_instance_info_select(vm, selection)\n\treturn ret\n",["return a list of the vms that are on the provider ."]]
["def to_basestring(value):\n\tif isinstance(value, _BASESTRING_TYPES):\n\t\treturn value\n\tif (not isinstance(value, bytes)):\n\t\traise TypeError(('Expected\tbytes,\tunicode,\tor\tNone;\tgot\t%r' % type(value)))\n\treturn value.decode('utf-8')\n",["converts a string argument to a subclass of basestring ."]]
["def binary_erosion(input, structure=None, iterations=1, mask=None, output=None, border_value=0, origin=0, brute_force=False):\n\treturn _binary_erosion(input, structure, iterations, mask, output, border_value, origin, 0, brute_force)\n",["return fast binary morphological erosion of an image ."]]
["@csrf_protect\n@permission_required('comments.can_moderate')\ndef delete(request, comment_id, next=None):\n\tcomment = get_object_or_404(comments.get_model(), pk=comment_id, site__pk=settings.SITE_ID)\n\tif (request.method == 'POST'):\n\t\tperform_delete(request, comment)\n\t\treturn next_redirect(request, next, delete_done, c=comment.pk)\n\telse:\n\t\treturn render_to_response('comments\/delete.html', {'comment': comment, 'next': next}, template.RequestContext(request))\n",["remove the named group cli example: ."]]
["def _unpack_tarfile(filename, extract_dir):\n\ttry:\n\t\ttarobj = tarfile.open(filename)\n\texcept tarfile.TarError:\n\t\traise ReadError(('%s\tis\tnot\ta\tcompressed\tor\tuncompressed\ttar\tfile' % filename))\n\ttry:\n\t\ttarobj.extractall(extract_dir)\n\tfinally:\n\t\ttarobj.close()\n",["unpack tar\/tar ."]]
["def identityRequest():\n\ta = TpPd(pd=3)\n\tb = MessageType(mesType=21)\n\tc = IdentityType2AndforceToStandby()\n\tpacket = ((a \/ b) \/ c)\n\treturn packet\n",["identity request section 9 ."]]
["def course_certificate():\n\tmode = session.s3.hrm.mode\n\tdef prep(r):\n\t\tif (mode is not None):\n\t\t\tauth.permission.fail()\n\t\treturn True\n\ts3.prep = prep\n\treturn s3_rest_controller('hrm', resourcename)\n",["courses to certificates controller ."]]
["def jellyToSource(obj, file=None):\n\taot = jellyToAOT(obj)\n\tif file:\n\t\tfile.write(getSource(aot).encode('utf-8'))\n\telse:\n\t\treturn getSource(aot)\n",["pass me an object and ."]]
["def get_version(package):\n\tinit_py = open(os.path.join(package, '__init__.py')).read()\n\treturn re.search('__version__\t=\t[\\'\"]([^\\'\"]+)[\\'\"]', init_py).group(1)\n",["return package version as listed in __version__ in init ."]]
["def idzr_svd(A, k):\n\tA = np.asfortranarray(A)\n\t(U, V, S, ier) = _id.idzr_svd(A, k)\n\tif ier:\n\t\traise _RETCODE_ERROR\n\treturn (U, V, S)\n",["compute svd of a complex matrix to a specified rank ."]]
["def _integrate_plugins():\n\tfrom airflow.plugins_manager import hooks_modules\n\tfor hooks_module in hooks_modules:\n\t\tsys.modules[hooks_module.__name__] = hooks_module\n\t\tglobals()[hooks_module._name] = hooks_module\n\t\tif (not _os.environ.get('AIRFLOW_USE_NEW_IMPORTS', False)):\n\t\t\tfrom zope.deprecation import deprecated as _deprecated\n\t\t\tfor _hook in hooks_module._objects:\n\t\t\t\thook_name = _hook.__name__\n\t\t\t\tglobals()[hook_name] = _hook\n\t\t\t\t_deprecated(hook_name, \"Importing\tplugin\thook\t'{i}'\tdirectly\tfrom\t'airflow.hooks'\thas\tbeen\tdeprecated.\tPlease\timport\tfrom\t'airflow.hooks.[plugin_module]'\tinstead.\tSupport\tfor\tdirect\timports\twill\tbe\tdropped\tentirely\tin\tAirflow\t2.0.\".format(i=hook_name))\n",["integrate plugins to the context ."]]
["def _wait_until_complete(operation, max_attempts=5):\n\tdef _operation_complete(result):\n\t\treturn result\n\tretry = RetryResult(_operation_complete, max_tries=max_attempts)\n\treturn retry(operation.poll)()\n",["wait until an operation has completed ."]]
["def is_HDN(text):\n\tif IPV4_RE.search(text):\n\t\treturn False\n\tif (text == ''):\n\t\treturn False\n\tif ((text[0] == '.') or (text[(-1)] == '.')):\n\t\treturn False\n\treturn True\n",["return true if text is a host domain name ."]]
["def stream_request_body(cls):\n\tif (not issubclass(cls, RequestHandler)):\n\t\traise TypeError('expected\tsubclass\tof\tRequestHandler,\tgot\t%r', cls)\n\tcls._stream_request_body = True\n\treturn cls\n",["apply to requesthandler subclasses to enable streaming body support ."]]
["def getTricomplextranslate(transformWords):\n\ttranslate = euclidean.getComplexByWords(transformWords)\n\treturn [complex(1.0, 0.0), complex(0.0, 1.0), translate]\n",["get matrixsvg by transformwords ."]]
["def shlex_quote(s):\n\tif (not s):\n\t\treturn \"''\"\n\tif (_find_unsafe(s) is None):\n\t\treturn s\n\treturn ((\"'\" + s.replace(\"'\", '\\'\"\\'\"\\'')) + \"'\")\n",["return a shell-escaped version of the string *s* ."]]
["def test_sample_wt_fit():\n\tratio = 'auto'\n\tee = EasyEnsemble(ratio=ratio, random_state=RND_SEED)\n\tassert_raises(RuntimeError, ee.sample, X, Y)\n",["test either if an error is raised when sample is called before fitting ."]]
["def _test_args():\n\timport pandas as pd\n\treturn {'start': pd.Timestamp('2004', tz='utc'), 'end': pd.Timestamp('2008', tz='utc')}\n",["extra arguments to use when ziplines automated tests run this example ."]]
["def warn_exception(func, *args, **kwargs):\n\ttry:\n\t\treturn func(*args, **kwargs)\n\texcept:\n\t\twarn((\"%s('%s')\tignored\" % sys.exc_info()[0:2]))\n",["executes the given function ."]]
["@environmentfilter\ndef do_last(environment, seq):\n\ttry:\n\t\treturn iter(reversed(seq)).next()\n\texcept StopIteration:\n\t\treturn environment.undefined('No\tlast\titem,\tsequence\twas\tempty.')\n",["return the last item of a sequence ."]]
["def getManipulatedPaths(close, elementNode, loop, prefix, sideLength):\n\tif (len(loop) < 1):\n\t\treturn [[]]\n\tderivation = BottomDerivation(elementNode, prefix)\n\ttargetMatrix = matrix.getBranchMatrixSetElementNode(elementNode)\n\ttransformedLoop = matrix.getTransformedVector3s(matrix.getIdentityTetragrid(targetMatrix.tetragrid), loop)\n\tlift = ((derivation.altitude + derivation.getAdditionalPathLift()) - euclidean.getBottomByPath(transformedLoop))\n\tfor point in loop:\n\t\tpoint.z += lift\n\treturn [loop]\n",["get equated paths ."]]
["def test_ascii_dash():\n\to = nikola.utils.slugify(u'hello-world', lang=u'en')\n\tassert (o == u'hello-world')\n\tassert isinstance(o, nikola.utils.unicode_str)\n",["test an ascii string ."]]
["def isvector(X):\n\twarnings.warn('isvector\thas\tbeen\tmoved\tto\tmatplotlib.mlab\t--\tplease\timport\tit\tfrom\tthere', DeprecationWarning)\n\timport matplotlib.mlab as mlab\n\treturn mlab.isvector(x, y, xi, extrap=extrap)\n",["this function has been moved to matplotlib ."]]
["def storage_directory(datadir, partition, name_hash):\n\treturn os.path.join(datadir, str(partition), name_hash[(-3):], name_hash)\n",["get the storage directory ."]]
["def is_list(value):\n\treturn isinstance(value, list)\n",["return true if the object is iterable ."]]
["def read_random_int(nbits):\n\trandomdata = read_random_bits(nbits)\n\tvalue = transform.bytes2int(randomdata)\n\tvalue |= (1 << (nbits - 1))\n\treturn value\n",["reads a random integer of approximately nbits bits rounded up to whole bytes ."]]
["@register.tag\ndef verbatim(parser, token):\n\tnodelist = parser.parse(('endverbatim',))\n\tparser.delete_first_token()\n\treturn VerbatimNode(nodelist.render(Context()))\n",["stops the template engine from rendering the contents of this block tag ."]]
["def isabs(s):\n\ts = splitdrive(s)[1]\n\treturn ((s != '') and (s[:1] in '\/\\\\'))\n",["test whether a path is absolute ."]]
["def disable(name, lbn, target, profile='default', tgt_type='glob', expr_form=None):\n\tif (expr_form is not None):\n\t\tsalt.utils.warn_until('Fluorine', \"the\ttarget\ttype\tshould\tbe\tpassed\tusing\tthe\t'tgt_type'\targument\tinstead\tof\t'expr_form'.\tSupport\tfor\tusing\t'expr_form'\twill\tbe\tremoved\tin\tSalt\tFluorine.\")\n\t\ttgt_type = expr_form\n\treturn _talk2modjk(name, lbn, target, 'worker_disable', profile, tgt_type)\n",["disable the named service to start at boot cli example: ."]]
["def fnmatch(name, globs):\n\tglobs = ((globs,) if isinstance(globs, str) else tuple(globs))\n\tif (len(globs) == 0):\n\t\treturn True\n\tname = os.path.normcase(name)\n\treturn any((compiled_pattern.match(name) for glob in globs for compiled_pattern in _compile_pattern(glob)))\n",["test whether filename matches pattern ."]]
["def educateSingleBackticks(str):\n\tstr = re.sub('`', '&#8216;', str)\n\tstr = re.sub(\"'\", '&#8217;', str)\n\treturn str\n",["parameter: string ."]]
["def _fake_run_horcmstart2(*args):\n\treturn (0 if (not run_horcmstart_returns_error2) else 3)\n",["return a value based on a flag value ."]]
["def urlquote(val):\n\tif (val is None):\n\t\treturn ''\n\tif (not isinstance(val, unicode)):\n\t\tval = str(val)\n\telse:\n\t\tval = val.encode('utf-8')\n\treturn urllib.quote(val)\n",["a version of pythons urllib ."]]
["def head(url, **kwargs):\n\tkwargs.setdefault('headers', dict(useragent_header))\n\tconfig = kwargs.pop('config', None)\n\tif config:\n\t\tkwargs.setdefault('verify', _get_tls_cacert(url, config))\n\twith warnings.catch_warnings():\n\t\tif (not kwargs.get('verify')):\n\t\t\twarnings.filterwarnings('ignore', category=InsecureRequestWarning)\n\t\treturn requests.get(url, **kwargs)\n",["sends a head request ."]]
["def regions():\n\tfrom boto.cloudtrail.layer1 import CloudTrailConnection\n\treturn get_regions('cloudtrail', connection_cls=CloudTrailConnection)\n",["get all available regions for the ec2 service ."]]
["def compute_node_get_all(context):\n\treturn IMPL.compute_node_get_all(context)\n",["get all computenodes ."]]
["def processXMLElement(xmlElement):\n\tgroup.processShape(Difference, xmlElement)\n",["process the xml element ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load if the influxdb module is available ."]]
["def getNewDerivation(elementNode):\n\treturn evaluate.EmptyObject(elementNode)\n",["get new derivation ."]]
["def decompress(fileobj, path):\n\tif path.endswith('.gz'):\n\t\treturn gunzip_stream(fileobj)\n\telif path.endswith('.bz2'):\n\t\tif (bz2 is None):\n\t\t\traise Exception('bz2\tmodule\twas\tnot\tsuccessfully\timported\t(likely\tnot\tinstalled).')\n\t\telse:\n\t\t\treturn bunzip2_stream(fileobj)\n\telse:\n\t\treturn fileobj\n",["decompress compressed text ."]]
["def _b64encode(s):\n\treturn b2a_base64(s).strip()\n",["encode a binary string as base64 with no trailing newline ."]]
["def register(func, msg_type=None, dispatcher_name=None, active_by_default=True):\n\tif (msg_type and (msg_type not in MSG_TYPE_MAP)):\n\t\traise InvalidHandlerType(('Invalid\tmessage\ttype\t[%s]:\ttype\tshould\tbe\tin\t%s' % (msg_type, str(MSG_TYPES))))\n\thandler = Handler(func=func, name=dispatcher_name)\n\tif (msg_type is None):\n\t\t_registry[RAW_TYPE].append(handler)\n\telse:\n\t\t_registry[msg_type].append(handler)\n\tif active_by_default:\n\t\t_active.add(dispatcher_name)\n",["required method to auto register this checker ."]]
["def norm(x):\n\treturn sqrt(squared_norm(x))\n",["dot product-based euclidean norm implementation see: url ."]]
["def reload():\n\tbrowser.reload()\n\treturn browser.get_url()\n",["reload a service ."]]
["def getSegmentsFromLoopListsPoints(loopLists, pointBegin, pointEnd):\n\tnormalizedSegment = (pointEnd - pointBegin)\n\tnormalizedSegmentLength = abs(normalizedSegment)\n\tif (normalizedSegmentLength == 0.0):\n\t\treturn []\n\tnormalizedSegment \/= normalizedSegmentLength\n\tsegmentYMirror = complex(normalizedSegment.real, (- normalizedSegment.imag))\n\tpointBeginRotated = (segmentYMirror * pointBegin)\n\tpointEndRotated = (segmentYMirror * pointEnd)\n\trotatedLoopLists = []\n\tfor loopList in loopLists:\n\t\trotatedLoopList = []\n\t\trotatedLoopLists.append(rotatedLoopList)\n\t\tfor loop in loopList:\n\t\t\trotatedLoop = euclidean.getPointsRoundZAxis(segmentYMirror, loop)\n\t\t\trotatedLoopList.append(rotatedLoop)\n\txIntersectionIndexList = []\n\txIntersectionIndexList.append(euclidean.XIntersectionIndex((-1), pointBeginRotated.real))\n\txIntersectionIndexList.append(euclidean.XIntersectionIndex((-1), pointEndRotated.real))\n\teuclidean.addXIntersectionIndexesFromLoopListsY(rotatedLoopLists, xIntersectionIndexList, pointBeginRotated.imag)\n\tsegments = euclidean.getSegmentsFromXIntersectionIndexes(xIntersectionIndexList, pointBeginRotated.imag)\n\tfor segment in segments:\n\t\tfor endpoint in segment:\n\t\t\tendpoint.point *= normalizedSegment\n\treturn segments\n",["get endpoint segments from the beginning and end of a line segment ."]]
["@log_call\n@utils.no_4byte_params\ndef metadef_property_create(context, namespace_name, values):\n\tglobal DATA\n\tproperty_values = copy.deepcopy(values)\n\tproperty_name = property_values['name']\n\trequired_attributes = ['name']\n\tallowed_attributes = ['name', 'description', 'json_schema', 'required']\n\tnamespace = metadef_namespace_get(context, namespace_name)\n\tfor property in DATA['metadef_properties']:\n\t\tif ((property['name'] == property_name) and (property['namespace_id'] == namespace['id'])):\n\t\t\tLOG.debug('Can\tnot\tcreate\tmetadata\tdefinition\tproperty.\tA\tproperty\twith\tname=%(name)s\talready\texists\tin\tnamespace=%(namespace_name)s.', {'name': property_name, 'namespace_name': namespace_name})\n\t\t\traise exception.MetadefDuplicateProperty(property_name=property_name, namespace_name=namespace_name)\n\tfor key in required_attributes:\n\t\tif (key not in property_values):\n\t\t\traise exception.Invalid(('%s\tis\ta\trequired\tattribute' % key))\n\tincorrect_keys = (set(property_values.keys()) - set(allowed_attributes))\n\tif incorrect_keys:\n\t\traise exception.Invalid(('The\tkeys\t%s\tare\tnot\tvalid' % str(incorrect_keys)))\n\tproperty_values['namespace_id'] = namespace['id']\n\t_check_namespace_visibility(context, namespace, namespace_name)\n\tproperty = _format_property(property_values)\n\tDATA['metadef_properties'].append(property)\n\treturn property\n",["create a metadef property ."]]
["def ssl_wrap_socket(socket, ssl_options, server_hostname=None, **kwargs):\n\tcontext = ssl_options_to_context(ssl_options)\n\tif (hasattr(ssl, 'SSLContext') and isinstance(context, ssl.SSLContext)):\n\t\tif ((server_hostname is not None) and getattr(ssl, 'HAS_SNI')):\n\t\t\treturn context.wrap_socket(socket, server_hostname=server_hostname, **kwargs)\n\t\telse:\n\t\t\treturn context.wrap_socket(socket, **kwargs)\n\telse:\n\t\treturn ssl.wrap_socket(socket, **dict(context, **kwargs))\n",["all arguments except for server_hostname ."]]
["def cache_page(*args, **kwargs):\n\tcache_alias = kwargs.pop('cache', None)\n\tkey_prefix = kwargs.pop('key_prefix', None)\n\tassert (not kwargs), 'The\tonly\tkeyword\targuments\tare\tcache\tand\tkey_prefix'\n\tdef warn():\n\t\timport warnings\n\t\twarnings.warn('The\tcache_page\tdecorator\tmust\tbe\tcalled\tlike:\tcache_page(timeout,\t[cache=cache\tname],\t[key_prefix=key\tprefix]).\tAll\tother\tways\tare\tdeprecated.', PendingDeprecationWarning, stacklevel=3)\n\tif (len(args) > 1):\n\t\tassert (len(args) == 2), 'cache_page\taccepts\tat\tmost\t2\targuments'\n\t\twarn()\n\t\tif callable(args[0]):\n\t\t\treturn decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[1], cache_alias=cache_alias, key_prefix=key_prefix)(args[0])\n\t\telif callable(args[1]):\n\t\t\treturn decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[0], cache_alias=cache_alias, key_prefix=key_prefix)(args[1])\n\t\telse:\n\t\t\tassert False, 'cache_page\tmust\tbe\tpassed\ta\tview\tfunction\tif\tcalled\twith\ttwo\targuments'\n\telif (len(args) == 1):\n\t\tif callable(args[0]):\n\t\t\twarn()\n\t\t\treturn decorator_from_middleware_with_args(CacheMiddleware)(cache_alias=cache_alias, key_prefix=key_prefix)(args[0])\n\t\telse:\n\t\t\treturn decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[0], cache_alias=cache_alias, key_prefix=key_prefix)\n\telse:\n\t\twarn()\n\t\treturn decorator_from_middleware_with_args(CacheMiddleware)(cache_alias=cache_alias, key_prefix=key_prefix)\n",["decorator for views that tries getting the page from the cache and populates the cache if the page isnt in the cache yet ."]]
["def make_query_from_filter(sample_filter, require_meter=True):\n\tq = {}\n\tif sample_filter.user:\n\t\tq['user_id'] = sample_filter.user\n\tif sample_filter.project:\n\t\tq['project_id'] = sample_filter.project\n\tif sample_filter.meter:\n\t\tq['counter_name'] = sample_filter.meter\n\telif require_meter:\n\t\traise RuntimeError('Missing\trequired\tmeter\tspecifier')\n\tts_range = make_timestamp_range(sample_filter.start_timestamp, sample_filter.end_timestamp, sample_filter.start_timestamp_op, sample_filter.end_timestamp_op)\n\tif ts_range:\n\t\tq['timestamp'] = ts_range\n\tif sample_filter.resource:\n\t\tq['resource_id'] = sample_filter.resource\n\tif sample_filter.source:\n\t\tq['source'] = sample_filter.source\n\tif sample_filter.message_id:\n\t\tq['message_id'] = sample_filter.message_id\n\tq.update(dict(((('resource_%s' % k), v) for (k, v) in six.iteritems(improve_keys(sample_filter.metaquery, metaquery=True)))))\n\treturn q\n",["return a query dictionary based on the settings in the filter ."]]
["def parse_args(arguments, apply_config=False):\n\tparser = create_parser()\n\targs = parser.parse_args(arguments)\n\tif ((not args.files) and (not args.list_fixes)):\n\t\tparser.error(u'incorrect\tnumber\tof\targuments')\n\targs.files = [decode_filename(name) for name in args.files]\n\tif apply_config:\n\t\tparser = read_config(args, parser)\n\t\targs = parser.parse_args(arguments)\n\t\targs.files = [decode_filename(name) for name in args.files]\n\tif (u'-' in args.files):\n\t\tif (len(args.files) > 1):\n\t\t\tparser.error(u'cannot\tmix\tstdin\tand\tregular\tfiles')\n\t\tif args.diff:\n\t\t\tparser.error(u'--diff\tcannot\tbe\tused\twith\tstandard\tinput')\n\t\tif args.in_place:\n\t\t\tparser.error(u'--in-place\tcannot\tbe\tused\twith\tstandard\tinput')\n\t\tif args.recursive:\n\t\t\tparser.error(u'--recursive\tcannot\tbe\tused\twith\tstandard\tinput')\n\tif ((len(args.files) > 1) and (not (args.in_place or args.diff))):\n\t\tparser.error(u'autopep8\tonly\ttakes\tone\tfilename\tas\targument\tunless\tthe\t\"--in-place\"\tor\t\"--diff\"\targs\tare\tused')\n\tif (args.recursive and (not (args.in_place or args.diff))):\n\t\tparser.error(u'--recursive\tmust\tbe\tused\twith\t--in-place\tor\t--diff')\n\tif (args.in_place and args.diff):\n\t\tparser.error(u'--in-place\tand\t--diff\tare\tmutually\texclusive')\n\tif (args.max_line_length <= 0):\n\t\tparser.error(u'--max-line-length\tmust\tbe\tgreater\tthan\t0')\n\tif args.select:\n\t\targs.select = _split_comma_separated(args.select)\n\tif args.ignore:\n\t\targs.ignore = _split_comma_separated(args.ignore)\n\telif (not args.select):\n\t\tif args.aggressive:\n\t\t\targs.select = [u'E', u'W']\n\t\telse:\n\t\t\targs.ignore = _split_comma_separated(DEFAULT_IGNORE)\n\tif args.exclude:\n\t\targs.exclude = _split_comma_separated(args.exclude)\n\telse:\n\t\targs.exclude = []\n\tif (args.jobs < 1):\n\t\timport multiprocessing\n\t\targs.jobs = multiprocessing.cpu_count()\n\tif ((args.jobs > 1) and (not args.in_place)):\n\t\tparser.error(u'parallel\tjobs\trequires\t--in-place')\n\tif args.line_range:\n\t\tif (args.line_range[0] <= 0):\n\t\t\tparser.error(u'--range\tmust\tbe\tpositive\tnumbers')\n\t\tif (args.line_range[0] > args.line_range[1]):\n\t\t\tparser.error(u'First\tvalue\tof\t--range\tshould\tbe\tless\tthan\tor\tequal\tto\tthe\tsecond')\n\treturn args\n",["parse input arguments ."]]
["def get_script_header(script_text, executable=sys_executable, wininst=False):\n\tfrom distutils.command.build_scripts import first_line_re\n\tfirst = (script_text + '\\n').splitlines()[0]\n\tmatch = first_line_re.match(first)\n\toptions = ''\n\tif match:\n\t\toptions = (match.group(1) or '')\n\t\tif options:\n\t\t\toptions = ('\t' + options)\n\tif wininst:\n\t\texecutable = 'python.exe'\n\telse:\n\t\texecutable = nt_quote_arg(executable)\n\thdr = ('#!%(executable)s%(options)s\\n' % locals())\n\tif (unicode(hdr, 'ascii', 'ignore').encode('ascii') != hdr):\n\t\tif options:\n\t\t\tif options.strip().startswith('-'):\n\t\t\t\toptions = ('\t-x' + options.strip()[1:])\n\t\telse:\n\t\t\toptions = '\t-x'\n\texecutable = fix_jython_executable(executable, options)\n\thdr = ('#!%(executable)s%(options)s\\n' % locals())\n\treturn hdr\n",["create a #! line ."]]
["def test_drop_channels_mixin():\n\t(raw, events) = _get_data()[:2]\n\tepochs = Epochs(raw, events, event_id, tmin, tmax, preload=True)\n\tdrop_ch = epochs.ch_names[:3]\n\tch_names = epochs.ch_names[3:]\n\tch_names_orig = epochs.ch_names\n\tdummy = epochs.copy().drop_channels(drop_ch)\n\tassert_equal(ch_names, dummy.ch_names)\n\tassert_equal(ch_names_orig, epochs.ch_names)\n\tassert_equal(len(ch_names_orig), epochs.get_data().shape[1])\n\tepochs.drop_channels(drop_ch)\n\tassert_equal(ch_names, epochs.ch_names)\n\tassert_equal(len(ch_names), epochs.get_data().shape[1])\n",["test channels-dropping functionality ."]]
["def learn_cache_key(request, response, cache_timeout=None, key_prefix=None, cache=None):\n\tif (key_prefix is None):\n\t\tkey_prefix = settings.CACHE_MIDDLEWARE_KEY_PREFIX\n\tif (cache_timeout is None):\n\t\tcache_timeout = settings.CACHE_MIDDLEWARE_SECONDS\n\tcache_key = _generate_cache_header_key(key_prefix, request)\n\tif (cache is None):\n\t\tcache = get_cache(settings.CACHE_MIDDLEWARE_ALIAS)\n\tif response.has_header('Vary'):\n\t\theaderlist = [('HTTP_' + header.upper().replace('-', '_')) for header in cc_delim_re.split(response['Vary'])]\n\t\tcache.set(cache_key, headerlist, cache_timeout)\n\t\treturn _generate_cache_key(request, request.method, headerlist, key_prefix)\n\telse:\n\t\tcache.set(cache_key, [], cache_timeout)\n\t\treturn _generate_cache_key(request, request.method, [], key_prefix)\n",["learns what headers to take into account for some request path from the response object ."]]
["def DocTestSuite(module=None, globs=None, extraglobs=None, test_finder=None, **options):\n\tif (test_finder is None):\n\t\ttest_finder = DocTestFinder()\n\tmodule = _normalize_module(module)\n\ttests = test_finder.find(module, globs=globs, extraglobs=extraglobs)\n\tif (globs is None):\n\t\tglobs = module.__dict__\n\tif (not tests):\n\t\traise ValueError(module, 'has\tno\ttests')\n\ttests.sort()\n\tsuite = unittest.TestSuite()\n\tfor test in tests:\n\t\tif (len(test.examples) == 0):\n\t\t\tcontinue\n\t\tif (not test.filename):\n\t\t\tfilename = module.__file__\n\t\t\tif (filename[(-4):] in ('.pyc', '.pyo')):\n\t\t\t\tfilename = filename[:(-1)]\n\t\t\ttest.filename = filename\n\t\tsuite.addTest(DocTestCase(test, **options))\n\treturn suite\n",["convert doctest tests for a module to a unittest test suite ."]]
["def _dnsname_match(dn, hostname, max_wildcards=1):\n\tpats = []\n\tif (not dn):\n\t\treturn False\n\tparts = dn.split('.')\n\tleftmost = parts[0]\n\twildcards = leftmost.count('*')\n\tif (wildcards > max_wildcards):\n\t\traise CertificateError(('too\tmany\twildcards\tin\tcertificate\tDNS\tname:\t' + repr(dn)))\n\tif (not wildcards):\n\t\treturn (dn.lower() == hostname.lower())\n\tif (leftmost == '*'):\n\t\tpats.append('[^.]+')\n\telif (leftmost.startswith('xn--') or hostname.startswith('xn--')):\n\t\tpats.append(re.escape(leftmost))\n\telse:\n\t\tpats.append(re.escape(leftmost).replace('\\\\*', '[^.]*'))\n\tfor frag in parts[1:]:\n\t\tpats.append(re.escape(frag))\n\tpat = re.compile((('\\\\A' + '\\\\.'.join(pats)) + '\\\\Z'), re.IGNORECASE)\n\treturn pat.match(hostname)\n",["matching according to rfc 6125 ."]]
["def getDOMImplementation(name=None, features=()):\n\timport os\n\tcreator = None\n\tmod = well_known_implementations.get(name)\n\tif mod:\n\t\tmod = __import__(mod, {}, {}, ['getDOMImplementation'])\n\t\treturn mod.getDOMImplementation()\n\telif name:\n\t\treturn registered[name]()\n\telif ('PYTHON_DOM' in os.environ):\n\t\treturn getDOMImplementation(name=os.environ['PYTHON_DOM'])\n\tif isinstance(features, str):\n\t\tfeatures = _parse_feature_string(features)\n\tfor creator in registered.values():\n\t\tdom = creator()\n\t\tif _good_enough(dom, features):\n\t\t\treturn dom\n\tfor creator in well_known_implementations.keys():\n\t\ttry:\n\t\t\tdom = getDOMImplementation(name=creator)\n\t\texcept Exception:\n\t\t\tcontinue\n\t\tif _good_enough(dom, features):\n\t\t\treturn dom\n\traise ImportError('no\tsuitable\tDOM\timplementation\tfound')\n",["getdomimplementation(name = none ."]]
["@require_role(role='super')\ndef asset_edit(request):\n\t(header_title, path1, path2) = (u'\\u4fee\\u6539\\u8d44\\u4ea7', u'\\u8d44\\u4ea7\\u7ba1\\u7406', u'\\u4fee\\u6539\\u8d44\\u4ea7')\n\tasset_id = request.GET.get('id', '')\n\tusername = request.user.username\n\tasset = get_object(Asset, id=asset_id)\n\tif asset:\n\t\tpassword_old = asset.password\n\taf = AssetForm(instance=asset)\n\tif (request.method == 'POST'):\n\t\taf_post = AssetForm(request.POST, instance=asset)\n\t\tip = request.POST.get('ip', '')\n\t\thostname = request.POST.get('hostname', '')\n\t\tpassword = request.POST.get('password', '')\n\t\tis_active = (True if (request.POST.get('is_active') == '1') else False)\n\t\tuse_default_auth = request.POST.get('use_default_auth', '')\n\t\ttry:\n\t\t\tasset_test = get_object(Asset, hostname=hostname)\n\t\t\tif (asset_test and (asset_id != unicode(asset_test.id))):\n\t\t\t\temg = (u'\\u8be5\\u4e3b\\u673a\\u540d\t%s\t\\u5df2\\u5b58\\u5728!' % hostname)\n\t\t\t\traise ServerError(emg)\n\t\t\tif (len(hostname) > 54):\n\t\t\t\temg = u'\\u4e3b\\u673a\\u540d\\u957f\\u5ea6\\u4e0d\\u80fd\\u8d85\\u8fc754\\u4f4d!'\n\t\t\t\traise ServerError(emg)\n\t\t\telif af_post.is_valid():\n\t\t\t\taf_save = af_post.save(commit=False)\n\t\t\t\tif use_default_auth:\n\t\t\t\t\taf_save.username = ''\n\t\t\t\t\taf_save.password = ''\n\t\t\t\telif password:\n\t\t\t\t\tpassword_encode = CRYPTOR.encrypt(password)\n\t\t\t\t\taf_save.password = password_encode\n\t\t\t\telse:\n\t\t\t\t\taf_save.password = password_old\n\t\t\t\taf_save.is_active = (True if is_active else False)\n\t\t\t\taf_save.save()\n\t\t\t\taf_post.save_m2m()\n\t\t\t\tinfo = asset_diff(af_post.__dict__.get('initial'), request.POST)\n\t\t\t\tdb_asset_alert(asset, username, info)\n\t\t\t\tsmg = (u'\\u4e3b\\u673a\t%s\t\\u4fee\\u6539\\u6210\\u529f' % ip)\n\t\t\telse:\n\t\t\t\temg = (u'\\u4e3b\\u673a\t%s\t\\u4fee\\u6539\\u5931\\u8d25' % ip)\n\t\t\t\traise ServerError(emg)\n\t\texcept ServerError as e:\n\t\t\terror = e.message\n\t\t\treturn my_render('jasset\/asset_edit.html', locals(), request)\n\t\treturn HttpResponseRedirect((reverse('asset_detail') + ('?id=%s' % asset_id)))\n\treturn my_render('jasset\/asset_edit.html', locals(), request)\n",["asset edit page ."]]
["def serve_file(path, content_type=None, disposition=None, name=None, debug=False):\n\tresponse = cherrypy.serving.response\n\tif (not os.path.isabs(path)):\n\t\tmsg = (\"'%s'\tis\tnot\tan\tabsolute\tpath.\" % path)\n\t\tif debug:\n\t\t\tcherrypy.log(msg, 'TOOLS.STATICFILE')\n\t\traise ValueError(msg)\n\ttry:\n\t\tst = os.stat(path)\n\texcept (OSError, TypeError, ValueError):\n\t\tif debug:\n\t\t\tcherrypy.log(('os.stat(%r)\tfailed' % path), 'TOOLS.STATIC')\n\t\traise cherrypy.NotFound()\n\tif stat.S_ISDIR(st.st_mode):\n\t\tif debug:\n\t\t\tcherrypy.log(('%r\tis\ta\tdirectory' % path), 'TOOLS.STATIC')\n\t\traise cherrypy.NotFound()\n\tresponse.headers['Last-Modified'] = httputil.HTTPDate(st.st_mtime)\n\tcptools.validate_since()\n\tif (content_type is None):\n\t\text = ''\n\t\ti = path.rfind('.')\n\t\tif (i != (-1)):\n\t\t\text = path[i:].lower()\n\t\tcontent_type = mimetypes.types_map.get(ext, None)\n\tif (content_type is not None):\n\t\tresponse.headers['Content-Type'] = content_type\n\tif debug:\n\t\tcherrypy.log(('Content-Type:\t%r' % content_type), 'TOOLS.STATIC')\n\tcd = None\n\tif (disposition is not None):\n\t\tif (name is None):\n\t\t\tname = os.path.basename(path)\n\t\tcd = ('%s;\tfilename=\"%s\"' % (disposition, name))\n\t\tresponse.headers['Content-Disposition'] = cd\n\tif debug:\n\t\tcherrypy.log(('Content-Disposition:\t%r' % cd), 'TOOLS.STATIC')\n\tcontent_length = st.st_size\n\tfileobj = open(path, 'rb')\n\treturn _serve_fileobj(fileobj, content_type, content_length, debug=debug)\n",["return a chunk from a file based on the data received ."]]
["def convert_to_tgt_list_and_itor_tgt_map(zone_mapping):\n\ttarget_wwns = []\n\titor_tgt_map = {}\n\tfor san_name in zone_mapping:\n\t\tone_map = zone_mapping[san_name]\n\t\tfor target in one_map['target_port_wwn_list']:\n\t\t\tif (target not in target_wwns):\n\t\t\t\ttarget_wwns.append(target)\n\t\tfor initiator in one_map['initiator_port_wwn_list']:\n\t\t\titor_tgt_map[initiator] = one_map['target_port_wwn_list']\n\tLOG.debug('target_wwns:\t%(tgt_wwns)s\\n\tinit_targ_map:\t%(itor_tgt_map)s', {'tgt_wwns': target_wwns, 'itor_tgt_map': itor_tgt_map})\n\treturn (target_wwns, itor_tgt_map)\n",["function to process data from lookup service ."]]
["def b58decode(v, length):\n\tlong_value = 0L\n\tfor (i, c) in enumerate(v[::(-1)]):\n\t\tlong_value += (__b58chars.find(c) * (__b58base ** i))\n\tresult = ''\n\twhile (long_value >= 256):\n\t\t(div, mod) = divmod(long_value, 256)\n\t\tresult = (chr(mod) + result)\n\t\tlong_value = div\n\tresult = (chr(long_value) + result)\n\tnPad = 0\n\tfor c in v:\n\t\tif (c == __b58chars[0]):\n\t\t\tnPad += 1\n\t\telse:\n\t\t\tbreak\n\tresult = ((chr(0) * nPad) + result)\n\tif ((length is not None) and (len(result) != length)):\n\t\treturn None\n\treturn result\n",["decode v into a string of len bytes ."]]
["def get_scene_numbering_for_show(indexer_id, indexer):\n\tif (indexer_id is None):\n\t\treturn {}\n\tindexer_id = int(indexer_id)\n\tindexer = int(indexer)\n\tresult = {}\n\tfor dbData in [x[u'doc'] for x in sickrage.srCore.mainDB.db.get_many(u'scene_numbering', indexer_id, with_doc=True)]:\n\t\tseason = int((dbData[u'season'] or 0))\n\t\tepisode = int((dbData[u'episode'] or 0))\n\t\tscene_season = int((dbData[u'scene_season'] or 0))\n\t\tscene_episode = int((dbData[u'scene_episode'] or 0))\n\t\tif ((int(dbData[u'indexer']) != indexer) or ((scene_season or scene_episode) == 0)):\n\t\t\tcontinue\n\t\tresult[(season, episode)] = (scene_season, scene_episode)\n\treturn result\n",["returns a dict of : mappings for an entire show ."]]
["def path(dev):\n\treturn info(dev).get('P', None)\n",["builder for rebulk object ."]]
["def figaspect(arg):\n\tisarray = (hasattr(arg, u'shape') and (not np.isscalar(arg)))\n\tfigsize_min = np.array((4.0, 2.0))\n\tfigsize_max = np.array((16.0, 16.0))\n\tif isarray:\n\t\t(nr, nc) = arg.shape[:2]\n\t\tarr_ratio = (float(nr) \/ nc)\n\telse:\n\t\tarr_ratio = float(arg)\n\tfig_height = rcParams[u'figure.figsize'][1]\n\tnewsize = np.array(((fig_height \/ arr_ratio), fig_height))\n\tnewsize \/= min(1.0, *(newsize \/ figsize_min))\n\tnewsize \/= max(1.0, *(newsize \/ figsize_max))\n\tnewsize = np.clip(newsize, figsize_min, figsize_max)\n\treturn newsize\n",["create a figure with specified aspect ratio ."]]
["def _make_key(args, kwds, typed, kwd_mark=(object(),), fasttypes=set([int, str, frozenset, type(None)]), sorted=sorted, tuple=tuple, type=type, len=len):\n\tkey = args\n\tif kwds:\n\t\tsorted_items = sorted(kwds.items())\n\t\tkey += kwd_mark\n\t\tfor item in sorted_items:\n\t\t\tkey += item\n\tif typed:\n\t\tkey += tuple((type(v) for v in args))\n\t\tif kwds:\n\t\t\tkey += tuple((type(v) for (k, v) in sorted_items))\n\telif ((len(key) == 1) and (type(key[0]) in fasttypes)):\n\t\treturn key[0]\n\treturn _HashedSeq(key)\n",["make a cache key from optionally typed positional and keyword arguments ."]]
["def parse229(resp, peer):\n\tif (resp[:3] != '229'):\n\t\traise error_reply(resp)\n\tleft = resp.find('(')\n\tif (left < 0):\n\t\traise error_proto(resp)\n\tright = resp.find(')', (left + 1))\n\tif (right < 0):\n\t\traise error_proto(resp)\n\tif (resp[(left + 1)] != resp[(right - 1)]):\n\t\traise error_proto(resp)\n\tparts = resp[(left + 1):right].split(resp[(left + 1)])\n\tif (len(parts) != 5):\n\t\traise error_proto(resp)\n\thost = peer[0]\n\tport = int(parts[3])\n\treturn (host, port)\n",["parse the 229 response for a epsv request ."]]
["@testing.requires_testing_data\ndef test_calculate_chpi_positions_on_chpi5_in_one_second_steps():\n\tmf_quats = read_head_pos(chpi5_pos_fname)\n\traw = read_raw_fif(chpi5_fif_fname, allow_maxshield='yes')\n\traw = _decimate_chpi(raw.crop(0.0, 15.0).load_data(), decim=8)\n\tpy_quats = _calculate_chpi_positions(raw, t_step_min=1.0, t_step_max=1.0, t_window=1.0, verbose='debug')\n\t_assert_quats(py_quats, mf_quats, dist_tol=0.0008, angle_tol=0.5)\n",["comparing estimated chpi positions with mf results ."]]
["def quarantine_db(object_file, server_type):\n\tobject_dir = os.path.dirname(object_file)\n\tquarantine_dir = os.path.abspath(os.path.join(object_dir, '..', '..', '..', '..', 'quarantined', (server_type + 's'), os.path.basename(object_dir)))\n\ttry:\n\t\trenamer(object_dir, quarantine_dir, fsync=False)\n\texcept OSError as e:\n\t\tif (e.errno not in (errno.EEXIST, errno.ENOTEMPTY)):\n\t\t\traise\n\t\tquarantine_dir = ('%s-%s' % (quarantine_dir, uuid.uuid4().hex))\n\t\trenamer(object_dir, quarantine_dir, fsync=False)\n",["in the case that a corrupt file is found ."]]
["def test_skipping_setuptools_doesnt_skip_legacy(script, data):\n\tresult = script.pip('install', 'script.wheel2==0.1', '--no-index', ('--find-links=' + data.find_links), expect_error=False)\n\tlegacy_file1 = (script.bin \/ 'testscript1.bat')\n\tlegacy_file2 = (script.bin \/ 'testscript2')\n\twrapper_helper = (script.bin \/ 't1-script.py')\n\tassert (legacy_file1 in result.files_created)\n\tassert (legacy_file2 in result.files_created)\n\tassert (wrapper_helper not in result.files_created)\n",["test installing scripts ."]]
["def RegisterHelpFile(helpFile, helpPath, helpDesc=None, bCheckFile=1):\n\tif (helpDesc is None):\n\t\thelpDesc = helpFile\n\tfullHelpFile = os.path.join(helpPath, helpFile)\n\ttry:\n\t\tif bCheckFile:\n\t\t\tos.stat(fullHelpFile)\n\texcept os.error:\n\t\traise ValueError('Help\tfile\tdoes\tnot\texist')\n\twin32api.RegSetValue(GetRootKey(), (BuildDefaultPythonKey() + ('\\\\Help\\\\%s' % helpDesc)), win32con.REG_SZ, fullHelpFile)\n",["register a help file in the registry ."]]
["def addLoopXSegmentIntersections(lineLoopsIntersections, loop, segmentFirstX, segmentSecondX, segmentYMirror, y):\n\trotatedLoop = euclidean.getRotatedComplexes(segmentYMirror, loop)\n\tfor pointIndex in xrange(len(rotatedLoop)):\n\t\tpointFirst = rotatedLoop[pointIndex]\n\t\tpointSecond = rotatedLoop[((pointIndex + 1) % len(rotatedLoop))]\n\t\taddLineXSegmentIntersection(lineLoopsIntersections, segmentFirstX, segmentSecondX, pointFirst, pointSecond, y)\n",["add intersections of the loop with the x segment ."]]
["def walk(top, func, arg):\n\twarnings.warnpy3k('In\t3.x,\tos.path.walk\tis\tremoved\tin\tfavor\tof\tos.walk.')\n\ttry:\n\t\tnames = os.listdir(top)\n\texcept os.error:\n\t\treturn\n\tfunc(arg, top, names)\n\tfor name in names:\n\t\tname = join(top, name)\n\t\tif isdir(name):\n\t\t\twalk(name, func, arg)\n",["backport of os ."]]
["@register.tag('trans')\ndef do_translate(parser, token):\n\tclass TranslateParser(TokenParser, ):\n\t\tdef top(self):\n\t\t\tvalue = self.value()\n\t\t\tif (value[0] == \"'\"):\n\t\t\t\tm = re.match(\"^'([^']+)'(\\\\|.*$)\", value)\n\t\t\t\tif m:\n\t\t\t\t\tvalue = ('\"%s\"%s' % (m.group(1).replace('\"', '\\\\\"'), m.group(2)))\n\t\t\t\telif (value[(-1)] == \"'\"):\n\t\t\t\t\tvalue = ('\"%s\"' % value[1:(-1)].replace('\"', '\\\\\"'))\n\t\t\tnoop = False\n\t\t\tasvar = None\n\t\t\tmessage_context = None\n\t\t\twhile self.more():\n\t\t\t\ttag = self.tag()\n\t\t\t\tif (tag == 'noop'):\n\t\t\t\t\tnoop = True\n\t\t\t\telif (tag == 'context'):\n\t\t\t\t\tmessage_context = parser.compile_filter(self.value())\n\t\t\t\telif (tag == 'as'):\n\t\t\t\t\tasvar = self.tag()\n\t\t\t\telse:\n\t\t\t\t\traise TemplateSyntaxError('Only\toptions\tfor\t\\'trans\\'\tare\t\\'noop\\',\t\\'context\t\"xxx\"\\',\tand\t\\'as\tVAR\\'.')\n\t\t\treturn (value, noop, asvar, message_context)\n\t(value, noop, asvar, message_context) = TranslateParser(token.contents).top()\n\treturn TranslateNode(parser.compile_filter(value), noop, asvar, message_context)\n",["translates message using the given translation_function name -- which will be either gettext or ugettext ."]]
["def getCarving(fileName):\n\tpluginModule = fabmetheus_interpret.getInterpretPlugin(fileName)\n\tif (pluginModule == None):\n\t\treturn None\n\treturn pluginModule.getCarving(fileName)\n",["get the carving for the csv file ."]]
["def basicConfig(**kwargs):\n\tif (len(root.handlers) == 0):\n\t\tfilename = kwargs.get('filename')\n\t\tif filename:\n\t\t\tmode = kwargs.get('filemode', 'a')\n\t\t\thdlr = FileHandler(filename, mode)\n\t\telse:\n\t\t\tstream = kwargs.get('stream')\n\t\t\thdlr = StreamHandler(stream)\n\t\tfs = kwargs.get('format', BASIC_FORMAT)\n\t\tdfs = kwargs.get('datefmt', None)\n\t\tfmt = Formatter(fs, dfs)\n\t\thdlr.setFormatter(fmt)\n\t\troot.addHandler(hdlr)\n\t\tlevel = kwargs.get('level')\n\t\tif (level is not None):\n\t\t\troot.setLevel(level)\n",["do basic configuration for the logging system ."]]
["def from_current_timezone(value):\n\tif (settings.USE_TZ and (value is not None) and timezone.is_naive(value)):\n\t\tcurrent_timezone = timezone.get_current_timezone()\n\t\ttry:\n\t\t\treturn timezone.make_aware(value, current_timezone)\n\t\texcept Exception:\n\t\t\traise ValidationError((_(u\"%(datetime)s\tcouldn't\tbe\tinterpreted\tin\ttime\tzone\t%(current_timezone)s;\tit\tmay\tbe\tambiguous\tor\tit\tmay\tnot\texist.\") % {u'datetime': value, u'current_timezone': current_timezone}))\n\treturn value\n",["when time zone support is enabled ."]]
["def get_library(library_name):\n\tlib = libraries.get(library_name, None)\n\tif (not lib):\n\t\ttemplatetags_modules = get_templatetags_modules()\n\t\ttried_modules = []\n\t\tfor module in templatetags_modules:\n\t\t\ttaglib_module = (u'%s.%s' % (module, library_name))\n\t\t\ttried_modules.append(taglib_module)\n\t\t\tlib = import_library(taglib_module)\n\t\t\tif lib:\n\t\t\t\tlibraries[library_name] = lib\n\t\t\t\tbreak\n\t\tif (not lib):\n\t\t\traise InvalidTemplateLibrary((u'Template\tlibrary\t%s\tnot\tfound,\ttried\t%s' % (library_name, u','.join(tried_modules))))\n\treturn lib\n",["load the template library module with the given name ."]]
["@register.inclusion_tag('horizon\/_nav_list.html', takes_context=True)\ndef horizon_main_nav(context):\n\tif ('request' not in context):\n\t\treturn {}\n\tcurrent_dashboard = context['request'].horizon.get('dashboard', None)\n\tdashboards = []\n\tfor dash in Horizon.get_dashboards():\n\t\tif dash.can_access(context):\n\t\t\tif (callable(dash.nav) and dash.nav(context)):\n\t\t\t\tdashboards.append(dash)\n\t\t\telif dash.nav:\n\t\t\t\tdashboards.append(dash)\n\treturn {'components': dashboards, 'user': context['request'].user, 'current': current_dashboard, 'request': context['request']}\n",["generates top-level dashboard navigation entries ."]]
["def copyfileobj(fsrc, fdst, length=(64 * 1024)):\n\twhile True:\n\t\tbuf = fsrc.read(length)\n\t\tif (not buf):\n\t\t\tbreak\n\t\tfdst.write(buf)\n",["copy length bytes from fileobj src to fileobj dst ."]]
["def start(name, call=None):\n\tif (call != 'action'):\n\t\traise SaltCloudSystemExit('The\tstart\taction\tmust\tbe\tcalled\twith\t-a\tor\t--action.')\n\tdata = show_instance(name, call='action')\n\tif (data.get('status') == 'active'):\n\t\treturn {'success': True, 'action': 'start', 'status': 'active', 'msg': 'Machine\tis\talready\trunning.'}\n\tret = query(droplet_id=data['id'], command='actions', args={'type': 'power_on'}, http_method='post')\n\treturn {'success': True, 'action': ret['action']['type'], 'state': ret['action']['status']}\n",["start a node ."]]
["def getLargestInsetLoopFromLoopRegardless(loop, radius):\n\tglobal globalDecreasingRadiusMultipliers\n\tfor decreasingRadiusMultiplier in globalDecreasingRadiusMultipliers:\n\t\tdecreasingRadius = (radius * decreasingRadiusMultiplier)\n\t\tlargestInsetLoop = getLargestInsetLoopFromLoop(loop, decreasingRadius)\n\t\tif (len(largestInsetLoop) > 0):\n\t\t\treturn largestInsetLoop\n\tprint 'Warning,\tthere\tshould\talways\tbe\ta\tlargestInsetLoop\tin\tgetLargestInsetLoopFromLoopRegardless\tin\tintercircle.'\n\tprint loop\n\treturn loop\n",["get the largest inset loop from the loop ."]]
["def scalePoints(elementNode, points, prefix):\n\tscaleVector3Default = Vector3(1.0, 1.0, 1.0)\n\tscaleVector3 = matrix.getCumulativeVector3Remove(scaleVector3Default.copy(), elementNode, prefix)\n\tif (scaleVector3 == scaleVector3Default):\n\t\treturn\n\tfor point in points:\n\t\tpoint.x *= scaleVector3.x\n\t\tpoint.y *= scaleVector3.y\n\t\tpoint.z *= scaleVector3.z\n",["scale the points ."]]
["def add_extension(module, name, code):\n\tcode = int(code)\n\tif (not (1 <= code <= 2147483647)):\n\t\traise ValueError, 'code\tout\tof\trange'\n\tkey = (module, name)\n\tif ((_extension_registry.get(key) == code) and (_inverted_registry.get(code) == key)):\n\t\treturn\n\tif (key in _extension_registry):\n\t\traise ValueError(('key\t%s\tis\talready\tregistered\twith\tcode\t%s' % (key, _extension_registry[key])))\n\tif (code in _inverted_registry):\n\t\traise ValueError(('code\t%s\tis\talready\tin\tuse\tfor\tkey\t%s' % (code, _inverted_registry[code])))\n\t_extension_registry[key] = code\n\t_inverted_registry[code] = key\n",["register an extension code ."]]
["def cache_page(*args, **kwargs):\n\tcache_alias = kwargs.pop('cache', None)\n\tkey_prefix = kwargs.pop('key_prefix', None)\n\tassert (not kwargs), 'The\tonly\tkeyword\targuments\tare\tcache\tand\tkey_prefix'\n\tdef warn():\n\t\timport warnings\n\t\twarnings.warn('The\tcache_page\tdecorator\tmust\tbe\tcalled\tlike:\tcache_page(timeout,\t[cache=cache\tname],\t[key_prefix=key\tprefix]).\tAll\tother\tways\tare\tdeprecated.', PendingDeprecationWarning, stacklevel=3)\n\tif (len(args) > 1):\n\t\tassert (len(args) == 2), 'cache_page\taccepts\tat\tmost\t2\targuments'\n\t\twarn()\n\t\tif callable(args[0]):\n\t\t\treturn decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[1], cache_alias=cache_alias, key_prefix=key_prefix)(args[0])\n\t\telif callable(args[1]):\n\t\t\treturn decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[0], cache_alias=cache_alias, key_prefix=key_prefix)(args[1])\n\t\telse:\n\t\t\tassert False, 'cache_page\tmust\tbe\tpassed\ta\tview\tfunction\tif\tcalled\twith\ttwo\targuments'\n\telif (len(args) == 1):\n\t\tif callable(args[0]):\n\t\t\twarn()\n\t\t\treturn decorator_from_middleware_with_args(CacheMiddleware)(cache_alias=cache_alias, key_prefix=key_prefix)(args[0])\n\t\telse:\n\t\t\treturn decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[0], cache_alias=cache_alias, key_prefix=key_prefix)\n\telse:\n\t\twarn()\n\t\treturn decorator_from_middleware_with_args(CacheMiddleware)(cache_alias=cache_alias, key_prefix=key_prefix)\n",["decorator for views that tries getting the page from the cache and populates the cache if the page isnt in the cache yet ."]]
["def scenario():\n\ts3db.configure('scenario_config', deletable=False)\n\tdef prep(r):\n\t\tif (r.interactive and r.component):\n\t\t\tif (r.component.name != 'config'):\n\t\t\t\ts3.crud.submit_button = T('Assign')\n\t\t\t\ts3.crud_labels['DELETE'] = T('Remove')\n\t\tif (r.component_name == 'site'):\n\t\t\tfield = db.scenario_site.site_id\n\t\t\tfield.readable = field.writable = True\n\t\treturn True\n\ts3.prep = prep\n\toutput = s3_rest_controller(rheader=s3db.scenario_rheader)\n\treturn output\n",["restful crud controller ."]]
["def referer(pattern, accept=True, accept_missing=False, error=403, message='Forbidden\tReferer\theader.', debug=False):\n\ttry:\n\t\tref = cherrypy.serving.request.headers['Referer']\n\t\tmatch = bool(re.match(pattern, ref))\n\t\tif debug:\n\t\t\tcherrypy.log(('Referer\t%r\tmatches\t%r' % (ref, pattern)), 'TOOLS.REFERER')\n\t\tif (accept == match):\n\t\t\treturn\n\texcept KeyError:\n\t\tif debug:\n\t\t\tcherrypy.log('No\tReferer\theader', 'TOOLS.REFERER')\n\t\tif accept_missing:\n\t\t\treturn\n\traise cherrypy.HTTPError(error, message)\n",["raise httperror if referer header does\/does not match the given pattern ."]]
["def copy(src, dst, createpath=0, copydates=1, forcetype=None):\n\tsrc = File.pathname(src)\n\tdst = File.pathname(dst)\n\tif createpath:\n\t\tmkdirs(os.path.split(dst)[0])\n\tifp = open(src, 'rb')\n\tofp = open(dst, 'wb')\n\td = ifp.read(BUFSIZ)\n\twhile d:\n\t\tofp.write(d)\n\t\td = ifp.read(BUFSIZ)\n\tifp.close()\n\tofp.close()\n\tifp = openrf(src, '*rb')\n\tofp = openrf(dst, '*wb')\n\td = ifp.read(BUFSIZ)\n\twhile d:\n\t\tofp.write(d)\n\t\td = ifp.read(BUFSIZ)\n\tifp.close()\n\tofp.close()\n\tsrcfss = File.FSSpec(src)\n\tdstfss = File.FSSpec(dst)\n\tsf = srcfss.FSpGetFInfo()\n\tdf = dstfss.FSpGetFInfo()\n\t(df.Creator, df.Type) = (sf.Creator, sf.Type)\n\tif (forcetype is not None):\n\t\tdf.Type = forcetype\n\tdf.Flags = (sf.Flags & COPY_FLAGS)\n\tdstfss.FSpSetFInfo(df)\n\tif copydates:\n\t\tsrcfsr = File.FSRef(src)\n\t\tdstfsr = File.FSRef(dst)\n\t\t(catinfo, _, _, _) = srcfsr.FSGetCatalogInfo(Files.kFSCatInfoAllDates)\n\t\tdstfsr.FSSetCatalogInfo(Files.kFSCatInfoAllDates, catinfo)\n",["filelist \u662f\u4e00\u4e2alist ."]]
["def relpath(path, start='.'):\n\tif ((sys.version_info < (2, 7)) and ('posix' in sys.builtin_module_names)):\n\t\tif (not path):\n\t\t\traise ValueError('no\tpath\tspecified')\n\t\tstart_list = [x for x in os.path.abspath(start).split(os.path.sep) if x]\n\t\tpath_list = [x for x in os.path.abspath(path).split(os.path.sep) if x]\n\t\ti = len(os.path.commonprefix([start_list, path_list]))\n\t\trel_list = (([os.path.pardir] * (len(start_list) - i)) + path_list[i:])\n\t\tif (not rel_list):\n\t\t\treturn os.path.curdir\n\t\treturn os.path.join(*rel_list)\n\treturn os.path.relpath(path, start=start)\n",["return a relative version of a path ."]]
["@register.inclusion_tag('inclusion.html')\ndef inclusion_unlimited_args_kwargs(one, two='hi', *args, **kwargs):\n\tsorted_kwarg = sorted(six.iteritems(kwargs), key=operator.itemgetter(0))\n\treturn {'result': ('inclusion_unlimited_args_kwargs\t-\tExpected\tresult:\t%s\t\/\t%s' % (',\t'.join([six.text_type(arg) for arg in ([one, two] + list(args))]), ',\t'.join([('%s=%s' % (k, v)) for (k, v) in sorted_kwarg])))}\n",["expected inclusion_unlimited_args_kwargs __doc__ ."]]
["def loss_fun(logits, labels):\n\tlabels = tf.cast(labels, tf.int64)\n\tcross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross_entropy_per_example')\n\tcross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n\ttf.add_to_collection('losses', cross_entropy_mean)\n\treturn tf.add_n(tf.get_collection('losses'), name='total_loss')\n",["add l2loss to all the trainable variables ."]]
["def send_followup_email_for_monthly_fee_payment(email, event_name, date, amount, payment_url):\n\tsend_email(to=email, action=MONTHLY_PAYMENT_FOLLOWUP_EMAIL, subject=MAILS[MONTHLY_PAYMENT_FOLLOWUP_EMAIL]['subject'].format(event_name=event_name, date=date), html=MAILS[MONTHLY_PAYMENT_FOLLOWUP_EMAIL]['message'].format(event_name=event_name, date=date, payment_url=payment_url, amount=amount, app_name=get_settings()['app_name']))\n",["send email every month with invoice to pay service fee ."]]
["def add_message(request, level, message, extra_tags='', fail_silently=False):\n\tif hasattr(request, '_messages'):\n\t\treturn request._messages.add(level, message, extra_tags)\n\tif (hasattr(request, 'user') and request.user.is_authenticated()):\n\t\treturn request.user.message_set.create(message=message)\n\tif (not fail_silently):\n\t\traise MessageFailure('Without\tthe\tdjango.contrib.messages\tmiddleware,\tmessages\tcan\tonly\tbe\tadded\tto\tauthenticated\tusers.')\n",["attempts to add a message to the request using the messages app ."]]
["def register_serializer(format, serializer_module, serializers=None):\n\tif ((serializers is None) and (not _serializers)):\n\t\t_load_serializers()\n\tmodule = importlib.import_module(serializer_module)\n\tif (serializers is None):\n\t\t_serializers[format] = module\n\telse:\n\t\tserializers[format] = module\n",["register a new serializer ."]]
["def request_from_dict(d, spider=None):\n\tcb = d['callback']\n\tif (cb and spider):\n\t\tcb = _get_method(spider, cb)\n\teb = d['errback']\n\tif (eb and spider):\n\t\teb = _get_method(spider, eb)\n\treturn Request(url=d['url'].encode('ascii'), callback=cb, errback=eb, method=d['method'], headers=d['headers'], body=d['body'], cookies=d['cookies'], meta=d['meta'], encoding=d['_encoding'], priority=d['priority'], dont_filter=d['dont_filter'])\n",["create request object from a dict ."]]
["def _exec_template(callable_, context, args=None, kwargs=None):\n\ttemplate = context._with_template\n\tif ((template is not None) and (template.format_exceptions or template.error_handler)):\n\t\terror = None\n\t\ttry:\n\t\t\tcallable_(context, *args, **kwargs)\n\t\texcept Exception as e:\n\t\t\t_render_error(template, context, e)\n\t\texcept:\n\t\t\te = sys.exc_info()[0]\n\t\t\t_render_error(template, context, e)\n\telse:\n\t\tcallable_(context, *args, **kwargs)\n",["execute a rendering callable given the callable ."]]
["def OSversion():\n\tfinder = _getfinder()\n\targs = {}\n\tattrs = {}\n\taeobj_00 = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('ver2'), fr=None)\n\targs['----'] = aeobj_00\n\t(_reply, args, attrs) = finder.send('core', 'getd', args, attrs)\n\tif ('errn' in args):\n\t\traise Error, aetools.decodeerror(args)\n\tif ('----' in args):\n\t\treturn args['----']\n",["return the version of the system software ."]]
["def unquote_unreserved(uri):\n\tparts = uri.split('%')\n\tfor i in range(1, len(parts)):\n\t\th = parts[i][0:2]\n\t\tif ((len(h) == 2) and h.isalnum()):\n\t\t\ttry:\n\t\t\t\tc = chr(int(h, 16))\n\t\t\texcept ValueError:\n\t\t\t\traise InvalidURL((\"Invalid\tpercent-escape\tsequence:\t'%s'\" % h))\n\t\t\tif (c in UNRESERVED_SET):\n\t\t\t\tparts[i] = (c + parts[i][2:])\n\t\t\telse:\n\t\t\t\tparts[i] = ('%' + parts[i])\n\t\telse:\n\t\t\tparts[i] = ('%' + parts[i])\n\treturn ''.join(parts)\n",["un-escape any percent-escape sequences in a uri that are unreserved characters ."]]
["def randrange_fmt(mode, char, obj):\n\tx = randrange(*fmtdict[mode][char])\n\tif (char == 'c'):\n\t\tx = bytes(chr(x), 'latin1')\n\tif (char == '?'):\n\t\tx = bool(x)\n\tif ((char == 'f') or (char == 'd')):\n\t\tx = struct.pack(char, x)\n\t\tx = struct.unpack(char, x)[0]\n\tif ((obj == 'numpy') and (x == '\\x00')):\n\t\tx = '\\x01'\n\treturn x\n",["return random item for a type specified by a mode and a single format character ."]]
["def groups_for_user(environ, username):\n\tUserModel = auth.get_user_model()\n\tdb.reset_queries()\n\ttry:\n\t\ttry:\n\t\t\tuser = UserModel._default_manager.get_by_natural_key(username)\n\t\texcept UserModel.DoesNotExist:\n\t\t\treturn []\n\t\tif (not user.is_active):\n\t\t\treturn []\n\t\treturn [force_bytes(group.name) for group in user.groups.all()]\n\tfinally:\n\t\tdb.close_old_connections()\n",["authorizes a user based on groups ."]]
["def make_url(namespace, max_age):\n\tsigner = MessageSigner(g.secrets['websocket'])\n\tsignature = signer.make_signature(namespace, max_age=datetime.timedelta(seconds=max_age))\n\tquery_string = urllib.urlencode({'m': signature})\n\treturn urlparse.urlunparse(('wss', g.websocket_host, namespace, None, query_string, None))\n",["adds the api key to the url if its not already there ."]]
["def keygen():\n\tparser = OptionParser(usage='usage:\t%prog\t[options]\tkeysize', description='Generates\ta\tnew\tRSA\tkeypair\tof\t\"keysize\"\tbits.')\n\tparser.add_option('--pubout', type='string', help='Output\tfilename\tfor\tthe\tpublic\tkey.\tThe\tpublic\tkey\tis\tnot\tsaved\tif\tthis\toption\tis\tnot\tpresent.\tYou\tcan\tuse\tpyrsa-priv2pub\tto\tcreate\tthe\tpublic\tkey\tfile\tlater.')\n\tparser.add_option('-o', '--out', type='string', help='Output\tfilename\tfor\tthe\tprivate\tkey.\tThe\tkey\tis\twritten\tto\tstdout\tif\tthis\toption\tis\tnot\tpresent.')\n\tparser.add_option('--form', help='key\tformat\tof\tthe\tprivate\tand\tpublic\tkeys\t-\tdefault\tPEM', choices=('PEM', 'DER'), default='PEM')\n\t(cli, cli_args) = parser.parse_args(sys.argv[1:])\n\tif (len(cli_args) != 1):\n\t\tparser.print_help()\n\t\traise SystemExit(1)\n\ttry:\n\t\tkeysize = int(cli_args[0])\n\texcept ValueError:\n\t\tparser.print_help()\n\t\tprint(('Not\ta\tvalid\tnumber:\t%s' % cli_args[0]), file=sys.stderr)\n\t\traise SystemExit(1)\n\tprint(('Generating\t%i-bit\tkey' % keysize), file=sys.stderr)\n\t(pub_key, priv_key) = rsa.newkeys(keysize)\n\tif cli.pubout:\n\t\tprint(('Writing\tpublic\tkey\tto\t%s' % cli.pubout), file=sys.stderr)\n\t\tdata = pub_key.save_pkcs1(format=cli.form)\n\t\twith open(cli.pubout, 'wb') as outfile:\n\t\t\toutfile.write(data)\n\tdata = priv_key.save_pkcs1(format=cli.form)\n\tif cli.out:\n\t\tprint(('Writing\tprivate\tkey\tto\t%s' % cli.out), file=sys.stderr)\n\t\twith open(cli.out, 'wb') as outfile:\n\t\t\toutfile.write(data)\n\telse:\n\t\tprint('Writing\tprivate\tkey\tto\tstdout', file=sys.stderr)\n\t\tsys.stdout.write(data)\n",["use libnacl to generate a private key cli examples: ."]]
["def InstanceActionAPI(*args, **kwargs):\n\timportutils = nova.openstack.common.importutils\n\tcompute_api_class_name = oslo.config.cfg.CONF.compute_api_class\n\tcompute_api_class = importutils.import_class(compute_api_class_name)\n\tclass_name = (compute_api_class.__module__ + '.InstanceActionAPI')\n\treturn importutils.import_object(class_name, *args, **kwargs)\n",["returns the instanceactionapi class from the same module as the configured compute api ."]]
["def fetch_crl(project_id):\n\tif (not CONF.crypto.use_project_ca):\n\t\tproject_id = None\n\tcrl_file_path = crl_path(project_id)\n\tif (not os.path.exists(crl_file_path)):\n\t\traise exception.CryptoCRLFileNotFound(project=project_id)\n\twith open(crl_file_path, 'r') as crlfile:\n\t\treturn crlfile.read()\n",["get crl file for project ."]]
["def ordinal(value):\n\ttry:\n\t\tvalue = int(value)\n\texcept (TypeError, ValueError):\n\t\treturn value\n\tt = ('th', 'st', 'nd', 'rd', 'th', 'th', 'th', 'th', 'th', 'th')\n\tif ((value % 100) in (11, 12, 13)):\n\t\treturn (u'%d%s' % (value, t[0]))\n\treturn (u'%d%s' % (value, t[(value % 10)]))\n",["converts an integer to its ordinal as a string ."]]
["def dispatch_hook(key, hooks, hook_data, **kwargs):\n\thooks = (hooks or dict())\n\thooks = hooks.get(key)\n\tif hooks:\n\t\tif hasattr(hooks, '__call__'):\n\t\t\thooks = [hooks]\n\t\tfor hook in hooks:\n\t\t\t_hook_data = hook(hook_data, **kwargs)\n\t\t\tif (_hook_data is not None):\n\t\t\t\thook_data = _hook_data\n\treturn hook_data\n",["dispatches a hook dictionary on a given piece of data ."]]
["def import_class(import_str):\n\t(mod_str, _sep, class_str) = import_str.rpartition('.')\n\ttry:\n\t\t__import__(mod_str)\n\t\treturn getattr(sys.modules[mod_str], class_str)\n\texcept (ValueError, AttributeError):\n\t\traise ImportError(('Class\t%s\tcannot\tbe\tfound\t(%s)' % (class_str, traceback.format_exception(*sys.exc_info()))))\n",["returns a class from a string including module and class ."]]
["def get_version(package):\n\tinit_py = open(os.path.join(package, '__init__.py')).read()\n\treturn re.search('__version__\t=\t[\\'\"]([^\\'\"]+)[\\'\"]', init_py).group(1)\n",["get the version of the project ."]]
["def forecast_data(year, quarter):\n\tif (ct._check_input(year, quarter) is True):\n\t\tct._write_head()\n\t\tdata = _get_forecast_data(year, quarter, 1, pd.DataFrame())\n\t\tdf = pd.DataFrame(data, columns=ct.FORECAST_COLS)\n\t\tdf['code'] = df['code'].map((lambda x: str(x).zfill(6)))\n\t\treturn df\n",["parameters year:int \u5e74\u5ea6 e ."]]
["def replace_query_param(url, key, val):\n\t(scheme, netloc, path, query, fragment) = urlparse.urlsplit(url)\n\tquery_dict = urlparse.parse_qs(query, keep_blank_values=True)\n\tquery_dict[key] = [val]\n\tquery = urlparse.urlencode(sorted(list(query_dict.items())), doseq=True)\n\treturn urlparse.urlunsplit((scheme, netloc, path, query, fragment))\n",["given a url and a key\/val pair ."]]
["def date(*args, **kwargs):\n\td = None\n\tf = None\n\tif ((len(args) == 0) and (kwargs.get('year') is not None) and kwargs.get('month') and kwargs.get('day')):\n\t\td = Date(**kwargs)\n\telif kwargs.get('week'):\n\t\tf = kwargs.pop('format', None)\n\t\td = Date(*_yyyywwd2yyyymmdd(kwargs.pop('year', ((args and args[0]) or Date.now().year)), kwargs.pop('week'), kwargs.pop('weekday', kwargs.pop('day', 1))), **kwargs)\n\telif ((len(args) == 0) or (args[0] == NOW)):\n\t\td = Date.now()\n\telif ((len(args) == 1) and isinstance(args[0], (Date, datetime))):\n\t\td = Date.fromtimestamp(int(mktime(args[0].timetuple())))\n\t\td += time(microseconds=args[0].microsecond)\n\telif ((len(args) == 1) and (isinstance(args[0], int) or (isinstance(args[0], basestring) and args[0].isdigit()))):\n\t\td = Date.fromtimestamp(int(args[0]))\n\telif ((len(args) == 1) and isinstance(args[0], basestring)):\n\t\ttry:\n\t\t\td = Date.fromtimestamp(mktime_tz(parsedate_tz(args[0])))\n\t\texcept:\n\t\t\tfor format in (((('format' in kwargs) and [kwargs['format']]) or []) + date_formats):\n\t\t\t\ttry:\n\t\t\t\t\td = Date.strptime(args[0], format)\n\t\t\t\t\tbreak\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\tif (d is None):\n\t\t\traise DateError(('unknown\tdate\tformat\tfor\t%s' % repr(args[0])))\n\telif ((len(args) == 2) and isinstance(args[0], basestring)):\n\t\td = Date.strptime(args[0], args[1])\n\telif (len(args) >= 3):\n\t\tf = kwargs.pop('format', None)\n\t\td = Date(*args[:7], **kwargs)\n\telse:\n\t\traise DateError('unknown\tdate\tformat')\n\td.format = (kwargs.get('format') or ((len(args) > 7) and args[7]) or f or Date.format)\n\treturn d\n",["formats a date according to the given format ."]]
["def export_book(databook):\n\twb = xlwt.Workbook(encoding='utf8')\n\tfor (i, dset) in enumerate(databook._datasets):\n\t\tws = wb.add_sheet((dset.title if dset.title else ('Sheet%s' % i)))\n\t\tdset_sheet(dset, ws)\n\tstream = BytesIO()\n\twb.save(stream)\n\treturn stream.getvalue()\n",["html representation of a databook ."]]
["def get_encoding_from_headers(headers):\n\tcontent_type = headers.get('content-type')\n\tif (not content_type):\n\t\treturn None\n\t(content_type, params) = cgi.parse_header(content_type)\n\tif ('charset' in params):\n\t\treturn params['charset'].strip('\\'\"')\n\tif ('text' in content_type):\n\t\treturn 'ISO-8859-1'\n",["returns encodings from given http header dict ."]]
["def get_max_age(response):\n\tif (not response.has_header('Cache-Control')):\n\t\treturn\n\tcc = dict([_to_tuple(el) for el in cc_delim_re.split(response['Cache-Control'])])\n\tif ('max-age' in cc):\n\t\ttry:\n\t\t\treturn int(cc['max-age'])\n\t\texcept (ValueError, TypeError):\n\t\t\tpass\n",["returns the max-age from the response cache-control header as an integer (or none if it wasnt found or wasnt an integer ."]]
["def libvlc_media_player_set_nsobject(p_mi, drawable):\n\tf = (_Cfunctions.get('libvlc_media_player_set_nsobject', None) or _Cfunction('libvlc_media_player_set_nsobject', ((1,), (1,)), None, None, MediaPlayer, ctypes.c_void_p))\n\treturn f(p_mi, drawable)\n",["set the nsview handler where the media player should render its video output ."]]
["def test_saving_state_include_domains(hass_recorder):\n\thass = hass_recorder({'include': {'domains': 'test2'}})\n\tstates = _add_entities(hass, ['test.recorder', 'test2.recorder'])\n\tassert (len(states) == 1)\n\tassert (hass.states.get('test2.recorder') == states[0])\n",["test saving and restoring a state ."]]
["def _ls_emr_step_stderr_logs(fs, log_dir_stream, step_id=None):\n\tmatches = _ls_logs(fs, log_dir_stream, _match_emr_step_stderr_path, step_id=step_id)\n\treturn sorted(matches, key=_match_sort_key, reverse=True)\n",["yield matching step logs ."]]
["def walk_to_end(ch, input_iter):\n\tif (ch == '('):\n\t\tnesting = 1\n\telse:\n\t\tnesting = 0\n\tfor (ch, escaped) in input_iter:\n\t\tif escaped:\n\t\t\tcontinue\n\t\telif (ch == '('):\n\t\t\tnesting += 1\n\t\telif (ch == ')'):\n\t\t\tif (not nesting):\n\t\t\t\treturn\n\t\t\tnesting -= 1\n",["the iterator is currently inside a capturing group ."]]
["def reboot(name, path=None):\n\tret = {'result': True, 'changes': {}, 'comment': '{0}\trebooted'.format(name)}\n\tdoes_exist = exists(name, path=path)\n\tif (does_exist and (state(name, path=path) == 'running')):\n\t\ttry:\n\t\t\tstop(name, path=path)\n\t\texcept (SaltInvocationError, CommandExecutionError) as exc:\n\t\t\tret['comment'] = 'Unable\tto\tstop\tcontainer:\t{0}'.format(exc)\n\t\t\tret['result'] = False\n\t\t\treturn ret\n\tif (does_exist and (state(name, path=path) != 'running')):\n\t\ttry:\n\t\t\tstart(name, path=path)\n\t\texcept (SaltInvocationError, CommandExecutionError) as exc:\n\t\t\tret['comment'] = 'Unable\tto\tstop\tcontainer:\t{0}'.format(exc)\n\t\t\tret['result'] = False\n\t\t\treturn ret\n\tret['changes'][name] = 'rebooted'\n\treturn ret\n",["reboot a machine by name ."]]
["def run_migrations_offline():\n\tset_mysql_engine()\n\tkwargs = dict()\n\tif neutron_config.database.connection:\n\t\tkwargs['url'] = neutron_config.database.connection\n\telse:\n\t\tkwargs['dialect_name'] = neutron_config.database.engine\n\tkwargs['include_object'] = include_object\n\tcontext.configure(**kwargs)\n\twith context.begin_transaction():\n\t\tcontext.run_migrations()\n",["run migrations in offline mode ."]]
["def buildTagMap(default, *args):\n\tbuilt = {}\n\tfor portion in args:\n\t\tif hasattr(portion, 'items'):\n\t\t\tfor (k, v) in portion.items():\n\t\t\t\tbuilt[k] = v\n\t\telif (isList(portion) and (not isString(portion))):\n\t\t\tfor k in portion:\n\t\t\t\tbuilt[k] = default\n\t\telse:\n\t\t\tbuilt[portion] = default\n\treturn built\n",["turns a list of maps ."]]
["def getAroundsFromPaths(paths, radius, thresholdRatio=0.9):\n\tpoints = []\n\tfor path in paths:\n\t\tpoints += getPointsFromPath(path, (1.01 * abs(radius)), thresholdRatio)\n\treturn getAroundsFromPoints(points, radius)\n",["get the arounds from the path ."]]
["def returnConnected(server, client):\n\tcio = StringIO()\n\tsio = StringIO()\n\tclient.makeConnection(FileWrapper(cio))\n\tserver.makeConnection(FileWrapper(sio))\n\tpump = IOPump(client, server, cio, sio)\n\tpump.flush()\n\tpump.flush()\n\treturn pump\n",["take two protocol instances and connect them ."]]
["def base62_encode(num, alphabet=ALPHABET):\n\tif (num == 0):\n\t\treturn alphabet[0]\n\tarr = []\n\tbase = len(alphabet)\n\twhile num:\n\t\trem = (num % base)\n\t\tnum = (num \/\/ base)\n\t\tarr.append(alphabet[rem])\n\tarr.reverse()\n\treturn ''.join(arr)\n",["encode a number in base x num: the number to encode alphabet: the alphabet to use for encoding ."]]
["def b64decode(s, altchars=None):\n\tif (altchars is not None):\n\t\ts = _translate(s, {altchars[0]: '+', altchars[1]: '\/'})\n\ttry:\n\t\treturn binascii.a2b_base64(s)\n\texcept binascii.Error as msg:\n\t\traise TypeError(msg)\n",["decode a base64 encoded string ."]]
["def quoteaddr(addr):\n\tm = (None, None)\n\ttry:\n\t\tm = email.utils.parseaddr(addr)[1]\n\texcept AttributeError:\n\t\tpass\n\tif (m == (None, None)):\n\t\treturn ('<%s>' % addr)\n\telif (m is None):\n\t\treturn '<>'\n\telse:\n\t\treturn ('<%s>' % m)\n",["turn an email address ."]]
["def Event():\n\tfrom multiprocessing.synchronize import Event\n\treturn Event()\n",["create an event ."]]
["def en_format(name):\n\tfrom django.conf.locale.en import formats\n\twarnings.warn((\"`django.forms.fields.DEFAULT_%s`\tis\tdeprecated;\tuse\t`django.utils.formats.get_format('%s')`\tinstead.\" % (name, name)), PendingDeprecationWarning)\n\treturn getattr(formats, name)\n",["helper function to stay backward compatible ."]]
["def stop(name):\n\tcmd = '\/etc\/rc.d\/{0}\t-f\tstop'.format(name)\n\treturn (not __salt__['cmd.retcode'](cmd))\n",["stop a vm ."]]
["def sleep(at_time=None):\n\tcmd = 'shutdown\t-s\tnow'\n\treturn _execute_command(cmd, at_time)\n",["put the current greenlet to sleep for at least *seconds* ."]]
["def test_iht_fit_invalid_ratio():\n\tratio = (1.0 \/ 10000.0)\n\tiht = InstanceHardnessThreshold(ESTIMATOR, ratio=ratio, random_state=RND_SEED)\n\tassert_raises(RuntimeError, iht.fit, X, Y)\n",["test either if an error is raised when the balancing ratio to fit is smaller than the one of the data ."]]
["def get_reader_class(reader_name):\n\treader_name = reader_name.lower()\n\tif (reader_name in _reader_aliases):\n\t\treader_name = _reader_aliases[reader_name]\n\ttry:\n\t\tmodule = __import__(reader_name, globals(), locals(), level=1)\n\texcept ImportError:\n\t\tmodule = __import__(reader_name, globals(), locals(), level=0)\n\treturn module.Reader\n",["return the reader class from the reader_name module ."]]
["def flatten_fieldsets(fieldsets):\n\tfield_names = []\n\tfor (name, opts) in fieldsets:\n\t\tfor field in opts[u'fields']:\n\t\t\tif (type(field) == tuple):\n\t\t\t\tfield_names.extend(field)\n\t\t\telse:\n\t\t\t\tfield_names.append(field)\n\treturn field_names\n",["returns a list of field names from an admin fieldsets structure ."]]
["def json_splitter(buffer):\n\ttry:\n\t\t(obj, index) = json_decoder.raw_decode(buffer)\n\t\trest = buffer[json.decoder.WHITESPACE.match(buffer, index).end():]\n\t\treturn (obj, rest)\n\texcept ValueError:\n\t\treturn None\n",["attempt to parse a json object from a buffer ."]]
["def extract_zip(source, remove=False, fatal=True):\n\ttempdir = tempfile.mkdtemp()\n\tzip_file = SafeUnzip(source)\n\ttry:\n\t\tif zip_file.is_valid(fatal):\n\t\t\tzip_file.extract_to_dest(tempdir)\n\texcept:\n\t\trm_local_tmp_dir(tempdir)\n\t\traise\n\tif remove:\n\t\tos.remove(source)\n\treturn tempdir\n",["extracts the zip file ."]]
["def read_py_file(filename, skip_encoding_cookie=True):\n\twith tokopen(filename) as f:\n\t\tif skip_encoding_cookie:\n\t\t\treturn ''.join(strip_encoding_cookie(f))\n\t\telse:\n\t\t\treturn f.read()\n",["read a python file ."]]
["def zero_cluster():\n\tif _TRAFFICCTL:\n\t\tcmd = _traffic_ctl('metric', 'clear', '--cluster')\n\telse:\n\t\tcmd = _traffic_line('-Z')\n\tlog.debug('Running:\t%s', cmd)\n\treturn _subprocess(cmd)\n",["reset performance statistics to zero across the cluster ."]]
["def convert_TimeProperty(model, prop, kwargs):\n\tif (prop.auto_now or prop.auto_now_add):\n\t\treturn None\n\treturn f.DateTimeField(format='%H:%M:%S', **kwargs)\n",["returns a form field for a db ."]]
["def __validate__(config):\n\tif (not isinstance(config, dict)):\n\t\treturn (False, 'Configuration\tfor\tbtmp\tbeacon\tmust\tbe\ta\tlist\tof\tdictionaries.')\n\treturn (True, 'Valid\tbeacon\tconfiguration')\n",["validate the beacon configuration ."]]
["def stop_cover(hass, entity_id=None):\n\tdata = ({ATTR_ENTITY_ID: entity_id} if entity_id else None)\n\thass.services.call(DOMAIN, SERVICE_STOP_COVER, data)\n",["stop all or specified cover ."]]
["def educateEllipses(s):\n\treturn s.replace('...', '&#8230;').replace('.\t.\t.', '&#8230;')\n",["parameter: string ."]]
["def console_get_all_by_instance(context, instance_uuid, columns_to_join=None):\n\treturn IMPL.console_get_all_by_instance(context, instance_uuid, columns_to_join)\n",["get consoles for a given instance ."]]
["def _relative_degree(z, p):\n\tdegree = (len(p) - len(z))\n\tif (degree < 0):\n\t\traise ValueError('Improper\ttransfer\tfunction.\tMust\thave\tat\tleast\tas\tmany\tpoles\tas\tzeros.')\n\telse:\n\t\treturn degree\n",["return relative degree of transfer function from zeros and poles ."]]
["def getManipulatedGeometryOutput(elementNode, geometryOutput, prefix):\n\tflippedGeometryOutput = triangle_mesh.getGeometryOutputCopy(geometryOutput)\n\tflip.flipPoints(elementNode, matrix.getVertexes(flippedGeometryOutput), prefix)\n\tif flip.getShouldReverse(elementNode, prefix):\n\t\tflippedFaces = face.getFaces(flippedGeometryOutput)\n\t\tfor flippedFace in flippedFaces:\n\t\t\tflippedFace.vertexIndexes.reverse()\n\treturn {'union': {'shapes': [flippedGeometryOutput, geometryOutput]}}\n",["get equated geometryoutput ."]]
["def test_sample_wt_fit():\n\tratio = 'auto'\n\tee = EasyEnsemble(ratio=ratio, random_state=RND_SEED)\n\tassert_raises(RuntimeError, ee.sample, X, Y)\n",["test either if an error is raised when sample is called before fitting ."]]
["def action_peek_xml(body):\n\tdom = utils.safe_minidom_parse_string(body)\n\taction_node = dom.childNodes[0]\n\treturn action_node.tagName\n",["determine action to invoke ."]]
["def import_book(stream):\n\t(format, stream) = detect(stream)\n\ttry:\n\t\tdatabook = Databook()\n\t\tformat.import_book(databook, stream)\n\t\treturn databook\n\texcept AttributeError:\n\t\treturn None\n",["return dataset of given stream ."]]
["def _test_args():\n\timport pandas as pd\n\treturn {'start': pd.Timestamp('2004', tz='utc'), 'end': pd.Timestamp('2008', tz='utc')}\n",["extra arguments to use when ziplines automated tests run this example ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load if boto libraries exist ."]]
["def writeOutput(fileName=''):\n\tfileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName)\n\tif (fileName != ''):\n\t\tskeinforge_craft.writeChainTextWithNounMessage(fileName, 'unpause')\n",["speed a gcode linear move file ."]]
["def test_sobel_zeros():\n\tresult = filters.sobel(np.zeros((10, 10)), np.ones((10, 10), bool))\n\tassert np.all((result == 0))\n",["sobel on an array of all zeros ."]]
["@declared\ndef down(obj, output):\n\tset_value(obj, output, None, 1)\n",["shutdown a network interface cli example: ."]]
["def urlize(text, trim_url_limit=None, nofollow=False, autoescape=False):\n\ttrim_url = (lambda x, limit=trim_url_limit: (((limit is not None) and ((len(x) > limit) and ('%s...' % x[:max(0, (limit - 3))]))) or x))\n\tsafe_input = isinstance(text, SafeData)\n\twords = word_split_re.split(force_unicode(text))\n\tfor (i, word) in enumerate(words):\n\t\tmatch = None\n\t\tif (('.' in word) or ('@' in word) or (':' in word)):\n\t\t\t(lead, middle, trail) = ('', word, '')\n\t\t\tfor punctuation in TRAILING_PUNCTUATION:\n\t\t\t\tif middle.endswith(punctuation):\n\t\t\t\t\tmiddle = middle[:(- len(punctuation))]\n\t\t\t\t\ttrail = (punctuation + trail)\n\t\t\tfor (opening, closing) in WRAPPING_PUNCTUATION:\n\t\t\t\tif middle.startswith(opening):\n\t\t\t\t\tmiddle = middle[len(opening):]\n\t\t\t\t\tlead = (lead + opening)\n\t\t\t\tif (middle.endswith(closing) and (middle.count(closing) == (middle.count(opening) + 1))):\n\t\t\t\t\tmiddle = middle[:(- len(closing))]\n\t\t\t\t\ttrail = (closing + trail)\n\t\t\turl = None\n\t\t\tnofollow_attr = ('\trel=\"nofollow\"' if nofollow else '')\n\t\t\tif simple_url_re.match(middle):\n\t\t\t\turl = smart_urlquote(middle)\n\t\t\telif simple_url_2_re.match(middle):\n\t\t\t\turl = smart_urlquote(('http:\/\/%s' % middle))\n\t\t\telif ((not (':' in middle)) and simple_email_re.match(middle)):\n\t\t\t\t(local, domain) = middle.rsplit('@', 1)\n\t\t\t\ttry:\n\t\t\t\t\tdomain = domain.encode('idna')\n\t\t\t\texcept UnicodeError:\n\t\t\t\t\tcontinue\n\t\t\t\turl = ('mailto:%s@%s' % (local, domain))\n\t\t\t\tnofollow_attr = ''\n\t\t\tif url:\n\t\t\t\ttrimmed = trim_url(middle)\n\t\t\t\tif (autoescape and (not safe_input)):\n\t\t\t\t\t(lead, trail) = (escape(lead), escape(trail))\n\t\t\t\t\t(url, trimmed) = (escape(url), escape(trimmed))\n\t\t\t\tmiddle = ('<a\thref=\"%s\"%s>%s<\/a>' % (url, nofollow_attr, trimmed))\n\t\t\t\twords[i] = mark_safe(('%s%s%s' % (lead, middle, trail)))\n\t\t\telif safe_input:\n\t\t\t\twords[i] = mark_safe(word)\n\t\t\telif autoescape:\n\t\t\t\twords[i] = escape(word)\n\t\telif safe_input:\n\t\t\twords[i] = mark_safe(word)\n\t\telif autoescape:\n\t\t\twords[i] = escape(word)\n\treturn u''.join(words)\n",["converts urls in plain text into clickable links ."]]
["def skipIfDBFeature(*features):\n\treturn _deferredSkip((lambda : any((getattr(connection.features, feature, False) for feature in features))), ('Database\thas\tfeature(s)\t%s' % ',\t'.join(features)))\n",["skip a test if a database has the named feature ."]]
["def dup_add_mul(f, g, h, K):\n\treturn dup_add(f, dup_mul(g, h, K), K)\n",["returns f + g*h where f ."]]
["def manage(command, args=None, as_thread=False):\n\tif (not args):\n\t\targs = []\n\targs = update_default_args(['--traceback'], args)\n\tif (not as_thread):\n\t\tif PROFILE:\n\t\t\tprofile_memory()\n\t\tutility = ManagementUtility(([os.path.basename(sys.argv[0]), command] + args))\n\t\tutility.prog_name = 'kalite\tmanage'\n\t\tutility.execute()\n\telse:\n\t\tget_commands()\n\t\tthread = ManageThread(command, args=args, name='\t'.join(([command] + args)))\n\t\tthread.start()\n\t\treturn thread\n",["return a proxy for a db-api module that automatically pools connections ."]]
["def componentFactory(componentid, password):\n\ta = ConnectComponentAuthenticator(componentid, password)\n\treturn xmlstream.XmlStreamFactory(a)\n",["xml stream factory for external server-side components ."]]
["def render_to_text(*args, **kwargs):\n\treturn HttpResponse(loader.render_to_string(*args, **kwargs), content_type='text\/plain')\n",["renders the response using the mime type for plain text ."]]
["def onCellAppDeath(addr):\n\tWARNING_MSG(('onCellAppDeath:\t%s' % str(addr)))\n",["kbengine method ."]]
["def _api_shutdown(name, output, kwargs):\n\tsabnzbd.halt()\n\tcherrypy.engine.exit()\n\tsabnzbd.SABSTOP = True\n\treturn report(output)\n",["api: accepts output ."]]
["def posixToNtSlashes(filepath):\n\treturn filepath.replace('\/', '\\\\')\n",["replaces all occurances of posix slashes (\/) in provided filepath with nt ones () ."]]
["def urlunquote(quoted_url):\n\treturn force_text(urllib_parse.unquote(force_str(quoted_url)))\n",["a wrapper for pythons urllib ."]]
["@register.tag\ndef get_comment_list(parser, token):\n\treturn CommentListNode.handle_token(parser, token)\n",["gets the list of comments for the given params and populates the template context with a variable containing that value ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load if the pygerduty module is available in __salt__ ."]]
["def strip_tags(s):\n\treturn re.compile(u'<([^>]+)>', flags=re.UNICODE).sub(u'\t', s)\n",["returns the given html with all tags stripped ."]]
["def L(seqn):\n\treturn chain(imap((lambda x: x), R(Ig(G(seqn)))))\n",["test multiple tiers of iterators ."]]
["def processXMLElement(xmlElement):\n\tgroup.processShape(Difference, xmlElement)\n",["process the xml element ."]]
["def apply_rollback(datastore, name):\n\treturn _proxy_cmd('apply_rollback', datastore, name)\n",["apply a system rollback ."]]
["def wordcount(value):\n\treturn len(value.split())\n",["returns the number of words ."]]
["def scan():\n\tret = []\n\tdevices = bluetooth.discover_devices(lookup_names=True)\n\tfor device in devices:\n\t\tret.append({device[0]: device[1]})\n\treturn ret\n",["scan for available ports ."]]
["def comment(parser, token):\n\tparser.skip_past('endcomment')\n\treturn CommentNode()\n",["ignores everything between {% comment %} and {% endcomment %} ."]]
["def test_ros_sk_estimator():\n\tcheck_estimator(RandomOverSampler)\n",["test the sklearn estimator compatibility ."]]
["def zone_type():\n\treturn s3_rest_controller()\n",["restful crud controller ."]]
["def evaluation_question():\n\treturn s3_rest_controller()\n",["restful crud controller ."]]
["@register.filter(is_safe=False)\n@stringfilter\ndef upper(value):\n\treturn value.upper()\n",["converts a string into all uppercase ."]]
["def start(name, call=None):\n\tif (call != 'action'):\n\t\traise SaltCloudSystemExit('The\tstart\taction\tmust\tbe\tcalled\twith\t-a\tor\t--action.')\n\tdata = show_instance(name, call='action')\n\tif (data.get('status') == 'active'):\n\t\treturn {'success': True, 'action': 'start', 'status': 'active', 'msg': 'Machine\tis\talready\trunning.'}\n\tret = query(droplet_id=data['id'], command='actions', args={'type': 'power_on'}, http_method='post')\n\treturn {'success': True, 'action': ret['action']['type'], 'state': ret['action']['status']}\n",["start a service ."]]
["def educateQuotesLatex(s, dquotes=('``', \"''\")):\n\ts = single_quote_start_re.sub('\\x04', s)\n\ts = double_quote_start_re.sub('\\x02', s)\n\ts = double_quote_sets_re.sub('\\x01\\x03', s)\n\ts = single_quote_sets_re.sub('\\x03\\x01', s)\n\ts = decade_abbr_re.sub('\\x04', s)\n\ts = opening_single_quotes_regex.sub('\\\\1\\x03', s)\n\ts = closing_single_quotes_regex.sub('\\\\1\\x04', s)\n\ts = closing_single_quotes_regex_2.sub('\\\\1\\x04\\\\2', s)\n\ts = s.replace(\"'\", '\\x03')\n\ts = opening_double_quotes_regex.sub('\\\\1\\x01', s)\n\ts = closing_double_quotes_regex.sub('\\x02', s)\n\ts = closing_double_quotes_regex_2.sub('\\\\1\\x02', s)\n\ts = s.replace('\"', '\\x01')\n\treturn s.replace('\\x01', dquotes[0]).replace('\\x02', dquotes[1]).replace('\\x03', '`').replace('\\x04', \"'\")\n",["parameter: string ."]]
["def main():\n\terrors = 0\n\tfits_files = handle_options(sys.argv[1:])\n\tsetup_logging()\n\tfor filename in fits_files:\n\t\terrors += process_file(filename)\n\tif errors:\n\t\tlog.warning('{}\terrors'.format(errors))\n\treturn int(bool(errors))\n",["the main function for this script ."]]
["def match_hostname(cert, hostname):\n\tif (not cert):\n\t\traise ValueError('empty\tor\tno\tcertificate')\n\tdnsnames = []\n\tsan = cert.get('subjectAltName', ())\n\tfor (key, value) in san:\n\t\tif (key == 'DNS'):\n\t\t\tif _dnsname_to_pat(value).match(hostname):\n\t\t\t\treturn\n\t\t\tdnsnames.append(value)\n\tif (not dnsnames):\n\t\tfor sub in cert.get('subject', ()):\n\t\t\tfor (key, value) in sub:\n\t\t\t\tif (key == 'commonName'):\n\t\t\t\t\tif _dnsname_to_pat(value).match(hostname):\n\t\t\t\t\t\treturn\n\t\t\t\t\tdnsnames.append(value)\n\tif (len(dnsnames) > 1):\n\t\traise CertificateError((\"hostname\t%r\tdoesn't\tmatch\teither\tof\t%s\" % (hostname, ',\t'.join(map(repr, dnsnames)))))\n\telif (len(dnsnames) == 1):\n\t\traise CertificateError((\"hostname\t%r\tdoesn't\tmatch\t%r\" % (hostname, dnsnames[0])))\n\telse:\n\t\traise CertificateError('no\tappropriate\tcommonName\tor\tsubjectAltName\tfields\twere\tfound')\n",["verify that *cert* (in decoded format as returned by sslsocket ."]]
["def _handle_ns(packageName, path_item):\n\timporter = get_importer(path_item)\n\tif (importer is None):\n\t\treturn None\n\tloader = importer.find_module(packageName)\n\tif (loader is None):\n\t\treturn None\n\tmodule = sys.modules.get(packageName)\n\tif (module is None):\n\t\tmodule = sys.modules[packageName] = types.ModuleType(packageName)\n\t\tmodule.__path__ = []\n\t\t_set_parent_ns(packageName)\n\telif (not hasattr(module, '__path__')):\n\t\traise TypeError('Not\ta\tpackage:', packageName)\n\thandler = _find_adapter(_namespace_handlers, importer)\n\tsubpath = handler(importer, path_item, packageName, module)\n\tif (subpath is not None):\n\t\tpath = module.__path__\n\t\tpath.append(subpath)\n\t\tloader.load_module(packageName)\n\t\tsys_path = [((p and _normalize_cached(p)) or p) for p in sys.path]\n\t\tdef sort_key(p):\n\t\t\tparts = p.split(os.sep)\n\t\t\tparts = parts[:(- (packageName.count('.') + 1))]\n\t\t\treturn sys_path.index(_normalize_cached(os.sep.join(parts)))\n\t\tpath.sort(key=sort_key)\n\t\tmodule.__path__[:] = [_normalize_cached(p) for p in path]\n\treturn subpath\n",["ensure that named package includes a subpath of path_item ."]]
["def emails_with_users_and_watches(subject, text_template, html_template, context_vars, users_and_watches, from_email=settings.TIDINGS_FROM_ADDRESS, default_locale=settings.WIKI_DEFAULT_LANGUAGE, **extra_kwargs):\n\t@safe_translation\n\tdef _make_mail(locale, user, watch):\n\t\tcontext_vars['user'] = user\n\t\tcontext_vars['watch'] = watch[0]\n\t\tcontext_vars['watches'] = watch\n\t\tmsg = EmailMultiAlternatives((subject % context_vars), render_email(text_template, context_vars), from_email, [user.email], **extra_kwargs)\n\t\tif html_template:\n\t\t\tmsg.attach_alternative(render_email(html_template, context_vars), 'text\/html')\n\t\treturn msg\n\tfor (user, watch) in users_and_watches:\n\t\tif hasattr(user, 'locale'):\n\t\t\tlocale = user.locale\n\t\telse:\n\t\t\tlocale = default_locale\n\t\t(yield _make_mail(locale, user, watch))\n",["return iterable of emailmessages with user and watch values substituted ."]]
["def action_event_finish(context, values):\n\tconvert_datetimes(values, 'start_time', 'finish_time')\n\tsession = get_session()\n\twith session.begin():\n\t\taction = _action_get_by_request_id(context, values['instance_uuid'], values['request_id'], session)\n\t\tif (not action):\n\t\t\traise exception.InstanceActionNotFound(request_id=values['request_id'], instance_uuid=values['instance_uuid'])\n\t\tevent_ref = model_query(context, models.InstanceActionEvent, session=session).filter_by(action_id=action['id']).filter_by(event=values['event']).first()\n\t\tif (not event_ref):\n\t\t\traise exception.InstanceActionEventNotFound(action_id=action['id'], event=values['event'])\n\t\tevent_ref.update(values)\n\t\tif (values['result'].lower() == 'error'):\n\t\t\taction.update({'message': 'Error'})\n\treturn event_ref\n",["finish an event on an instance action ."]]
["def commit_on_success(using=None, read_committed=False):\n\tif callable(using):\n\t\treturn CommitOnSuccessManager(DEFAULT_DB_ALIAS, read_committed)(using)\n\telse:\n\t\treturn CommitOnSuccessManager(using, read_committed)\n",["this decorator activates commit on response ."]]
["def working_copy(remote_url, path=None, branch='master', update=True, use_sudo=False, user=None):\n\tcommand()\n\tif (path is None):\n\t\tpath = remote_url.split('\/')[(-1)]\n\t\tif path.endswith('.git'):\n\t\t\tpath = path[:(-4)]\n\tif is_dir(path, use_sudo=use_sudo):\n\t\tgit.fetch(path=path, use_sudo=use_sudo, user=user)\n\t\tgit.checkout(path=path, branch=branch, use_sudo=use_sudo, user=user)\n\t\tif update:\n\t\t\tgit.pull(path=path, use_sudo=use_sudo, user=user)\n\telif (not is_dir(path, use_sudo=use_sudo)):\n\t\tgit.clone(remote_url, path=path, use_sudo=use_sudo, user=user)\n\t\tgit.checkout(path=path, branch=branch, use_sudo=use_sudo, user=user)\n\telse:\n\t\traise ValueError('Invalid\tcombination\tof\tparameters.')\n",["require a working copy of the repository from the remote_url ."]]
["def _security_group_get_by_names(context, session, project_id, group_names):\n\tquery = _security_group_get_query(context, session=session, read_deleted='no', join_rules=False).filter_by(project_id=project_id).filter(models.SecurityGroup.name.in_(group_names))\n\tsg_models = query.all()\n\tif (len(sg_models) == len(group_names)):\n\t\treturn sg_models\n\tgroup_names_from_models = [x.name for x in sg_models]\n\tfor group_name in group_names:\n\t\tif (group_name not in group_names_from_models):\n\t\t\traise exception.SecurityGroupNotFoundForProject(project_id=project_id, security_group_id=group_name)\n",["get security group models for a project by a list of names ."]]
["def _margeff_cov_params_count(model, cov_margins, params, exog, count_ind, method, J):\n\tfor i in count_ind:\n\t\texog0 = exog.copy()\n\t\texog0[:, i] -= 1\n\t\tdfdb0 = model._derivative_predict(params, exog0, method)\n\t\texog0[:, i] += 2\n\t\tdfdb1 = model._derivative_predict(params, exog0, method)\n\t\tdfdb = (dfdb1 - dfdb0)\n\t\tif (dfdb.ndim >= 2):\n\t\t\tdfdb = (dfdb.mean(0) \/ 2)\n\t\tif (J > 1):\n\t\t\tK = (dfdb.shape[1] \/ (J - 1))\n\t\t\tcov_margins[i::K, :] = dfdb\n\t\telse:\n\t\t\tcov_margins[i, :] = dfdb\n\treturn cov_margins\n",["returns the jacobian for discrete regressors for use in margeff_cov_params ."]]
["def model_query(context, model, *args, **kwargs):\n\tsession = (kwargs.get('session') or get_session())\n\tread_deleted = (kwargs.get('read_deleted') or context.read_deleted)\n\tproject_only = kwargs.get('project_only', False)\n\tdef issubclassof_nova_base(obj):\n\t\treturn (isinstance(obj, type) and issubclass(obj, models.NovaBase))\n\tbase_model = model\n\tif (not issubclassof_nova_base(base_model)):\n\t\tbase_model = kwargs.get('base_model', None)\n\t\tif (not issubclassof_nova_base(base_model)):\n\t\t\traise Exception(_('model\tor\tbase_model\tparameter\tshould\tbe\tsubclass\tof\tNovaBase'))\n\tquery = session.query(model, *args)\n\tdefault_deleted_value = base_model.__mapper__.c.deleted.default.arg\n\tif (read_deleted == 'no'):\n\t\tquery = query.filter((base_model.deleted == default_deleted_value))\n\telif (read_deleted == 'yes'):\n\t\tpass\n\telif (read_deleted == 'only'):\n\t\tquery = query.filter((base_model.deleted != default_deleted_value))\n\telse:\n\t\traise Exception((_(\"Unrecognized\tread_deleted\tvalue\t'%s'\") % read_deleted))\n\tif (nova.context.is_user_context(context) and project_only):\n\t\tif (project_only == 'allow_none'):\n\t\t\tquery = query.filter(or_((base_model.project_id == context.project_id), (base_model.project_id == None)))\n\t\telse:\n\t\t\tquery = query.filter_by(project_id=context.project_id)\n\treturn query\n",["query helper that accounts for contexts read_deleted field ."]]
["def addToProfileMenu(profileSelection, profileType, repository):\n\tpluginFileNames = skeinforge_profile.getPluginFileNames()\n\tcraftTypeName = skeinforge_profile.getCraftTypeName()\n\tpluginModule = skeinforge_profile.getCraftTypePluginModule()\n\tprofilePluginSettings = settings.getReadRepository(pluginModule.getNewRepository())\n\tfor pluginFileName in pluginFileNames:\n\t\tskeinforge_profile.ProfileTypeMenuRadio().getFromMenuButtonDisplay(profileType, pluginFileName, repository, (craftTypeName == pluginFileName))\n\tfor profileName in profilePluginSettings.profileList.value:\n\t\tskeinforge_profile.ProfileSelectionMenuRadio().getFromMenuButtonDisplay(profileSelection, profileName, repository, (profileName == profilePluginSettings.profileListbox.value))\n",["add a profile menu ."]]
["def get_cls_kwargs(cls, _set=None):\n\ttoplevel = (_set is None)\n\tif toplevel:\n\t\t_set = set()\n\tctr = cls.__dict__.get('__init__', False)\n\thas_init = (ctr and isinstance(ctr, types.FunctionType) and isinstance(ctr.__code__, types.CodeType))\n\tif has_init:\n\t\t(names, has_kw) = inspect_func_args(ctr)\n\t\t_set.update(names)\n\t\tif ((not has_kw) and (not toplevel)):\n\t\t\treturn None\n\tif ((not has_init) or has_kw):\n\t\tfor c in cls.__bases__:\n\t\t\tif (get_cls_kwargs(c, _set) is None):\n\t\t\t\tbreak\n\t_set.discard('self')\n\treturn _set\n",["return the full set of inherited kwargs for the given cls ."]]
["def put_versioning(Bucket, Status, MFADelete=None, MFA=None, region=None, key=None, keyid=None, profile=None):\n\ttry:\n\t\tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\t\tVersioningConfiguration = {'Status': Status}\n\t\tif (MFADelete is not None):\n\t\t\tVersioningConfiguration['MFADelete'] = MFADelete\n\t\tkwargs = {}\n\t\tif (MFA is not None):\n\t\t\tkwargs['MFA'] = MFA\n\t\tconn.put_bucket_versioning(Bucket=Bucket, VersioningConfiguration=VersioningConfiguration, **kwargs)\n\t\treturn {'updated': True, 'name': Bucket}\n\texcept ClientError as e:\n\t\treturn {'updated': False, 'error': __utils__['boto3.get_error'](e)}\n",["given a valid config ."]]
["def get_host_ref(session, cluster=None):\n\tif (cluster is None):\n\t\thost_mor = session._call_method(vim_util, 'get_objects', 'HostSystem')[0].obj\n\telse:\n\t\thost_ret = session._call_method(vim_util, 'get_dynamic_property', cluster, 'ClusterComputeResource', 'host')\n\t\tif (host_ret is None):\n\t\t\treturn\n\t\tif (not host_ret.ManagedObjectReference):\n\t\t\treturn\n\t\thost_mor = host_ret.ManagedObjectReference[0]\n\treturn host_mor\n",["get reference to a host within the cluster specified ."]]
["@ffi.callback('int(void*\thandle,\tint\trevents)')\ndef _python_callback(handle, revents):\n\ttry:\n\t\tthe_watcher = ffi.from_handle(handle)\n\t\targs = the_watcher.args\n\t\tif (args is None):\n\t\t\targs = _NOARGS\n\t\tif ((len(args) > 0) and (args[0] == GEVENT_CORE_EVENTS)):\n\t\t\targs = ((revents,) + args[1:])\n\t\tthe_watcher.callback(*args)\n\texcept:\n\t\tthe_watcher._exc_info = sys.exc_info()\n\t\tthe_watcher.loop._keepaliveset.add(the_watcher)\n\t\treturn (-1)\n\telse:\n\t\tif (the_watcher in the_watcher.loop._keepaliveset):\n\t\t\treturn 0\n\t\treturn 1\n",["returns an integer having one of three values: - -1 an exception occurred during the callback and you must call :func:_python_handle_error to deal with it ."]]
["def bulk_replace(values, existing_adapter, new_adapter):\n\tassert isinstance(values, list)\n\tidset = util.IdentitySet\n\texisting_idset = idset((existing_adapter or ()))\n\tconstants = existing_idset.intersection((values or ()))\n\tadditions = idset((values or ())).difference(constants)\n\tremovals = existing_idset.difference(constants)\n\tappender = new_adapter.bulk_appender()\n\tfor member in (values or ()):\n\t\tif (member in additions):\n\t\t\tappender(member)\n\t\telif (member in constants):\n\t\t\tappender(member, _sa_initiator=False)\n\tif existing_adapter:\n\t\tremover = existing_adapter.bulk_remover()\n\t\tfor member in removals:\n\t\t\tremover(member)\n",["load a new collection ."]]
["def _match_query(query, attrs, attrs_checked):\n\tinner = query[1:(-1)]\n\tif inner.startswith(('&', '|')):\n\t\tif (inner[0] == '&'):\n\t\t\tmatchfn = all\n\t\telse:\n\t\t\tmatchfn = any\n\t\tgroups = _paren_groups(inner[1:])\n\t\treturn matchfn((_match_query(group, attrs, attrs_checked) for group in groups))\n\tif inner.startswith('!'):\n\t\treturn (not _match_query(query[2:(-1)], attrs, attrs_checked))\n\t(k, _sep, v) = inner.partition('=')\n\tattrs_checked.add(k.lower())\n\treturn _match(k, v, attrs)\n",["match an ldap query to an attribute dictionary ."]]
["def ip_network(address, strict=True):\n\ttry:\n\t\treturn IPv4Network(address, strict)\n\texcept (AddressValueError, NetmaskValueError):\n\t\tpass\n\ttry:\n\t\treturn IPv6Network(address, strict)\n\texcept (AddressValueError, NetmaskValueError):\n\t\tpass\n\tif isinstance(address, bytes):\n\t\traise AddressValueError((u'%r\tdoes\tnot\tappear\tto\tbe\tan\tIPv4\tor\tIPv6\tnetwork.\tDid\tyou\tpass\tin\ta\tbytes\t(str\tin\tPython\t2)\tinstead\tof\ta\tunicode\tobject?' % address))\n\traise ValueError((u'%r\tdoes\tnot\tappear\tto\tbe\tan\tIPv4\tor\tIPv6\tnetwork' % address))\n",["take an ip string\/int and return an object of the correct type ."]]
["def safe_join(directory, *pathnames):\n\tfor filename in pathnames:\n\t\tif (filename != ''):\n\t\t\tfilename = posixpath.normpath(filename)\n\t\tfor sep in _os_alt_seps:\n\t\t\tif (sep in filename):\n\t\t\t\traise NotFound()\n\t\tif (os.path.isabs(filename) or (filename == '..') or filename.startswith('..\/')):\n\t\t\traise NotFound()\n\t\tdirectory = os.path.join(directory, filename)\n\treturn directory\n",["joins one or more path components to the base path component intelligently ."]]
["def wrap(prefix, text, cols):\n\tpad = ('\t' * len(prefix.expandtabs()))\n\tavailable = (cols - len(pad))\n\tseq = text.split('\t')\n\tNseq = len(seq)\n\tind = 0\n\tlines = []\n\twhile (ind < Nseq):\n\t\tlastInd = ind\n\t\tind += get_split_ind(seq[ind:], available)\n\t\tlines.append(seq[lastInd:ind])\n\tret = ((prefix + '\t'.join(lines[0])) + '\\n')\n\tfor line in lines[1:]:\n\t\tret += ((pad + '\t'.join(line)) + '\\n')\n\treturn ret\n",["wrap *text* with *prefix* at length *cols* ."]]
["def recursive_repr(func):\n\trepr_running = set()\n\t@wraps(func)\n\tdef wrapper(self):\n\t\tkey = (id(self), get_ident())\n\t\tif (key in repr_running):\n\t\t\treturn '...'\n\t\trepr_running.add(key)\n\t\ttry:\n\t\t\treturn func(self)\n\t\tfinally:\n\t\t\trepr_running.discard(key)\n\treturn wrapper\n",["decorator to make a repr function return fillvalue for a recursive call ."]]
["def assess():\n\tassess_tables()\n\timpact_tables()\n\ttablename = ('%s_%s' % (module, resourcename))\n\ttable = db[tablename]\n\tdef prep(r):\n\t\tif (session.s3.mobile and (r.method == 'create') and r.interactive):\n\t\t\tredirect(URL(f='assess_short_mobile'))\n\t\treturn True\n\tresponse.s3.prep = prep\n\ttabs = [(T('Edit\tDetails'), None), (T('Baselines'), 'baseline'), (T('Impacts'), 'impact'), (T('Summary'), 'summary')]\n\trheader = (lambda r: assess_rheader(r, tabs))\n\treturn s3_rest_controller(rheader=rheader)\n",["restful crud controller ."]]
["def paginate(context, window=DEFAULT_WINDOW, hashtag=''):\n\ttry:\n\t\tpaginator = context['paginator']\n\t\tpage_obj = context['page_obj']\n\t\tpage_suffix = context.get('page_suffix', '')\n\t\tpage_range = paginator.page_range\n\t\trecords = {'first': (1 + ((page_obj.number - 1) * paginator.per_page))}\n\t\trecords['last'] = ((records['first'] + paginator.per_page) - 1)\n\t\tif ((records['last'] + paginator.orphans) >= paginator.count):\n\t\t\trecords['last'] = paginator.count\n\t\tfirst = set(page_range[:window])\n\t\tlast = set(page_range[(- window):])\n\t\tcurrent_start = ((page_obj.number - 1) - window)\n\t\tif (current_start < 0):\n\t\t\tcurrent_start = 0\n\t\tcurrent_end = ((page_obj.number - 1) + window)\n\t\tif (current_end < 0):\n\t\t\tcurrent_end = 0\n\t\tcurrent = set(page_range[current_start:current_end])\n\t\tpages = []\n\t\tif (len(first.intersection(current)) == 0):\n\t\t\tfirst_list = list(first)\n\t\t\tfirst_list.sort()\n\t\t\tsecond_list = list(current)\n\t\t\tsecond_list.sort()\n\t\t\tpages.extend(first_list)\n\t\t\tdiff = (second_list[0] - first_list[(-1)])\n\t\t\tif (diff == 2):\n\t\t\t\tpages.append((second_list[0] - 1))\n\t\t\telif (diff == 1):\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tpages.append(None)\n\t\t\tpages.extend(second_list)\n\t\telse:\n\t\t\tunioned = list(first.union(current))\n\t\t\tunioned.sort()\n\t\t\tpages.extend(unioned)\n\t\tif (len(current.intersection(last)) == 0):\n\t\t\tsecond_list = list(last)\n\t\t\tsecond_list.sort()\n\t\t\tdiff = (second_list[0] - pages[(-1)])\n\t\t\tif (diff == 2):\n\t\t\t\tpages.append((second_list[0] - 1))\n\t\t\telif (diff == 1):\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tpages.append(None)\n\t\t\tpages.extend(second_list)\n\t\telse:\n\t\t\tdifferenced = list(last.difference(current))\n\t\t\tdifferenced.sort()\n\t\t\tpages.extend(differenced)\n\t\tto_return = {'MEDIA_URL': settings.MEDIA_URL, 'request': context['request'], 'pages': pages, 'records': records, 'page_obj': page_obj, 'paginator': paginator, 'hashtag': hashtag, 'is_paginated': (paginator.count > paginator.per_page), 'page_suffix': page_suffix}\n\t\tif ('request' in context):\n\t\t\tgetvars = context['request'].GET.copy()\n\t\t\tif (('page%s' % page_suffix) in getvars):\n\t\t\t\tdel getvars[('page%s' % page_suffix)]\n\t\t\tif (len(getvars.keys()) > 0):\n\t\t\t\tto_return['getvars'] = ('&%s' % getvars.urlencode())\n\t\t\telse:\n\t\t\t\tto_return['getvars'] = ''\n\t\treturn to_return\n\texcept KeyError as AttributeError:\n\t\treturn {}\n",["get a paginator ."]]
["def unquote_unreserved(uri):\n\tparts = uri.split('%')\n\tfor i in range(1, len(parts)):\n\t\th = parts[i][0:2]\n\t\tif ((len(h) == 2) and h.isalnum()):\n\t\t\ttry:\n\t\t\t\tc = chr(int(h, 16))\n\t\t\texcept ValueError:\n\t\t\t\traise InvalidURL((\"Invalid\tpercent-escape\tsequence:\t'%s'\" % h))\n\t\t\tif (c in UNRESERVED_SET):\n\t\t\t\tparts[i] = (c + parts[i][2:])\n\t\t\telse:\n\t\t\t\tparts[i] = ('%' + parts[i])\n\t\telse:\n\t\t\tparts[i] = ('%' + parts[i])\n\treturn ''.join(parts)\n",["un-escape any percent-escape sequences in a uri that are unreserved characters ."]]
["def unescapeHTMLEntities(text):\n\tdef fixup(m):\n\t\ttext = m.group(0)\n\t\tif (text[:2] == '&#'):\n\t\t\ttry:\n\t\t\t\tif (text[:3] == '&#x'):\n\t\t\t\t\treturn unichr(int(text[3:(-1)], 16))\n\t\t\t\telse:\n\t\t\t\t\treturn unichr(int(text[2:(-1)]))\n\t\t\texcept ValueError:\n\t\t\t\tpass\n\t\telse:\n\t\t\ttry:\n\t\t\t\ttext = unichr(htmlentitydefs.name2codepoint[text[1:(-1)]])\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\treturn text\n\treturn re.sub('&#?\\\\w+;', fixup, text)\n",["removes html or xml character references and entities from a text string ."]]
["def storify(f, *requireds, **defaults):\n\tstor = Storage()\n\tfor k in (requireds + tuple(f.keys())):\n\t\tv = f[k]\n\t\tif isinstance(v, list):\n\t\t\tv = v[(-1)]\n\t\tif hasattr(v, 'value'):\n\t\t\tv = v.value\n\t\tsetattr(stor, k, v)\n\tfor (k, v) in defaults.iteritems():\n\t\tresult = v\n\t\tif hasattr(stor, k):\n\t\t\tresult = stor[k]\n\t\tif ((v == ()) and (not isinstance(result, tuple))):\n\t\t\tresult = (result,)\n\t\tsetattr(stor, k, result)\n\treturn stor\n",["creates a storage object from dictionary d ."]]
["def tsem(a, limits=None, inclusive=(True, True), axis=0, ddof=1):\n\ta = ma.asarray(a).ravel()\n\tif (limits is None):\n\t\tn = float(a.count())\n\t\treturn (a.std(axis=axis, ddof=ddof) \/ ma.sqrt(n))\n\tam = trima(a.ravel(), limits, inclusive)\n\tsd = np.sqrt(am.var(axis=axis, ddof=ddof))\n\treturn (sd \/ np.sqrt(am.count()))\n",["compute the trimmed standard error of the mean ."]]
["def bytes_to_long(s):\n\tacc = 0L\n\tunpack = struct.unpack\n\tlength = len(s)\n\tif (length % 4):\n\t\textra = (4 - (length % 4))\n\t\ts = ((b('\\x00') * extra) + s)\n\t\tlength = (length + extra)\n\tfor i in range(0, length, 4):\n\t\tacc = ((acc << 32) + unpack('>I', s[i:(i + 4)])[0])\n\treturn acc\n",["bytes_to_long : long convert a byte string to a long integer ."]]
["def list_nodes(call=None):\n\tif (call == 'action'):\n\t\traise SaltCloudSystemExit('The\tlist_nodes\tfunction\tmust\tbe\tcalled\twith\t-f\tor\t--function.')\n\tnodes = list_nodes_full()\n\tret = {}\n\tfor (instance_id, full_node) in nodes.items():\n\t\tret[instance_id] = {'id': full_node['id'], 'image': full_node['image'], 'size': full_node['size'], 'state': full_node['state'], 'public_ips': full_node['public_ips'], 'private_ips': full_node['private_ips']}\n\treturn ret\n",["list vms on this azure account ."]]
["def writeOutput(fileName=''):\n\tfileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName)\n\tif (fileName != ''):\n\t\tskeinforge_craft.writeChainTextWithNounMessage(fileName, 'unpause')\n",["write scalable vector graphics for a skeinforge gcode file ."]]
["def start_server(data_stream, port=5557, hwm=20):\n\tlogging.basicConfig(level='INFO')\n\tcontext = zmq.Context()\n\tsocket = context.socket(zmq.PUSH)\n\tsocket.set_hwm(hwm)\n\tsocket.bind('tcp:\/\/*:{}'.format(port))\n\tit = data_stream\n\tlogger.info('server\tstarted')\n\twhile True:\n\t\ttry:\n\t\t\tdata = next(it)\n\t\t\tstop = False\n\t\t\tlogger.debug('sending\t{}\tarrays'.format(len(data)))\n\t\texcept StopIteration:\n\t\t\tit = data_stream\n\t\t\tdata = None\n\t\t\tstop = True\n\t\t\tlogger.debug('sending\tStopIteration')\n\t\tsend_arrays(socket, data, stop=stop)\n",["start the server mode ."]]
["def get_credential_name(tenant_id, credential_name):\n\tsession = db.get_session()\n\ttry:\n\t\tcred = session.query(l2network_models.Credential).filter_by(tenant_id=tenant_id).filter_by(credential_name=credential_name).one()\n\t\treturn cred\n\texcept exc.NoResultFound:\n\t\traise c_exc.CredentialNameNotFound(credential_name=credential_name, tenant_id=tenant_id)\n",["lists the creds for given a cred_name and tenant_id ."]]
["def getSquareValues(pixelDictionary, x, y):\n\tsquareValues = []\n\tfor xStep in xrange((x - 1), (x + 2)):\n\t\tfor yStep in xrange((y - 1), (y + 2)):\n\t\t\tstepKey = getStepKey(xStep, yStep)\n\t\t\tif (stepKey in pixelDictionary):\n\t\t\t\tsquareValues += pixelDictionary[stepKey]\n\treturn squareValues\n",["get a list of the values in a square around the x and y pixel coordinates ."]]
["def make_signed_jwt(signer, payload):\n\theader = {'typ': 'JWT', 'alg': 'RS256'}\n\tsegments = [_urlsafe_b64encode(_json_encode(header)), _urlsafe_b64encode(_json_encode(payload))]\n\tsigning_input = '.'.join(segments)\n\tsignature = signer.sign(signing_input)\n\tsegments.append(_urlsafe_b64encode(signature))\n\tlogging.debug(str(segments))\n\treturn '.'.join(segments)\n",["make a signed jwt ."]]
["def _setHTTPTimeout():\n\tif conf.timeout:\n\t\tinfoMsg = 'setting\tthe\tHTTP\ttimeout'\n\t\tlogger.log(CUSTOM_LOGGING.SYSINFO, infoMsg)\n\t\tconf.timeout = float(conf.timeout)\n\t\tif (conf.timeout < 3.0):\n\t\t\twarnMsg = 'the\tminimum\tHTTP\ttimeout\tis\t3\tseconds,\tpocsuite\twill\tgoing\tto\treset\tit'\n\t\t\tlogger.log(CUSTOM_LOGGING.WARNING, warnMsg)\n\t\t\tconf.timeout = 3.0\n\telse:\n\t\tconf.timeout = 30.0\n\tsocket.setdefaulttimeout(conf.timeout)\n",["set the http timeout ."]]
["def test_deleted_folder_on_select(monkeypatch, generic_client, constants):\n\tdef raise_invalid_folder_exc(*args, **kwargs):\n\t\traise imapclient.IMAPClient.Error(\"select\tfailed:\t'[TRYCREATE]\tEXAMINE\terror\t-\tFolder\tdoes\tnot\texist\tor\tserver\tencountered\tan\terror\")\n\tmonkeypatch.setattr('imapclient.IMAPClient.select_folder', raise_invalid_folder_exc)\n\twith pytest.raises(FolderMissingError):\n\t\tgeneric_client.select_folder('missing_folder', (lambda : True))\n",["test that a select failed examine error specifying that a folder doesnt exist is converted into a foldermissingerror ."]]
["def proxy_bypass_environment(host):\n\tno_proxy = (os.environ.get('no_proxy', '') or os.environ.get('NO_PROXY', ''))\n\tif (no_proxy == '*'):\n\t\treturn 1\n\t(hostonly, port) = splitport(host)\n\tfor name in no_proxy.split(','):\n\t\tif (name and (hostonly.endswith(name) or host.endswith(name))):\n\t\t\treturn 1\n\treturn 0\n",["test if proxies should not be used for a particular host ."]]
["def merge(file, names, config, coord):\n\tinputs = get_tiles(names, config, coord)\n\toutput = {'type': 'Topology', 'transform': inputs[0]['transform'], 'objects': dict(), 'arcs': list()}\n\tfor (name, input) in zip(names, inputs):\n\t\tfor (index, object) in enumerate(input['objects'].values()):\n\t\t\tif (len(input['objects']) > 1):\n\t\t\t\toutput['objects'][('%(name)s-%(index)d' % locals())] = object\n\t\t\telse:\n\t\t\t\toutput['objects'][name] = object\n\t\t\tfor geometry in object['geometries']:\n\t\t\t\tupdate_arc_indexes(geometry, output['arcs'], input['arcs'])\n\tfile.write(json.dumps(output, separators=(',', ':')).encode('utf8'))\n",["merge one app config into another ."]]
["def getGeometryOutput(elementNode):\n\tderivation = HeightmapDerivation(elementNode)\n\theightGrid = derivation.heightGrid\n\tif (derivation.fileName != ''):\n\t\theightGrid = getHeightGrid(archive.getAbsoluteFolderPath(elementNode.getOwnerDocument().fileName, derivation.fileName))\n\treturn getGeometryOutputByHeightGrid(derivation, elementNode, heightGrid)\n",["get geometry output from paths ."]]
["def tab_in_leading(s):\n\tn = (len(s) - len(s.lstrip()))\n\tif (not (s[n:(n + 3)] in ['...', '>>>'])):\n\t\tcheck = s[:n]\n\telse:\n\t\tsmore = s[(n + 3):]\n\t\tcheck = (s[:n] + smore[:(len(smore) - len(smore.lstrip()))])\n\treturn (not (check.expandtabs() == check))\n",["returns true if there are tabs in the leading whitespace of a line ."]]
["def find_instrument_devices(track_or_chain):\n\tinstrument = find_if((lambda d: (d.type == Live.Device.DeviceType.instrument)), track_or_chain.devices)\n\tif (instrument and (not instrument.can_have_drum_pads) and instrument.can_have_chains):\n\t\treturn chain([instrument], *imap(find_instrument_devices, instrument.chains))\n\treturn []\n",["returns a list with all instrument rack descendants from a track or chain ."]]
["def _getSimplePatterns(numOnes, numPatterns):\n\tnumCols = (numOnes * numPatterns)\n\tp = []\n\tfor i in xrange(numPatterns):\n\t\tx = numpy.zeros(numCols, dtype='float32')\n\t\tx[(i * numOnes):((i + 1) * numOnes)] = 1\n\t\tp.append(x)\n\treturn p\n",["very simple patterns ."]]
["@process_request_body\ndef json_processor(entity):\n\tbody = entity.fp.read()\n\ttry:\n\t\tcherrypy.serving.request.unserialized_data = json.loads(body)\n\texcept ValueError:\n\t\traise cherrypy.HTTPError(400, 'Invalid\tJSON\tdocument')\n\tcherrypy.serving.request.raw_body = body\n",["read application\/json data into request ."]]
["def getTransformedOutlineByPath(elementNode, path, yAxisPointingUpward):\n\taroundsFromPath = intercircle.getAroundsFromPath(path, getStrokeRadius(elementNode))\n\treturn getChainMatrixSVGIfNecessary(elementNode, yAxisPointingUpward).getTransformedPaths(aroundsFromPath)\n",["get the outline from the path ."]]
["def test_roberts_diagonal1():\n\timage = np.tri(10, 10, 0)\n\texpected = (~ (np.tri(10, 10, (-1)).astype(bool) | np.tri(10, 10, (-2)).astype(bool).transpose()))\n\texpected = _mask_filter_result(expected, None)\n\tresult = filters.roberts(image).astype(bool)\n\tassert_close(result, expected)\n",["roberts filter on a diagonal edge should be a diagonal line ."]]
["def emptytrash():\n\tfinder = _getfinder()\n\targs = {}\n\tattrs = {}\n\targs['----'] = aetypes.ObjectSpecifier(want=aetypes.Type('prop'), form='prop', seld=aetypes.Type('trsh'), fr=None)\n\t(_reply, args, attrs) = finder.send('fndr', 'empt', args, attrs)\n\tif args.has_key('errn'):\n\t\traise aetools.Error, aetools.decodeerror(args)\n",["empty the trash ."]]
["def new_figure_manager(num, *args, **kwargs):\n\t_focus = windowing.FocusManager()\n\tFigureClass = kwargs.pop('FigureClass', Figure)\n\tfigure = FigureClass(*args, **kwargs)\n\twindow = Tk.Tk()\n\tcanvas = FigureCanvasTkAgg(figure, master=window)\n\tfigManager = FigureManagerTkAgg(canvas, num, window)\n\tif matplotlib.is_interactive():\n\t\tfigManager.show()\n\treturn figManager\n",["create a new figure manager instance ."]]
["def fetch_stream_from_url(url, config, data=None, handlers=None):\n\t(return_code, return_message, response) = open_url(url, config, data=data, handlers=handlers)\n\tif (return_code and (return_code == httplib.OK)):\n\t\treturn response\n\telse:\n\t\traise URLFetchError(return_message)\n",["returns data retrieved from a url ."]]
["def net_if_stats():\n\tnames = net_io_counters().keys()\n\tret = {}\n\tfor name in names:\n\t\t(isup, duplex, speed, mtu) = cext_posix.net_if_stats(name)\n\t\tif hasattr(_common, 'NicDuplex'):\n\t\t\tduplex = _common.NicDuplex(duplex)\n\t\tret[name] = _common.snicstats(isup, duplex, speed, mtu)\n\treturn ret\n",["get nic stats ."]]
["def load_custom_properties():\n\tfilename = 'schema-image.json'\n\tmatch = CONF.find_file(filename)\n\tif match:\n\t\tschema_file = open(match)\n\t\tschema_data = schema_file.read()\n\t\treturn json.loads(schema_data)\n\telse:\n\t\tmsg = _('Could\tnot\tfind\tschema\tproperties\tfile\t%s.\tContinuing\twithout\tcustom\tproperties')\n\t\tLOG.warn((msg % filename))\n\t\treturn {}\n",["find the schema properties files and load them into a dict ."]]
["def test_nm2__wrong_nn_obj():\n\tratio = 'auto'\n\tnn = 'rnd'\n\tnm2 = NearMiss(ratio=ratio, random_state=RND_SEED, version=VERSION_NEARMISS, return_indices=True, n_neighbors=nn)\n\tassert_raises(ValueError, nm2.fit_sample, X, Y)\n",["test either if an error is raised with wrong nn object ."]]
["def unescape(text):\n\trv = ''\n\ti = 0\n\twhile (i < len(text)):\n\t\tif (((i + 1) < len(text)) and (text[i] == '\\\\')):\n\t\t\trv += text[(i + 1)]\n\t\t\ti += 1\n\t\telse:\n\t\t\trv += text[i]\n\t\ti += 1\n\treturn rv\n",["return a string with nulls removed or restored to backslashes ."]]
["def parse(json_string):\n\ttry:\n\t\tjson_data = json.loads(json_string)\n\texcept:\n\t\traise SchemaParseException(('Error\tparsing\tJSON:\t%s' % json_string))\n\tnames = Names()\n\treturn make_avsc_object(json_data, names)\n",["parse a string or file-like object into a tree ."]]
["def compress_kml(kml):\n\tkmz = cStringIO.StringIO()\n\tzf = zipfile.ZipFile(kmz, 'a', zipfile.ZIP_DEFLATED)\n\tzf.writestr('doc.kml', kml.encode(settings.DEFAULT_CHARSET))\n\tzf.close()\n\tkmz.seek(0)\n\treturn kmz.read()\n",["returns compressed kmz from the given kml string ."]]
["def notify(conf, context, topic, msg, envelope):\n\ttopic = topic.replace('.', '-')\n\tcast(conf, context, topic, msg, envelope=envelope)\n",["notifies the recipient of the desired event given the model ."]]
["def zeros_like(a, dtype=None):\n\tif (dtype is None):\n\t\tdtype = a.dtype\n\treturn zeros(a.shape, dtype=dtype)\n",["equivalent of numpy ."]]
["def service_get_all(context, backend_match_level=None, **filters):\n\treturn IMPL.service_get_all(context, backend_match_level, **filters)\n",["get all services that match the criteria ."]]
["def guess_locale_from_lang_windows(lang):\n\tlocale_n = str(LEGAL_VALUES[u'_WINDOWS_LOCALE_GUESSES'].get(lang, None))\n\tif (not is_valid_locale(locale_n)):\n\t\tlocale_n = None\n\treturn locale_n\n",["guess a locale ."]]
["def sitemap_urls_from_robots(robots_text):\n\tfor line in robots_text.splitlines():\n\t\tif line.lstrip().startswith('Sitemap:'):\n\t\t\t(yield line.split(':', 1)[1].strip())\n",["return an iterator over all sitemap urls contained in the given robots ."]]
["@config.command()\n@click.argument('pattern', default='*', required=False)\n@configuration\ndef list(pattern):\n\tfrom fnmatch import fnmatch\n\tfrom sentry.options import default_manager as manager\n\tfor key in manager.all():\n\t\tif fnmatch(key.name, pattern):\n\t\t\tclick.echo(('%s\t%s' % (key.name, key.type.name.upper())))\n",["return a list of all locale identifiers for which locale data is available ."]]
["def setup_input(pin, pull_mode):\n\timport Adafruit_BBIO.GPIO as GPIO\n\tGPIO.setup(pin, GPIO.IN, (GPIO.PUD_DOWN if (pull_mode == 'DOWN') else GPIO.PUD_UP))\n",["setup a gpio as input ."]]
["@register.inclusion_tag(engine.get_template('inclusion.html'))\ndef inclusion_one_param_from_template(arg):\n\treturn {'result': ('inclusion_one_param_from_template\t-\tExpected\tresult:\t%s' % arg)}\n",["expected inclusion_one_param_from_template __doc__ ."]]
["def get_c_type(name):\n\tif (name in asdl.builtin_types):\n\t\treturn name\n\telse:\n\t\treturn ('%s_ty' % name)\n",["return a string for the c name of the type ."]]
["def connect_to_region(region_name, **kw_params):\n\tfor region in regions():\n\t\tif (region.name == region_name):\n\t\t\treturn region.connect(**kw_params)\n\treturn None\n",["given a valid region name ."]]
["def convert_DateProperty(model, prop, kwargs):\n\tif (prop.auto_now or prop.auto_now_add):\n\t\treturn None\n\tkwargs.setdefault('format', '%Y-%m-%d')\n\treturn f.DateField(**kwargs)\n",["returns a form field for a db ."]]
["def gen_keys(key='', key_path_dir=''):\n\tkey_basename = ('key-' + uuid4().hex)\n\tif (not key_path_dir):\n\t\tkey_path_dir = os.path.join(KEY_DIR, 'role_key', key_basename)\n\tprivate_key = os.path.join(key_path_dir, 'id_rsa')\n\tpublic_key = os.path.join(key_path_dir, 'id_rsa.pub')\n\tmkdir(key_path_dir, mode=755)\n\tif (not key):\n\t\tkey = RSAKey.generate(2048)\n\t\tkey.write_private_key_file(private_key)\n\telse:\n\t\tkey_file = os.path.join(key_path_dir, 'id_rsa')\n\t\twith open(key_file, 'w') as f:\n\t\t\tf.write(key)\n\t\t\tf.close()\n\t\twith open(key_file) as f:\n\t\t\ttry:\n\t\t\t\tkey = RSAKey.from_private_key(f)\n\t\t\texcept SSHException as e:\n\t\t\t\tshutil.rmtree(key_path_dir, ignore_errors=True)\n\t\t\t\traise SSHException(e)\n\tos.chmod(private_key, 420)\n\twith open(public_key, 'w') as content_file:\n\t\tfor data in [key.get_name(), '\t', key.get_base64(), ('\t%s@%s' % ('jumpserver', os.uname()[1]))]:\n\t\t\tcontent_file.write(data)\n\treturn key_path_dir\n",["generate rsa keys of nbits bits ."]]
["def _env_is_exposed(env):\n\treturn salt.utils.check_whitelist_blacklist(env, whitelist=__opts__['hgfs_env_whitelist'], blacklist=__opts__['hgfs_env_blacklist'])\n",["check if an environment is exposed by comparing it against a whitelist and blacklist ."]]
["def startLoggingWithObserver(observer, setStdout=1):\n\ttheLogPublisher._startLogging(observer, setStdout)\n\tmsg('Log\topened.')\n",["initialize logging to a specified observer ."]]
["@contextlib.contextmanager\ndef remove_path_on_error(path):\n\ttry:\n\t\t(yield)\n\texcept Exception:\n\t\twith excutils.save_and_reraise_exception():\n\t\t\tdelete_if_exists(path)\n",["protect code that wants to operate on path atomically ."]]
["def parsePWDResponse(response):\n\tmatch = re.search('\"(.*)\"', response)\n\tif match:\n\t\treturn match.groups()[0]\n\telse:\n\t\treturn None\n",["returns the path from a response to a pwd command ."]]
["def all_config_files():\n\tuser = user_config_files()\n\tif os.path.exists('setup.cfg'):\n\t\treturn (user + ['setup.cfg'])\n\treturn user\n",["return path to any existing user config files ."]]
["def get_command_from_state(state):\n\tcommand = None\n\tif (state == 'present'):\n\t\tcommand = 'vrouter-loopback-interface-add'\n\tif (state == 'absent'):\n\t\tcommand = 'vrouter-loopback-interface-remove'\n\treturn command\n",["this method gets appropriate command name for the state specified ."]]
["def writeOutput(fileName=''):\n\tfileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName)\n\tif (fileName != ''):\n\t\tskeinforge_craft.writeChainTextWithNounMessage(fileName, 'unpause')\n",["drill a gcode linear move file ."]]
["def _format_info(data):\n\tgecos_field = data.pw_gecos.split(',', 3)\n\twhile (len(gecos_field) < 4):\n\t\tgecos_field.append('')\n\treturn {'gid': data.pw_gid, 'groups': list_groups(data.pw_name), 'home': data.pw_dir, 'name': data.pw_name, 'passwd': data.pw_passwd, 'shell': data.pw_shell, 'uid': data.pw_uid, 'fullname': gecos_field[0], 'roomnumber': gecos_field[1], 'workphone': gecos_field[2], 'homephone': gecos_field[3]}\n",["return formatted information in a pretty way ."]]
["@environmentfilter\ndef do_first(environment, seq):\n\ttry:\n\t\treturn next(iter(seq))\n\texcept StopIteration:\n\t\treturn environment.undefined('No\tfirst\titem,\tsequence\twas\tempty.')\n",["return the first item of a sequence ."]]
["def show(config_file=False):\n\tcmd = 'sysctl'\n\tret = {}\n\tout = __salt__['cmd.run_stdout'](cmd, output_loglevel='trace')\n\tfor line in out.splitlines():\n\t\tif ((not line) or ('=' not in line)):\n\t\t\tcontinue\n\t\tcomps = line.split('=', 1)\n\t\tret[comps[0]] = comps[1]\n\treturn ret\n",["called from digits ."]]
["def validate_path_exists(s):\n\tif os.path.exists(s):\n\t\treturn s\n\telse:\n\t\traise RuntimeError(('\"%s\"\tshould\tbe\ta\tpath\tbut\tit\tdoes\tnot\texist' % s))\n",["if s is a path ."]]
["def definite_article(word, gender=MALE):\n\tif (MASCULINE in gender):\n\t\treturn (((PLURAL in gender) and 'los') or 'el')\n\treturn (((PLURAL in gender) and 'las') or 'la')\n",["returns the definite article for a given word ."]]
["def _equalsIgnoreCase(a, b):\n\treturn ((a == b) or (string.lower(a) == string.lower(b)))\n",["return true iff a and b have the same lowercase representation ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load if boto is available ."]]
["def getNewDerivation(elementNode):\n\treturn evaluate.EmptyObject(elementNode)\n",["get new derivation ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load if boto is available ."]]
["def _api_version(profile=None, **connection_args):\n\tglobal _TENANT_ID\n\tglobal _OS_IDENTITY_API_VERSION\n\ttry:\n\t\tif (float(__salt__['keystone.api_version'](profile=profile, **connection_args).strip('v')) >= 3):\n\t\t\t_TENANT_ID = 'project_id'\n\t\t\t_OS_IDENTITY_API_VERSION = 3\n\texcept KeyError:\n\t\tpass\n",["api: accepts output ."]]
["def list_clusters(call=None):\n\tif (call == 'action'):\n\t\traise SaltCloudSystemExit('The\tlist_clusters\tfunction\tmust\tbe\tcalled\twith\t-f\tor\t--function.')\n\t(server, user, password) = _get_xml_rpc()\n\tauth = ':'.join([user, password])\n\tcluster_pool = server.one.clusterpool.info(auth)[1]\n\tclusters = {}\n\tfor cluster in _get_xml(cluster_pool):\n\t\tclusters[cluster.find('NAME').text] = _xml_to_dict(cluster)\n\treturn clusters\n",["returns a list of clusters associated with a given service instance ."]]
["def responder(f):\n\treturn update_wrapper((lambda *a: f(*a)(*a[(-2):])), f)\n",["marks a function as responder ."]]
["def add(song):\n\tif (not g.userhist.get('history')):\n\t\tg.userhist['history'] = Playlist('history')\n\tg.userhist['history'].songs.append(song)\n\tsave()\n",["add a non-fuzzy translation to the dictionary ."]]
["def test_find_module_2():\n\tnt.assert_is_none(mp.find_module('xmod', []))\n",["testing sys ."]]
["def to_local_timezone(datetime):\n\treturn get_i18n().to_local_timezone(datetime)\n",["see :meth:i18n ."]]
["def get_readable_field_data_type(field):\n\treturn (field.description % field.__dict__)\n",["returns the description for a given field type ."]]
["def convert_ByteStringProperty(model, prop, kwargs):\n\treturn get_TextField(kwargs)\n",["returns a form field for a db ."]]
["def processXMLElement(xmlElement):\n\tgroup.processShape(Difference, xmlElement)\n",["process the xml element ."]]
["@register.filter(is_safe=False)\ndef unlocalize(value):\n\treturn force_text(value)\n",["forces a value to be rendered as a non-localized value ."]]
["def demo_repr_rule_format():\n\tpostag(ruleformat='repr')\n",["exemplify repr (see also str and rule ."]]
["def itervalues(d):\n\treturn getattr(d, _itervalues)()\n",["return an iterator over the values of a dictionary ."]]
["def sendStayAwake():\n\treturn False\n",["sends a signal to your system to indicate that the computer is in use and should not sleep ."]]
["def describe(Name, region=None, key=None, keyid=None, profile=None):\n\ttry:\n\t\tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\t\trule = conn.describe_rule(Name=Name)\n\t\tif rule:\n\t\t\tkeys = ('Name', 'Arn', 'EventPattern', 'ScheduleExpression', 'State', 'Description', 'RoleArn')\n\t\t\treturn {'rule': dict([(k, rule.get(k)) for k in keys])}\n\t\telse:\n\t\t\treturn {'rule': None}\n\texcept ClientError as e:\n\t\terr = __utils__['boto3.get_error'](e)\n\t\tif (e.response.get('Error', {}).get('Code') == 'RuleNotFoundException'):\n\t\t\treturn {'error': 'Rule\t{0}\tnot\tfound'.format(Rule)}\n\t\treturn {'error': __utils__['boto3.get_error'](e)}\n",["given a trail name describe its properties ."]]
["@handle_response_format\n@treeio_login_required\ndef status_edit(request, status_id, response_format='html'):\n\tstatus = get_object_or_404(TicketStatus, pk=status_id)\n\tif ((not request.user.profile.has_permission(status, mode='w')) and (not request.user.profile.is_admin('treeio_services'))):\n\t\treturn user_denied(request, \"You\tdon't\thave\taccess\tto\tthis\tTicket\tStatus\", response_format)\n\tif request.POST:\n\t\tif ('cancel' not in request.POST):\n\t\t\tform = TicketStatusForm(request.user.profile, request.POST, instance=status)\n\t\t\tif form.is_valid():\n\t\t\t\tstatus = form.save()\n\t\t\t\treturn HttpResponseRedirect(reverse('services_status_view', args=[status.id]))\n\t\telse:\n\t\t\treturn HttpResponseRedirect(reverse('services_status_view', args=[status.id]))\n\telse:\n\t\tform = TicketStatusForm(request.user.profile, instance=status)\n\tcontext = _get_default_context(request)\n\tcontext.update({'form': form, 'status': status})\n\treturn render_to_response('services\/status_edit', context, context_instance=RequestContext(request), response_format=response_format)\n",["ticketstatus edit ."]]
["def addSparseEndpointsFromSegment(doubleInfillWidth, endpoints, horizontalSegmentsDictionary, horizontalSegmentsDictionaryKey, infillSolidity, removedEndpoints, segment, solidSurfaceThickness, surroundingXIntersections):\n\tif (infillSolidity > 0.0):\n\t\tif (int(round((round((float(horizontalSegmentsDictionaryKey) * infillSolidity)) \/ infillSolidity))) == horizontalSegmentsDictionaryKey):\n\t\t\tendpoints += segment\n\t\t\treturn\n\t\tif (abs((segment[0].point - segment[1].point)) < doubleInfillWidth):\n\t\t\tendpoints += segment\n\t\t\treturn\n\t\tif (not isSegmentAround(horizontalSegmentsDictionary, (horizontalSegmentsDictionaryKey - 1), segment)):\n\t\t\tendpoints += segment\n\t\t\treturn\n\t\tif (not isSegmentAround(horizontalSegmentsDictionary, (horizontalSegmentsDictionaryKey + 1), segment)):\n\t\t\tendpoints += segment\n\t\t\treturn\n\tif (solidSurfaceThickness == 0):\n\t\tremovedEndpoints += segment\n\t\treturn\n\tif isSegmentCompletelyInAnIntersection(segment, surroundingXIntersections):\n\t\tremovedEndpoints += segment\n\t\treturn\n\tendpoints += segment\n",["add sparse endpoints from a segment ."]]
["def getLargestCenterOutsetLoopFromLoop(loop, radius, thresholdRatio=0.9):\n\tif (radius == 0.0):\n\t\treturn loop\n\tradius = abs(radius)\n\tpoints = getPointsFromLoop(loop, radius, thresholdRatio)\n\tcenters = getCentersFromPoints(points, (globalIntercircleMultiplier * radius))\n\tlargestCenterOutset = None\n\tlargestOutsetArea = (-987654321.0)\n\tfor center in centers:\n\t\toutset = getSimplifiedInsetFromClockwiseLoop(center, radius)\n\t\tif isLargeSameDirection(outset, center, radius):\n\t\t\tif (euclidean.isPathInsideLoop(loop, outset) != euclidean.isWiddershins(loop)):\n\t\t\t\tcenterOutset = CenterOutset(center, outset)\n\t\t\t\toutsetArea = abs(euclidean.getAreaLoop(outset))\n\t\t\t\tif (outsetArea > largestOutsetArea):\n\t\t\t\t\tlargestOutsetArea = outsetArea\n\t\t\t\t\tlargestCenterOutset = centerOutset\n\tif (largestCenterOutset == None):\n\t\treturn None\n\tlargestCenterOutset.center = euclidean.getSimplifiedLoop(largestCenterOutset.center, radius)\n\treturn largestCenterOutset\n",["get the largest circle outset loop from the loop ."]]
["def rename_ep_file(cur_path, new_path, old_path_length=0):\n\tif ((old_path_length == 0) or (old_path_length > len(cur_path))):\n\t\t(cur_file_name, cur_file_ext) = os.path.splitext(cur_path)\n\telse:\n\t\tcur_file_ext = cur_path[old_path_length:]\n\t\tcur_file_name = cur_path[:old_path_length]\n\tif (cur_file_ext[1:] in subtitleExtensions):\n\t\tsublang = os.path.splitext(cur_file_name)[1][1:]\n\t\tfrom sickrage.core.searchers import subtitle_searcher\n\t\tif subtitle_searcher.isValidLanguage(sublang):\n\t\t\tcur_file_ext = ((u'.' + sublang) + cur_file_ext)\n\tnew_path += cur_file_ext\n\tmake_dirs(os.path.dirname(new_path))\n\ttry:\n\t\tsickrage.srCore.srLogger.info((u'Renaming\tfile\tfrom\t%s\tto\t%s' % (cur_path, new_path)))\n\t\tmoveFile(cur_path, new_path)\n\texcept (OSError, IOError) as e:\n\t\tsickrage.srCore.srLogger.error((u'Failed\trenaming\t%s\tto\t%s\t:\t%r' % (cur_path, new_path, e)))\n\t\treturn False\n\tdelete_empty_folders(os.path.dirname(cur_path))\n\treturn True\n",["creates all folders needed to move a file to its new location ."]]
["def translate_exception(req, e):\n\tif (not hasattr(req, 'best_match_language')):\n\t\treturn e\n\tlocale = req.best_match_language()\n\tif isinstance(e, webob.exc.HTTPError):\n\t\te.explanation = i18n.translate(e.explanation, locale)\n\t\te.detail = i18n.translate(e.detail, locale)\n\t\tif getattr(e, 'body_template', None):\n\t\t\te.body_template = i18n.translate(e.body_template, locale)\n\treturn e\n",["if passed an exc_info it will automatically rewrite the exceptions all the way down to the correct line numbers and frames ."]]
["def template():\n\tdef prep(r):\n\t\tif r.component:\n\t\t\tif (r.component_name == 'translate'):\n\t\t\t\ttable = s3db.survey_translate\n\t\t\t\tif (r.component_id == None):\n\t\t\t\t\ttable.file.readable = False\n\t\t\t\t\ttable.file.writable = False\n\t\t\t\telse:\n\t\t\t\t\ttable.language.writable = False\n\t\t\t\t\ttable.code.writable = False\n\t\t\t\ts3db.configure('survey_translate', deletable=False)\n\t\telse:\n\t\t\ttable = r.table\n\t\t\ts3_action_buttons(r)\n\t\t\trows = db((table.status == 1)).select(table.id)\n\t\t\ttry:\n\t\t\t\ts3.actions[1]['restrict'].extend((str(row.id) for row in rows))\n\t\t\texcept KeyError:\n\t\t\t\ts3.actions[1]['restrict'] = [str(row.id) for row in rows]\n\t\t\texcept IndexError:\n\t\t\t\tpass\n\t\t\ts3.dataTableStyleAlert = [str(row.id) for row in rows]\n\t\t\trows = db((table.status == 3)).select(table.id)\n\t\t\ts3.dataTableStyleDisabled = [str(row.id) for row in rows]\n\t\t\ts3.dataTableStyleWarning = [str(row.id) for row in rows]\n\t\t\trows = db((table.status == 4)).select(table.id)\n\t\t\ts3.dataTableStyleWarning.extend((str(row.id) for row in rows))\n\t\t\ts3db.configure('survey_template', orderby='survey_template.status', create_next=URL(c='survey', f='template'), update_next=URL(c='survey', f='template'))\n\t\treturn True\n\ts3.prep = prep\n\tdef postp(r, output):\n\t\tif r.component:\n\t\t\ttemplate_id = r.id\n\t\t\tif (r.component_name == 'translate'):\n\t\t\t\ts3_action_buttons(r)\n\t\t\t\ts3.actions.extend([dict(label=str(T('Download')), _class='action-btn', url=r.url(method='translate_download', component='translate', component_id='[id]', representation='xls')), dict(label=str(T('Upload')), _class='action-btn', url=URL(c=module, f='template', args=[template_id, 'translate', '[id]']))])\n\t\treturn output\n\ts3.postp = postp\n\tif request.ajax:\n\t\tpost = request.post_vars\n\t\taction = post.get('action')\n\t\ttemplate_id = post.get('parent_id')\n\t\tsection_id = post.get('section_id')\n\t\tsection_text = post.get('section_text')\n\t\tif ((action == 'section') and (template_id != None)):\n\t\t\tid = db.survey_section.insert(name=section_text, template_id=template_id, cloned_section_id=section_id)\n\t\t\tif (id is None):\n\t\t\t\tprint 'Failed\tto\tinsert\trecord'\n\t\t\treturn\n\ts3db.configure('survey_template', listadd=False)\n\toutput = s3_rest_controller(rheader=s3db.survey_template_rheader)\n\treturn output\n",["get a rendered template as a string iterator ."]]
["def get_versions():\n\tcommands = ['gcc\t-dumpversion', 'ld\t-v', 'dllwrap\t--version']\n\treturn tuple([_find_exe_version(cmd) for cmd in commands])\n",["get version information or return default if unable to do so ."]]
["def insertTwistPortions(derivation, elementNode):\n\tinterpolationDictionary = derivation.interpolationDictionary\n\tinterpolationTwist = Interpolation().getByPrefixX(elementNode, derivation.twistPathDefault, 'twist')\n\tinterpolationDictionary['twist'] = interpolationTwist\n\tfor point in interpolationTwist.path:\n\t\tpoint.y = math.radians(point.y)\n\tremainderPortionDirections = interpolationTwist.portionDirections[1:]\n\tinterpolationTwist.portionDirections = [interpolationTwist.portionDirections[0]]\n\tif (elementNode != None):\n\t\ttwistPrecision = setting.getTwistPrecisionRadians(elementNode)\n\tfor remainderPortionDirection in remainderPortionDirections:\n\t\taddTwistPortions(interpolationTwist, remainderPortionDirection, twistPrecision)\n\t\tinterpolationTwist.portionDirections.append(remainderPortionDirection)\n",["insert twist portions and radian the twist ."]]
["@treeio_login_required\n@handle_response_format\ndef transaction_delete(request, transaction_id, response_format='html'):\n\ttransaction = get_object_or_404(Transaction, pk=transaction_id)\n\tif (not request.user.profile.has_permission(transaction, mode='w')):\n\t\treturn user_denied(request, \"You\tdon't\thave\taccess\tto\tthis\tTransaction\", response_format)\n\tif request.POST:\n\t\tif ('delete' in request.POST):\n\t\t\tif ('trash' in request.POST):\n\t\t\t\ttransaction.trash = True\n\t\t\t\ttransaction.save()\n\t\t\telse:\n\t\t\t\ttransaction.delete()\n\t\t\treturn HttpResponseRedirect(reverse('finance_index_transactions'))\n\t\telif ('cancel' in request.POST):\n\t\t\treturn HttpResponseRedirect(reverse('finance_transaction_view', args=[transaction.id]))\n\treturn render_to_response('finance\/transaction_delete', {'transaction': transaction}, context_instance=RequestContext(request), response_format=response_format)\n",["transaction delete ."]]
["def phone2numeric(phone):\n\tletters = re.compile('[A-PR-Y]', re.I)\n\tchar2number = (lambda m: {'a': '2', 'c': '2', 'b': '2', 'e': '3', 'd': '3', 'g': '4', 'f': '3', 'i': '4', 'h': '4', 'k': '5', 'j': '5', 'm': '6', 'l': '5', 'o': '6', 'n': '6', 'p': '7', 's': '7', 'r': '7', 'u': '8', 't': '8', 'w': '9', 'v': '8', 'y': '9', 'x': '9'}.get(m.group(0).lower()))\n\treturn letters.sub(char2number, phone)\n",["converts a phone number with letters into its numeric equivalent ."]]
["def getCarvableObject(elementNode, globalObject, object):\n\tobject.xmlObject = globalObject()\n\tobject.xmlObject.elementNode = object\n\tobject.attributes['id'] = elementNode.getFirstChildByLocalName('name').getTextContent()\n\tcoords = elementNode.getFirstChildByLocalName('coords')\n\ttransformElementNode = getTransformElementNode(coords, 'transformFrom')\n\tif (len(transformElementNode.attributes) < 16):\n\t\ttransformElementNode = getTransformElementNode(coords, 'transformTo')\n\tmatrix.setElementNodeDictionaryMatrix(object, object.xmlObject.matrix4X4.getFromElementNode(transformElementNode, ''))\n\treturn object.xmlObject\n",["get new carvable object info ."]]
["def formatwarning(message, category, filename, lineno, line=None):\n\ts = ('%s:%s:\t%s:\t%s\\n' % (filename, lineno, category.__name__, message))\n\tline = (linecache.getline(filename, lineno) if (line is None) else line)\n\tif line:\n\t\tline = line.strip()\n\t\ts += ('\t\t%s\\n' % line)\n\treturn s\n",["function to format a warning the standard way ."]]
["def _parse_args():\n\tparser = optparse.OptionParser()\n\tparser.add_option('--user', dest='user_install', action='store_true', default=False, help='install\tin\tuser\tsite\tpackage\t(requires\tPython\t2.6\tor\tlater)')\n\tparser.add_option('--download-base', dest='download_base', metavar='URL', default=DEFAULT_URL, help='alternative\tURL\tfrom\twhere\tto\tdownload\tthe\tsetuptools\tpackage')\n\tparser.add_option('--insecure', dest='downloader_factory', action='store_const', const=(lambda : download_file_insecure), default=get_best_downloader, help='Use\tinternal,\tnon-validating\tdownloader')\n\tparser.add_option('--version', help='Specify\twhich\tversion\tto\tdownload', default=DEFAULT_VERSION)\n\t(options, args) = parser.parse_args()\n\treturn options\n",["parse the command line for options ."]]
["def test_read_no_header_autocolumn_NoHeader():\n\ttable = '\\n|\t\tJohn\t\t|\t555-1234\t|192.168.1.10|\\n|\t\tMary\t\t|\t555-2134\t|192.168.1.12|\\n|\t\t\tBob\t\t|\t555-4527\t|\t192.168.1.9|\\n'\n\tdat = ascii.read(table, Reader=ascii.FixedWidthNoHeader)\n\tassert_equal(tuple(dat.dtype.names), ('col1', 'col2', 'col3'))\n\tassert_equal(dat[1][0], 'Mary')\n\tassert_equal(dat[0][1], '555-1234')\n\tassert_equal(dat[2][2], '192.168.1.9')\n",["table with no header row and auto-column naming ."]]
["def originalTextFor(expr, asString=True):\n\tlocMarker = Empty().setParseAction((lambda s, loc, t: loc))\n\tendlocMarker = locMarker.copy()\n\tendlocMarker.callPreparse = False\n\tmatchExpr = ((locMarker('_original_start') + expr) + endlocMarker('_original_end'))\n\tif asString:\n\t\textractText = (lambda s, l, t: s[t._original_start:t._original_end])\n\telse:\n\t\tdef extractText(s, l, t):\n\t\t\tdel t[:]\n\t\t\tt.insert(0, s[t._original_start:t._original_end])\n\t\t\tdel t['_original_start']\n\t\t\tdel t['_original_end']\n\tmatchExpr.setParseAction(extractText)\n\treturn matchExpr\n",["helper to return the original ."]]
["def GetFeedItems(client, feed):\n\tfeed_item_service = client.GetService('FeedItemService', 'v201607')\n\tfeed_items = []\n\tmore_pages = True\n\tselector = {'fields': ['FeedItemId', 'AttributeValues', 'Scheduling'], 'predicates': [{'field': 'Status', 'operator': 'EQUALS', 'values': ['ENABLED']}, {'field': 'FeedId', 'operator': 'EQUALS', 'values': [feed['id']]}], 'paging': {'startIndex': 0, 'numberResults': PAGE_SIZE}}\n\twhile more_pages:\n\t\tpage = feed_item_service.get(selector)\n\t\tif ('entries' in page):\n\t\t\tfeed_items.extend(page['entries'])\n\t\tselector['paging']['startIndex'] += PAGE_SIZE\n\t\tmore_pages = (selector['paging']['startIndex'] < int(page['totalNumEntries']))\n\treturn feed_items\n",["returns the feed items for a given feed ."]]
["def merge(file, names, config, coord):\n\tinputs = get_tiles(names, config, coord)\n\toutput = {'type': 'Topology', 'transform': inputs[0]['transform'], 'objects': dict(), 'arcs': list()}\n\tfor (name, input) in zip(names, inputs):\n\t\tfor (index, object) in enumerate(input['objects'].values()):\n\t\t\tif (len(input['objects']) > 1):\n\t\t\t\toutput['objects'][('%(name)s-%(index)d' % locals())] = object\n\t\t\telse:\n\t\t\t\toutput['objects'][name] = object\n\t\t\tfor geometry in object['geometries']:\n\t\t\t\tupdate_arc_indexes(geometry, output['arcs'], input['arcs'])\n\tfile.write(json.dumps(output, separators=(',', ':')).encode('utf8'))\n",["merge the data from dict2 into the dict1 dictionary ."]]
["def logout(request, next_page=None, template_name='registration\/logged_out.html', redirect_field_name=REDIRECT_FIELD_NAME, current_app=None, extra_context=None):\n\tauth_logout(request)\n\tif (next_page is not None):\n\t\tnext_page = resolve_url(next_page)\n\tif (redirect_field_name in request.REQUEST):\n\t\tnext_page = request.REQUEST[redirect_field_name]\n\t\tif (not is_safe_url(url=next_page, host=request.get_host())):\n\t\t\tnext_page = request.path\n\tif next_page:\n\t\treturn HttpResponseRedirect(next_page)\n\tcurrent_site = get_current_site(request)\n\tcontext = {'site': current_site, 'site_name': current_site.name, 'title': _('Logged\tout')}\n\tif (extra_context is not None):\n\t\tcontext.update(extra_context)\n\treturn TemplateResponse(request, template_name, context, current_app=current_app)\n",["logs out the user and displays you are logged out message ."]]
["def _formatparam(param, value=None, quote=True):\n\tif ((value is not None) and (len(value) > 0)):\n\t\tif isinstance(value, tuple):\n\t\t\tparam += '*'\n\t\t\tvalue = utils.encode_rfc2231(value[2], value[0], value[1])\n\t\tif (quote or tspecials.search(value)):\n\t\t\treturn ('%s=\"%s\"' % (param, utils.quote(value)))\n\t\telse:\n\t\t\treturn ('%s=%s' % (param, value))\n\telse:\n\t\treturn param\n",["convenience function to format and return a key=value pair ."]]
["def base_encode(v, base):\n\tif (base == 58):\n\t\tchars = __b58chars\n\telif (base == 43):\n\t\tchars = __b43chars\n\tlong_value = 0L\n\tfor (i, c) in enumerate(v[::(-1)]):\n\t\tlong_value += ((256 ** i) * ord(c))\n\tresult = ''\n\twhile (long_value >= base):\n\t\t(div, mod) = divmod(long_value, base)\n\t\tresult = (chars[mod] + result)\n\t\tlong_value = div\n\tresult = (chars[long_value] + result)\n\tnPad = 0\n\tfor c in v:\n\t\tif (c == '\\x00'):\n\t\t\tnPad += 1\n\t\telse:\n\t\t\tbreak\n\treturn ((chars[0] * nPad) + result)\n",["encode v ."]]
["def get_formats():\n\tFORMAT_SETTINGS = ('DATE_FORMAT', 'DATETIME_FORMAT', 'TIME_FORMAT', 'YEAR_MONTH_FORMAT', 'MONTH_DAY_FORMAT', 'SHORT_DATE_FORMAT', 'SHORT_DATETIME_FORMAT', 'FIRST_DAY_OF_WEEK', 'DECIMAL_SEPARATOR', 'THOUSAND_SEPARATOR', 'NUMBER_GROUPING', 'DATE_INPUT_FORMATS', 'TIME_INPUT_FORMATS', 'DATETIME_INPUT_FORMATS')\n\tresult = {}\n\tfor module in ([settings] + get_format_modules(reverse=True)):\n\t\tfor attr in FORMAT_SETTINGS:\n\t\t\tresult[attr] = get_format(attr)\n\tsrc = []\n\tfor (k, v) in result.items():\n\t\tif isinstance(v, (basestring, int)):\n\t\t\tsrc.append((\"formats['%s']\t=\t'%s';\\n\" % (javascript_quote(k), javascript_quote(smart_unicode(v)))))\n\t\telif isinstance(v, (tuple, list)):\n\t\t\tv = [javascript_quote(smart_unicode(value)) for value in v]\n\t\t\tsrc.append((\"formats['%s']\t=\t['%s'];\\n\" % (javascript_quote(k), \"',\t'\".join(v))))\n\treturn ''.join(src)\n",["returns all formats strings required for i18n to work ."]]
["def pil_resize(maxwidth, path_in, path_out=None):\n\tpath_out = (path_out or temp_file_for(path_in))\n\tfrom PIL import Image\n\tlog.debug(u'artresizer:\tPIL\tresizing\t{0}\tto\t{1}', util.displayable_path(path_in), util.displayable_path(path_out))\n\ttry:\n\t\tim = Image.open(util.syspath(path_in))\n\t\tsize = (maxwidth, maxwidth)\n\t\tim.thumbnail(size, Image.ANTIALIAS)\n\t\tim.save(path_out)\n\t\treturn path_out\n\texcept IOError:\n\t\tlog.error(u\"PIL\tcannot\tcreate\tthumbnail\tfor\t'{0}'\", util.displayable_path(path_in))\n\t\treturn path_in\n",["resize using python imaging library ."]]
["def add_qos(tenant_id, qos_name, qos_desc):\n\tLOG.debug(_('add_qos()\tcalled'))\n\tsession = db.get_session()\n\ttry:\n\t\tqos = session.query(network_models_v2.QoS).filter_by(tenant_id=tenant_id).filter_by(qos_name=qos_name).one()\n\t\traise c_exc.QosNameAlreadyExists(qos_name=qos_name, tenant_id=tenant_id)\n\texcept exc.NoResultFound:\n\t\tqos = network_models_v2.QoS(tenant_id, qos_name, qos_desc)\n\t\tsession.add(qos)\n\t\tsession.flush()\n\t\treturn qos\n",["adds a qos to tenant association ."]]
["def get_date_formats():\n\twarnings.warn(\"'django.utils.translation.get_date_formats'\tis\tdeprecated.\tPlease\tupdate\tyour\tcode\tto\tuse\tthe\tnew\ti18n\taware\tformatting.\", PendingDeprecationWarning)\n\tfrom google.appengine._internal.django.conf import settings\n\tdate_format = ugettext('DATE_FORMAT')\n\tdatetime_format = ugettext('DATETIME_FORMAT')\n\ttime_format = ugettext('TIME_FORMAT')\n\tif (date_format == 'DATE_FORMAT'):\n\t\tdate_format = settings.DATE_FORMAT\n\tif (datetime_format == 'DATETIME_FORMAT'):\n\t\tdatetime_format = settings.DATETIME_FORMAT\n\tif (time_format == 'TIME_FORMAT'):\n\t\ttime_format = settings.TIME_FORMAT\n\treturn (date_format, datetime_format, time_format)\n",["checks whether translation files provide a translation for some technical message id to store date and time formats ."]]
["def _fastq_illumina_convert_fastq_solexa(in_handle, out_handle, alphabet=None):\n\tfrom Bio.SeqIO.QualityIO import solexa_quality_from_phred\n\tmapping = ''.join((([chr(0) for ascii in range(0, 64)] + [chr((64 + int(round(solexa_quality_from_phred(q))))) for q in range(0, (62 + 1))]) + [chr(0) for ascii in range(127, 256)]))\n\tassert (len(mapping) == 256)\n\treturn _fastq_generic(in_handle, out_handle, mapping)\n",["fast illumina 1 ."]]
["def _type_pprint(obj, p, cycle):\n\tif ([m for m in _get_mro(type(obj)) if ('__repr__' in vars(m))][:1] != [type]):\n\t\t_repr_pprint(obj, p, cycle)\n\t\treturn\n\tmod = _safe_getattr(obj, '__module__', None)\n\ttry:\n\t\tname = obj.__qualname__\n\t\tif (not isinstance(name, str)):\n\t\t\traise Exception('Try\t__name__')\n\texcept Exception:\n\t\tname = obj.__name__\n\t\tif (not isinstance(name, str)):\n\t\t\tname = '<unknown\ttype>'\n\tif (mod in (None, '__builtin__', 'builtins', 'exceptions')):\n\t\tp.text(name)\n\telse:\n\t\tp.text(((mod + '.') + name))\n",["the pprint for classes and types ."]]
["@register.tag\ndef lorem(parser, token):\n\tbits = list(token.split_contents())\n\ttagname = bits[0]\n\tcommon = (bits[(-1)] != 'random')\n\tif (not common):\n\t\tbits.pop()\n\tif (bits[(-1)] in ('w', 'p', 'b')):\n\t\tmethod = bits.pop()\n\telse:\n\t\tmethod = 'b'\n\tif (len(bits) > 1):\n\t\tcount = bits.pop()\n\telse:\n\t\tcount = '1'\n\tcount = parser.compile_filter(count)\n\tif (len(bits) != 1):\n\t\traise template.TemplateSyntaxError(('Incorrect\tformat\tfor\t%r\ttag' % tagname))\n\treturn LoremNode(count, method, common)\n",["creates random latin text useful for providing test data in templates ."]]
["def get_yaml_path(builtin_name, runtime=''):\n\tif (_handler_dir is None):\n\t\tset_builtins_dir(DEFAULT_DIR)\n\tavailable_builtins = set(_available_builtins)\n\tif (builtin_name not in available_builtins):\n\t\traise InvalidBuiltinName(('%s\tis\tnot\tthe\tname\tof\ta\tvalid\tbuiltin.\\nAvailable\thandlers\tare:\t%s' % (builtin_name, ',\t'.join(sorted(available_builtins)))))\n\treturn _get_yaml_path(builtin_name, runtime)\n",["returns the full path to a yaml file by giving the builtin modules name ."]]
["def parse_args(arguments, apply_config=False):\n\tparser = create_parser()\n\targs = parser.parse_args(arguments)\n\tif ((not args.files) and (not args.list_fixes)):\n\t\tparser.error(u'incorrect\tnumber\tof\targuments')\n\targs.files = [decode_filename(name) for name in args.files]\n\tif apply_config:\n\t\tparser = read_config(args, parser)\n\t\targs = parser.parse_args(arguments)\n\t\targs.files = [decode_filename(name) for name in args.files]\n\tif (u'-' in args.files):\n\t\tif (len(args.files) > 1):\n\t\t\tparser.error(u'cannot\tmix\tstdin\tand\tregular\tfiles')\n\t\tif args.diff:\n\t\t\tparser.error(u'--diff\tcannot\tbe\tused\twith\tstandard\tinput')\n\t\tif args.in_place:\n\t\t\tparser.error(u'--in-place\tcannot\tbe\tused\twith\tstandard\tinput')\n\t\tif args.recursive:\n\t\t\tparser.error(u'--recursive\tcannot\tbe\tused\twith\tstandard\tinput')\n\tif ((len(args.files) > 1) and (not (args.in_place or args.diff))):\n\t\tparser.error(u'autopep8\tonly\ttakes\tone\tfilename\tas\targument\tunless\tthe\t\"--in-place\"\tor\t\"--diff\"\targs\tare\tused')\n\tif (args.recursive and (not (args.in_place or args.diff))):\n\t\tparser.error(u'--recursive\tmust\tbe\tused\twith\t--in-place\tor\t--diff')\n\tif (args.in_place and args.diff):\n\t\tparser.error(u'--in-place\tand\t--diff\tare\tmutually\texclusive')\n\tif (args.max_line_length <= 0):\n\t\tparser.error(u'--max-line-length\tmust\tbe\tgreater\tthan\t0')\n\tif args.select:\n\t\targs.select = _split_comma_separated(args.select)\n\tif args.ignore:\n\t\targs.ignore = _split_comma_separated(args.ignore)\n\telif (not args.select):\n\t\tif args.aggressive:\n\t\t\targs.select = [u'E', u'W']\n\t\telse:\n\t\t\targs.ignore = _split_comma_separated(DEFAULT_IGNORE)\n\tif args.exclude:\n\t\targs.exclude = _split_comma_separated(args.exclude)\n\telse:\n\t\targs.exclude = []\n\tif (args.jobs < 1):\n\t\timport multiprocessing\n\t\targs.jobs = multiprocessing.cpu_count()\n\tif ((args.jobs > 1) and (not args.in_place)):\n\t\tparser.error(u'parallel\tjobs\trequires\t--in-place')\n\tif args.line_range:\n\t\tif (args.line_range[0] <= 0):\n\t\t\tparser.error(u'--range\tmust\tbe\tpositive\tnumbers')\n\t\tif (args.line_range[0] > args.line_range[1]):\n\t\t\tparser.error(u'First\tvalue\tof\t--range\tshould\tbe\tless\tthan\tor\tequal\tto\tthe\tsecond')\n\treturn args\n",["parse input arguments ."]]
["def framework_find(fn, executable_path=None, env=None):\n\ttry:\n\t\treturn dyld_find(fn, executable_path=executable_path, env=env)\n\texcept ValueError:\n\t\tpass\n\tfmwk_index = fn.rfind('.framework')\n\tif (fmwk_index == (-1)):\n\t\tfmwk_index = len(fn)\n\t\tfn += '.framework'\n\tfn = os.path.join(fn, os.path.basename(fn[:fmwk_index]))\n\treturn dyld_find(fn, executable_path=executable_path, env=env)\n",["find a framework using dyld semantics in a very loose manner ."]]
["def get_server_certificate(addr, ssl_version=PROTOCOL_SSLv23, ca_certs=None):\n\t(host, port) = addr\n\tif (ca_certs is not None):\n\t\tcert_reqs = CERT_REQUIRED\n\telse:\n\t\tcert_reqs = CERT_NONE\n\tcontext = _create_stdlib_context(ssl_version, cert_reqs=cert_reqs, cafile=ca_certs)\n\twith closing(create_connection(addr)) as sock:\n\t\twith closing(context.wrap_socket(sock)) as sslsock:\n\t\t\tdercert = sslsock.getpeercert(True)\n\treturn DER_cert_to_PEM_cert(dercert)\n",["retrieve the certificate from the server at the specified address ."]]
["def getChainTextFromProcedures(fileName, procedures, text):\n\tlastProcedureTime = time.time()\n\tfor procedure in procedures:\n\t\tcraftModule = getCraftModule(procedure)\n\t\tif (craftModule != None):\n\t\t\ttext = craftModule.getCraftedText(fileName, text)\n\t\t\tif gcodec.isProcedureDone(text, procedure):\n\t\t\t\tprint ('%s\tprocedure\ttook\t%s.' % (procedure.capitalize(), euclidean.getDurationString((time.time() - lastProcedureTime))))\n\t\t\t\tlastProcedureTime = time.time()\n\treturn text\n",["get a crafted shape file from a list of procedures ."]]
["def encode_labels(labels, nclass=5):\n\tY = np.zeros((len(labels), nclass)).astype('float32')\n\tfor (j, y) in enumerate(labels):\n\t\tfor i in range(nclass):\n\t\t\tif ((i + 1) == (np.floor(y) + 1)):\n\t\t\t\tY[(j, i)] = (y - np.floor(y))\n\t\t\tif ((i + 1) == np.floor(y)):\n\t\t\t\tY[(j, i)] = ((np.floor(y) - y) + 1)\n\treturn Y\n",["label encoding from tree lstm paper ."]]
["def create(context, namespace_name, values, session):\n\tnamespace = namespace_api.get(context, namespace_name, session)\n\tresource_type_name = values['name']\n\tmetadef_utils.drop_protected_attrs(models.MetadefNamespaceResourceType, values)\n\ttry:\n\t\tresource_type = resource_type_api.get(context, resource_type_name, session)\n\texcept exc.NotFound:\n\t\tresource_type = None\n\t\tLOG.debug('Creating\tresource-type\t%s', resource_type_name)\n\tif (resource_type is None):\n\t\tresource_type_dict = {'name': resource_type_name, 'protected': False}\n\t\tresource_type = resource_type_api.create(context, resource_type_dict, session)\n\tns_resource_type_dict = _to_db_dict(namespace['id'], resource_type['id'], values)\n\tnew_rec = _create_association(context, namespace_name, resource_type_name, ns_resource_type_dict, session)\n\treturn _to_model_dict(resource_type_name, new_rec)\n",["create a resource_type ."]]
["def _api_switch(name, output, kwargs):\n\tvalue = kwargs.get('value')\n\tvalue2 = kwargs.get('value2')\n\tif (value and value2):\n\t\t(pos, prio) = NzbQueue.do.switch(value, value2)\n\t\tif (output not in ('xml', 'json')):\n\t\t\treturn report(output, data=(pos, prio))\n\t\telse:\n\t\t\treturn report(output, keyword='result', data={'position': pos, 'priority': prio})\n\telse:\n\t\treturn report(output, _MSG_NO_VALUE2)\n",["api: accepts output ."]]
["@register.tag('get_language_info_list')\ndef do_get_language_info_list(parser, token):\n\targs = token.split_contents()\n\tif ((len(args) != 5) or (args[1] != 'for') or (args[3] != 'as')):\n\t\traise TemplateSyntaxError((\"'%s'\trequires\t'for\tsequence\tas\tvariable'\t(got\t%r)\" % (args[0], args[1:])))\n\treturn GetLanguageInfoListNode(parser.compile_filter(args[2]), args[4])\n",["this will store a list of language information dictionaries for the given language codes in a context variable ."]]
["def identify_names(code):\n\tfinder = NameFinder()\n\ttry:\n\t\tfinder.visit(ast.parse(code))\n\texcept SyntaxError:\n\t\treturn {}\n\texample_code_obj = {}\n\tfor (name, full_name) in finder.get_mapping():\n\t\tsplitted = full_name.rsplit('.', 1)\n\t\tif (len(splitted) == 1):\n\t\t\tcontinue\n\t\t(module, attribute) = splitted\n\t\tmodule_short = get_short_module_name(module, attribute)\n\t\tcobj = {'name': attribute, 'module': module, 'module_short': module_short}\n\t\texample_code_obj[name] = cobj\n\treturn example_code_obj\n",["builds a codeobj summary by identifying and resolving used names ."]]
["def put_request_payment(Bucket, Payer, region=None, key=None, keyid=None, profile=None):\n\ttry:\n\t\tconn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\t\tconn.put_bucket_request_payment(Bucket=Bucket, RequestPaymentConfiguration={'Payer': Payer})\n\t\treturn {'updated': True, 'name': Bucket}\n\texcept ClientError as e:\n\t\treturn {'updated': False, 'error': __utils__['boto3.get_error'](e)}\n",["given a valid config ."]]
["def set_vif_host_backend_802qbg_config(conf, devname, managerid, typeid, typeidversion, instanceid, tapname=None):\n\tconf.net_type = 'direct'\n\tconf.source_dev = devname\n\tconf.source_mode = 'vepa'\n\tconf.vporttype = '802.1Qbg'\n\tconf.add_vport_param('managerid', managerid)\n\tconf.add_vport_param('typeid', typeid)\n\tconf.add_vport_param('typeidversion', typeidversion)\n\tconf.add_vport_param('instanceid', instanceid)\n\tif tapname:\n\t\tconf.target_dev = tapname\n",["populate a libvirtconfigguestinterface instance with host backend details for an 802 ."]]
["def _should_use_proxy(url, no_proxy=None):\n\tif (no_proxy is None):\n\t\tno_proxy_effective = os.environ.get('no_proxy', '')\n\telse:\n\t\tno_proxy_effective = no_proxy\n\turlObj = urlparse_.urlparse(_url_as_string(url))\n\tfor np in [h.strip() for h in no_proxy_effective.split(',')]:\n\t\tif (urlObj.hostname == np):\n\t\t\treturn False\n\treturn True\n",["determines whether a proxy should be used to open a connection to the specified url ."]]
["def coordinate_from_string(coord_string):\n\tmatch = COORD_RE.match(coord_string.upper())\n\tif (not match):\n\t\tmsg = ('Invalid\tcell\tcoordinates\t(%s)' % coord_string)\n\t\traise CellCoordinatesException(msg)\n\t(column, row) = match.groups()\n\treturn (column, int(row))\n",["convert a coordinate string like b12 to a tuple ."]]
["@get('\/admin\/<taskid>\/flush')\ndef task_flush(taskid):\n\tfor key in list(DataStore.tasks):\n\t\tif (is_admin(taskid) or (DataStore.tasks[key].remote_addr == request.remote_addr)):\n\t\t\tDataStore.tasks[key].engine_kill()\n\t\t\tdel DataStore.tasks[key]\n\tlogger.debug(('[%s]\tFlushed\ttask\tpool\t(%s)' % (taskid, ('admin' if is_admin(taskid) else request.remote_addr))))\n\treturn jsonize({'success': True})\n",["flush task spool ."]]
["def username_password_authn(environ, start_response, reference, key, redirect_uri, headers=None):\n\tlogger.info('The\tlogin\tpage')\n\tkwargs = dict(mako_template='login.mako', template_lookup=LOOKUP)\n\tif headers:\n\t\tkwargs['headers'] = headers\n\tresp = Response(**kwargs)\n\targv = {'action': '\/verify', 'login': '', 'password': '', 'key': key, 'authn_reference': reference, 'redirect_uri': redirect_uri}\n\tlogger.info(('do_authentication\targv:\t%s' % argv))\n\treturn resp(environ, start_response, **argv)\n",["display the login form ."]]
["@require_context\ndef ec2_snapshot_create(context, snapshot_uuid, id=None):\n\tec2_snapshot_ref = models.SnapshotIdMapping()\n\tec2_snapshot_ref.update({'uuid': snapshot_uuid})\n\tif (id is not None):\n\t\tec2_snapshot_ref.update({'id': id})\n\tec2_snapshot_ref.save()\n\treturn ec2_snapshot_ref\n",["create ec2 compatible snapshot by provided uuid ."]]
["def send_html_mail_jinja(subject, html_template, text_template, context, *args, **kwargs):\n\twith no_jinja_autoescape():\n\t\thtml_template = get_env().get_template(html_template)\n\t\ttext_template = get_env().get_template(text_template)\n\tmsg = send_mail(subject, text_template.render(context), html_message=html_template.render(context), *args, **kwargs)\n\treturn msg\n",["sends html mail using a jinja template with autoescaping turned off ."]]
["def groups_for_user(environ, username):\n\tUserModel = auth.get_user_model()\n\tdb.reset_queries()\n\ttry:\n\t\ttry:\n\t\t\tuser = UserModel._default_manager.get_by_natural_key(username)\n\t\texcept UserModel.DoesNotExist:\n\t\t\treturn []\n\t\tif (not user.is_active):\n\t\t\treturn []\n\t\treturn [force_bytes(group.name) for group in user.groups.all()]\n\tfinally:\n\t\tdb.close_old_connections()\n",["authorizes a user based on groups ."]]
["@preserve_value(sys, 'dont_write_bytecode')\ndef _load_module_no_bytecode(filename, module_file, module_file_path, py_source_description):\n\tsys.dont_write_bytecode = 1\n\tnew_module = imp.load_module(os.path.splitext(filename)[0].replace('-', '_'), module_file, module_file_path, py_source_description)\n\treturn new_module\n",["helper function to load a module while setting sys ."]]
["def IndexYamlForQuery(kind, ancestor, props):\n\tserialized_yaml = []\n\tserialized_yaml.append(('-\tkind:\t%s' % kind))\n\tif ancestor:\n\t\tserialized_yaml.append('\t\tancestor:\tyes')\n\tif props:\n\t\tserialized_yaml.append('\t\tproperties:')\n\t\tfor (name, direction) in props:\n\t\t\tserialized_yaml.append(('\t\t-\tname:\t%s' % name))\n\t\t\tif (direction == DESCENDING):\n\t\t\t\tserialized_yaml.append('\t\t\t\tdirection:\tdesc')\n\treturn '\\n'.join(serialized_yaml)\n",["return the composite index definition yaml needed for a query ."]]
["def normpath(path):\n\tif (path == ''):\n\t\treturn '.'\n\tinitial_slashes = path.startswith('\/')\n\tif (initial_slashes and path.startswith('\/\/') and (not path.startswith('\/\/\/'))):\n\t\tinitial_slashes = 2\n\tcomps = path.split('\/')\n\tnew_comps = []\n\tfor comp in comps:\n\t\tif (comp in ('', '.')):\n\t\t\tcontinue\n\t\tif ((comp != '..') or ((not initial_slashes) and (not new_comps)) or (new_comps and (new_comps[(-1)] == '..'))):\n\t\t\tnew_comps.append(comp)\n\t\telif new_comps:\n\t\t\tnew_comps.pop()\n\tcomps = new_comps\n\tpath = '\/'.join(comps)\n\tif initial_slashes:\n\t\tpath = (('\/' * initial_slashes) + path)\n\treturn (path or '.')\n",["replacement for os ."]]
["def time(value, arg=None):\n\tfrom google.appengine._internal.django.utils import dateformat\n\tif (value in (None, u'')):\n\t\treturn u''\n\tif (arg is None):\n\t\targ = settings.TIME_FORMAT\n\ttry:\n\t\treturn formats.time_format(value, arg)\n\texcept AttributeError:\n\t\ttry:\n\t\t\treturn dateformat.time_format(value, arg)\n\t\texcept AttributeError:\n\t\t\treturn ''\n",["formats a time according to the given format ."]]
["def CheckForBadCharacters(filename, lines, error):\n\tfor (linenum, line) in enumerate(lines):\n\t\tif (unicode_escape_decode('\\\\ufffd') in line):\n\t\t\terror(filename, linenum, 'readability\/utf8', 5, 'Line\tcontains\tinvalid\tUTF-8\t(or\tUnicode\treplacement\tcharacter).')\n\t\tif ('\\x00' in line):\n\t\t\terror(filename, linenum, 'readability\/nul', 5, 'Line\tcontains\tNUL\tbyte.')\n",["logs an error for each line containing bad characters ."]]
["def vol_usage_update(context, id, rd_req, rd_bytes, wr_req, wr_bytes, instance_id, last_refreshed=None, update_totals=False):\n\treturn IMPL.vol_usage_update(context, id, rd_req, rd_bytes, wr_req, wr_bytes, instance_id, last_refreshed=last_refreshed, update_totals=update_totals)\n",["update cached volume usage for a volume creates new record if needed ."]]
["def remove_qos(tenant_id, qos_id):\n\tsession = db.get_session()\n\ttry:\n\t\tqos = session.query(l2network_models.QoS).filter_by(tenant_id=tenant_id).filter_by(qos_id=qos_id).one()\n\t\tsession.delete(qos)\n\t\tsession.flush()\n\t\treturn qos\n\texcept exc.NoResultFound:\n\t\tpass\n",["removes a qos to tenant association ."]]
["def get_jids():\n\tret = {}\n\tfor returner_ in __opts__[CONFIG_KEY]:\n\t\tret.update(_mminion().returners['{0}.get_jids'.format(returner_)]())\n\treturn ret\n",["return a list of all job ids ."]]
["@contextfunction\ndef finance_liability_list(context, liabilities, skip_group=False):\n\trequest = context['request']\n\tresponse_format = 'html'\n\tif ('response_format' in context):\n\t\tresponse_format = context['response_format']\n\treturn Markup(render_to_string('finance\/tags\/liability_list', {'liabilities': liabilities, 'skip_group': skip_group}, context_instance=RequestContext(request), response_format=response_format))\n",["print a list of orders ."]]
["def _extract_labels(filename, num_labels):\n\tprint('Extracting\tlabels\tfrom:\t', filename)\n\twith gzip.open(filename) as bytestream:\n\t\tbytestream.read(8)\n\t\tbuf = bytestream.read((1 * num_labels))\n\t\tlabels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n\treturn labels\n",["extract the labels into a vector of int64 label ids ."]]
["def generate_credits(component, start_date, end_date):\n\tresult = []\n\tfor translation in component.translation_set.all():\n\t\tauthors = Change.objects.authors_list(translation, (start_date, end_date))\n\t\tif (not authors):\n\t\t\tcontinue\n\t\tresult.append({translation.language.name: sorted(set(authors))})\n\treturn result\n",["generates credits data for given component ."]]
["def dup_trial_division(f, factors, K):\n\tresult = []\n\tfor factor in factors:\n\t\tk = 0\n\t\twhile True:\n\t\t\t(q, r) = dup_div(f, factor, K)\n\t\t\tif (not r):\n\t\t\t\t(f, k) = (q, (k + 1))\n\t\t\telse:\n\t\t\t\tbreak\n\t\tresult.append((factor, k))\n\treturn _sort_factors(result)\n",["determine multiplicities of factors using trial division ."]]
["def chexor(old, name, timestamp):\n\tif (name is None):\n\t\traise Exception('name\tis\tNone!')\n\tnew = hashlib.md5(('%s-%s' % (name, timestamp)).encode('utf8')).hexdigest()\n\treturn ('%032x' % (int(old, 16) ^ int(new, 16)))\n",["each entry in the account and container databases is xored by the 128-bit hash on insert or delete ."]]
["def translate_text(target, text):\n\ttranslate_client = translate.Client()\n\tresult = translate_client.translate(text, target_language=target)\n\tprint u'Text:\t{}'.format(result['input'])\n\tprint u'Translation:\t{}'.format(result['translatedText'])\n\tprint u'Detected\tsource\tlanguage:\t{}'.format(result['detectedSourceLanguage'])\n",["translates text into the target language ."]]
["def highlight(code, lexer, formatter, outfile=None):\n\treturn format(lex(code, lexer), formatter, outfile)\n",["allows you to put a highlighted source code <pre> block in your code ."]]
["def libvlc_audio_get_track_description(p_mi):\n\tf = (_Cfunctions.get('libvlc_audio_get_track_description', None) or _Cfunction('libvlc_audio_get_track_description', ((1,),), None, ctypes.POINTER(TrackDescription), MediaPlayer))\n\treturn f(p_mi)\n",["get the description of available audio tracks ."]]
["def put(dct, entry):\n\tid = int(entry['id'])\n\tif (id in dct):\n\t\tif (entry == dct[id]):\n\t\t\tpass\n\t\telse:\n\t\t\tprint entry\n\t\t\tprint dct[id]\n\t\t\tassert False\n\telse:\n\t\tdct[id] = entry\n",["do the actual put ."]]
["def get(name, default=_UNSET, scope='global', window=None, tab=None):\n\treg = _get_registry(scope, window, tab)\n\ttry:\n\t\treturn reg[name]\n\texcept KeyError:\n\t\tif (default is not _UNSET):\n\t\t\treturn default\n\t\telse:\n\t\t\traise\n",["attempt to retrieve the named value from grains ."]]
["@conf.commands.register\ndef srp1(*args, **kargs):\n\tif (not kargs.has_key('timeout')):\n\t\tkargs['timeout'] = (-1)\n\t(a, b) = srp(*args, **kargs)\n\tif (len(a) > 0):\n\t\treturn a[0][1]\n\telse:\n\t\treturn None\n",["send and receive packets at layer 2 and return only the first answer nofilter: put 1 to avoid use of bpf filters retry: if positive ."]]
["def read_py_url(url, errors='replace', skip_encoding_cookie=True):\n\tfrom urllib.request import urlopen\n\tresponse = urlopen(url)\n\tbuffer = io.BytesIO(response.read())\n\treturn source_to_unicode(buffer, errors, skip_encoding_cookie)\n",["read a python file from a url ."]]
["def vary_on_headers(*headers):\n\tdef decorator(func):\n\t\tdef inner_func(*args, **kwargs):\n\t\t\tresponse = func(*args, **kwargs)\n\t\t\tpatch_vary_headers(response, headers)\n\t\t\treturn response\n\t\treturn wraps(func, assigned=available_attrs(func))(inner_func)\n\treturn decorator\n",["a view decorator that adds the specified headers to the vary header of the response ."]]
["def add_organization_course(organization_data, course_id):\n\tif (not organizations_enabled()):\n\t\treturn None\n\tfrom organizations import api as organizations_api\n\treturn organizations_api.add_organization_course(organization_data=organization_data, course_key=course_id)\n",["client api operation adapter\/wrapper ."]]
["def GetInvisibleSpecialPropertyNames():\n\tinvisible_names = []\n\tfor (name, value) in _SPECIAL_PROPERTY_MAP.items():\n\t\t(is_visible, _, _) = value\n\t\tif (not is_visible):\n\t\t\tinvisible_names.append(name)\n\treturn invisible_names\n",["gets the names of all non user-visible special properties ."]]
["def get_default_timezone():\n\tglobal _localtime\n\tif (_localtime is None):\n\t\tif (isinstance(settings.TIME_ZONE, six.string_types) and (pytz is not None)):\n\t\t\t_localtime = pytz.timezone(settings.TIME_ZONE)\n\t\telse:\n\t\t\t_localtime = LocalTimezone()\n\treturn _localtime\n",["returns the default time zone as a tzinfo instance ."]]
["def _indent(elem, level=0):\n\ti = ('\\n' + (level * '\t\t'))\n\tif len(elem):\n\t\tif ((not elem.text) or (not elem.text.strip())):\n\t\t\telem.text = (i + '\t\t')\n\t\tfor e in elem:\n\t\t\t_indent(e, (level + 1))\n\t\t\tif ((not e.tail) or (not e.tail.strip())):\n\t\t\t\te.tail = (i + '\t\t')\n\t\tif ((not e.tail) or (not e.tail.strip())):\n\t\t\te.tail = i\n\telif (level and ((not elem.tail) or (not elem.tail.strip()))):\n\t\telem.tail = i\n",["add the given number of space characters to the beginning of every non-blank line in s ."]]
["def test_nm3_sample_wt_fit():\n\tratio = 'auto'\n\tnm3 = NearMiss(ratio=ratio, random_state=RND_SEED, version=VERSION_NEARMISS)\n\tassert_raises(RuntimeError, nm3.sample, X, Y)\n",["test either if an error is raised when sample is called before fitting ."]]
["def custom_check(cmd, ignore_retcode=False):\n\tp = custom_popen(cmd)\n\t(out, _) = p.communicate()\n\tif (p.returncode and (not ignore_retcode)):\n\t\traise RarExecError('Check-run\tfailed')\n\treturn out\n",["run command ."]]
["def check_message(keywords, message):\n\t(exc_type, exc_value, exc_traceback) = sys.exc_info()\n\tif set(str(exc_value).split('\t')).issuperset(set(keywords)):\n\t\texc_value._safe_message = message\n\t\traise\n",["checks an exception for given keywords and raises a new actionerror with the desired message if the keywords are found ."]]
["@register.filter(is_safe=True)\n@stringfilter\ndef slugify(value):\n\treturn _slugify(value)\n",["converts to lowercase ."]]
["def test_iht_init():\n\tratio = 'auto'\n\tiht = InstanceHardnessThreshold(ESTIMATOR, ratio=ratio, random_state=RND_SEED)\n\tassert_equal(iht.ratio, ratio)\n\tassert_equal(iht.random_state, RND_SEED)\n",["test the initialisation of the object ."]]
["def make_dist(name, version, **kwargs):\n\tsummary = kwargs.pop(u'summary', u'Placeholder\tfor\tsummary')\n\tmd = Metadata(**kwargs)\n\tmd.name = name\n\tmd.version = version\n\tmd.summary = (summary or u'Placeholder\tfor\tsummary')\n\treturn Distribution(md)\n",["a convenience method for making a dist given just a name and version ."]]
["def test_nearmiss_wrong_version():\n\tversion = 1000\n\tnm1 = NearMiss(version=version, random_state=RND_SEED)\n\tassert_raises(ValueError, nm1.fit_sample, X, Y)\n",["test either if an error is raised when the version is unknown ."]]
["def get_auth_from_url(url):\n\tparsed = urlparse(url)\n\ttry:\n\t\tauth = (unquote(parsed.username), unquote(parsed.password))\n\texcept (AttributeError, TypeError):\n\t\tauth = ('', '')\n\treturn auth\n",["given a url with authentication components ."]]
["def get_poetry(address):\n\tsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\tsock.connect(address)\n\tpoem = ''\n\twhile True:\n\t\tdata = sock.recv(1024)\n\t\tif (not data):\n\t\t\tsock.close()\n\t\t\tbreak\n\t\tpoem += data\n\treturn poem\n",["download a poem from the given host and port ."]]
["def get_paths(scheme=_get_default_scheme(), vars=None, expand=True):\n\t_ensure_cfg_read()\n\tif expand:\n\t\treturn _expand_vars(scheme, vars)\n\telse:\n\t\treturn dict(_SCHEMES.items(scheme))\n",["return a mapping containing an install scheme ."]]
["def xpath_tokenizer(p):\n\tout = []\n\tfor (op, tag) in ElementPath.xpath_tokenizer(p):\n\t\tout.append((op or tag))\n\treturn out\n",["test the xpath tokenizer ."]]
["def get_load(jid):\n\tquery = 'SELECT\tload\tFROM\tsalt.jids\tWHERE\tjid\t=\t?;'\n\tret = {}\n\ttry:\n\t\tdata = __salt__['cassandra_cql.cql_query_with_prepare'](query, 'get_load', [jid])\n\t\tif data:\n\t\t\tload = data[0].get('load')\n\t\t\tif load:\n\t\t\t\tret = json.loads(load)\n\texcept CommandExecutionError:\n\t\tlog.critical('Could\tnot\tget\tload\tfrom\tjids\ttable.')\n\t\traise\n\texcept Exception as e:\n\t\tlog.critical('Unexpected\terror\twhile\tgetting\tload\tfrom\tjids:\t{0}'.format(str(e)))\n\t\traise\n\treturn ret\n",["return the load data that marks a specified jid ."]]
["def check_password(environ, username, password):\n\tUserModel = auth.get_user_model()\n\tdb.reset_queries()\n\ttry:\n\t\ttry:\n\t\t\tuser = UserModel._default_manager.get_by_natural_key(username)\n\t\texcept UserModel.DoesNotExist:\n\t\t\treturn None\n\t\tif (not user.is_active):\n\t\t\treturn None\n\t\treturn user.check_password(password)\n\tfinally:\n\t\tdb.close_old_connections()\n",["returns a boolean of whether the raw_password was correct ."]]
["def getPluginsDirectoryPath():\n\treturn archive.getAbsoluteFolderPath(os.path.dirname(__file__), os.path.join('skeinforge_plugins', 'meta_plugins'))\n",["get the plugins directory path ."]]
["def file_upload_echo(request):\n\tr = {k: f.name for (k, f) in request.FILES.items()}\n\treturn HttpResponse(json.dumps(r))\n",["simple view to echo back info about uploaded files for tests ."]]
["def human_readable(size, isbits=False, unit=None):\n\ttry:\n\t\treturn basic.bytes_to_human(size, isbits, unit)\n\texcept:\n\t\traise errors.AnsibleFilterError((\"human_readable()\tcan't\tinterpret\tfollowing\tstring:\t%s\" % size))\n",["convert a size in bytes into megabytes ."]]
["def image_tag_delete(context, image_id, value, session=None):\n\t_check_image_id(image_id)\n\tsession = (session or get_session())\n\tquery = session.query(models.ImageTag).filter_by(image_id=image_id).filter_by(value=value).filter_by(deleted=False)\n\ttry:\n\t\ttag_ref = query.one()\n\texcept sa_orm.exc.NoResultFound:\n\t\traise exception.NotFound()\n\ttag_ref.delete(session=session)\n",["delete an image tag ."]]
["def split(line):\n\tif (not line.strip()):\n\t\traise exceptions.MpdNoCommand(u'No\tcommand\tgiven')\n\tmatch = WORD_RE.match(line)\n\tif (not match):\n\t\traise exceptions.MpdUnknownError(u'Invalid\tword\tcharacter')\n\t(whitespace, command, remainder) = match.groups()\n\tif whitespace:\n\t\traise exceptions.MpdUnknownError(u'Letter\texpected')\n\tresult = [command]\n\twhile remainder:\n\t\tmatch = PARAM_RE.match(remainder)\n\t\tif (not match):\n\t\t\tmsg = _determine_error_message(remainder)\n\t\t\traise exceptions.MpdArgError(msg, command=command)\n\t\t(unquoted, quoted, remainder) = match.groups()\n\t\tresult.append((unquoted or UNESCAPE_RE.sub(u'\\\\g<1>', quoted)))\n\treturn result\n",["split *sql* into single statements ."]]
["def get_language_bidi():\n\tfrom django.conf import settings\n\tbase_lang = get_language().split(u'-')[0]\n\treturn (base_lang in settings.LANGUAGES_BIDI)\n",["returns selected languages bidi layout ."]]
["def test_roc_auc_one_vs_one():\n\tskip_if_no_sklearn()\n\ttrainer = yaml_parse.load(test_yaml_ovo)\n\ttrainer.main_loop()\n",["test one vs ."]]
["def render_to_kmz(*args, **kwargs):\n\treturn HttpResponse(compress_kml(loader.render_to_string(*args, **kwargs)), mimetype='application\/vnd.google-earth.kmz')\n",["compresses the kml content and returns as kmz ."]]
["def assert_server_running(server):\n\tif (server.poll() is not None):\n\t\traise RuntimeError('Server\tdied\tunexpectedly!')\n",["get the exit code of the server ."]]
["def validate_argmin_with_skipna(skipna, args, kwargs):\n\t(skipna, args) = process_skipna(skipna, args)\n\tvalidate_argmin(args, kwargs)\n\treturn skipna\n",["if series ."]]
["def skipUnlessDBFeature(feature):\n\treturn _deferredSkip((lambda : (not getattr(connection.features, feature))), (u\"Database\tdoesn't\tsupport\tfeature\t%s\" % feature))\n",["skip a test unless a database has the named feature ."]]
["def certificate_get_all_by_project(context, project_id):\n\treturn IMPL.certificate_get_all_by_project(context, project_id)\n",["get all certificates for a project ."]]
["def format_time(seconds, count=3, accuracy=6, simple=False):\n\tif simple:\n\t\tperiods = [('c', ((((60 * 60) * 24) * 365) * 100)), ('de', ((((60 * 60) * 24) * 365) * 10)), ('y', (((60 * 60) * 24) * 365)), ('mo', (((60 * 60) * 24) * 30)), ('d', ((60 * 60) * 24)), ('h', (60 * 60)), ('m', 60), ('s', 1)]\n\telse:\n\t\tperiods = [(('century', 'centuries'), ((((60 * 60) * 24) * 365) * 100)), (('decade', 'decades'), ((((60 * 60) * 24) * 365) * 10)), (('year', 'years'), (((60 * 60) * 24) * 365)), (('month', 'months'), (((60 * 60) * 24) * 30)), (('day', 'days'), ((60 * 60) * 24)), (('hour', 'hours'), (60 * 60)), (('minute', 'minutes'), 60), (('second', 'seconds'), 1)]\n\tperiods = periods[(- accuracy):]\n\tstrings = []\n\ti = 0\n\tfor (period_name, period_seconds) in periods:\n\t\tif (i < count):\n\t\t\tif (seconds > period_seconds):\n\t\t\t\t(period_value, seconds) = divmod(seconds, period_seconds)\n\t\t\t\ti += 1\n\t\t\t\tif simple:\n\t\t\t\t\tstrings.append('{}{}'.format(period_value, period_name))\n\t\t\t\telif (period_value == 1):\n\t\t\t\t\tstrings.append('{}\t{}'.format(period_value, period_name[0]))\n\t\t\t\telse:\n\t\t\t\t\tstrings.append('{}\t{}'.format(period_value, period_name[1]))\n\t\telse:\n\t\t\tbreak\n\tif simple:\n\t\treturn '\t'.join(strings)\n\telse:\n\t\treturn formatting.get_text_list(strings, 'and')\n",["see :meth:i18n ."]]
["def urlquote_plus(url, safe=''):\n\treturn force_unicode(urllib.quote_plus(smart_str(url), safe))\n",["a version of pythons urllib ."]]
["def _make_entity_from_pb(annotations):\n\treturn [EntityAnnotation.from_pb(annotation) for annotation in annotations]\n",["create an entity from a protobuf response ."]]
["def autoassign(self, locals):\n\tfor (k, v) in locals.iteritems():\n\t\tif (k == 'self'):\n\t\t\tcontinue\n\t\tsetattr(self, k, v)\n",["automatically assigns local variables to self ."]]
["def eggs(x, y):\n\tglobal fr, st\n\tfr = inspect.currentframe()\n\tst = inspect.stack()\n\tp = x\n\tq = (y \/ 0)\n",["a docstring ."]]
["def test_resize_icon_enlarge():\n\tresize_size = [1000]\n\tfinal_size = [(339, 128)]\n\t_uploader(resize_size, final_size)\n",["image stays the same ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only work on posix-like systems ."]]
["def search(opts, returners, whitelist=None):\n\treturn LazyLoader(_module_dirs(opts, 'search', 'search'), opts, tag='search', whitelist=whitelist, pack={'__ret__': returners})\n",["perform a re ."]]
["def test_rmtree_retries(tmpdir, monkeypatch):\n\tmonkeypatch.setattr(shutil, 'rmtree', Failer(duration=1).call)\n\trmtree('foo')\n",["test pip ."]]
["def idd_sfrm(l, n, w, x):\n\treturn _id.idd_sfrm(l, n, w, x)\n",["transform real vector via a composition of rokhlins random transform ."]]
["def isCocoaTk():\n\tassert (_tk_type is not None)\n\treturn (_tk_type == 'cocoa')\n",["returns true if idle is using a cocoa aqua tk ."]]
["def extra_padding_x(original_size, padding):\n\treturn _resize(original_size, 0, padding=padding)\n",["reduce the width of original_size by padding ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load this module if splunk is installed on this minion ."]]
["def convert_PhoneNumberProperty(model, prop, kwargs):\n\treturn get_TextField(kwargs)\n",["returns a form field for a db ."]]
["def instance_get(context, instance_id, columns_to_join=None):\n\treturn IMPL.instance_get(context, instance_id, columns_to_join=columns_to_join)\n",["get an instance or raise if it does not exist ."]]
["def escape(text):\n\ttext = text.replace('\\\\', '\\\\\\\\')\n\ttext = text.replace('\"\"\"', '\"\"\\\\\"')\n\ttext = text.replace('\t\\n', '\t\\\\n\\\\\\n')\n\treturn text\n",["escape a url including any \/ ."]]
["def load_image(path, height, width, mode='RGB'):\n\timage = PIL.Image.open(path)\n\timage = image.convert(mode)\n\timage = np.array(image)\n\timage = scipy.misc.imresize(image, (height, width), 'bilinear')\n\treturn image\n",["load an image ."]]
["def export_book(databook):\n\twb = xlwt.Workbook(encoding='utf8')\n\tfor (i, dset) in enumerate(databook._datasets):\n\t\tws = wb.add_sheet((dset.title if dset.title else ('Sheet%s' % i)))\n\t\tdset_sheet(dset, ws)\n\tstream = BytesIO()\n\twb.save(stream)\n\treturn stream.getvalue()\n",["returns json representation of databook ."]]
["def processXMLElement(xmlElement):\n\tgroup.processShape(Difference, xmlElement)\n",["process the xml element ."]]
["def inspect_response(response, spider):\n\tShell(spider.crawler).start(response=response)\n",["open a shell to inspect the given response ."]]
["def register(func, msg_type=None, dispatcher_name=None, active_by_default=True):\n\tif (msg_type and (msg_type not in MSG_TYPE_MAP)):\n\t\traise InvalidHandlerType(('Invalid\tmessage\ttype\t[%s]:\ttype\tshould\tbe\tin\t%s' % (msg_type, str(MSG_TYPES))))\n\thandler = Handler(func=func, name=dispatcher_name)\n\tif (msg_type is None):\n\t\t_registry[RAW_TYPE].append(handler)\n\telse:\n\t\t_registry[msg_type].append(handler)\n\tif active_by_default:\n\t\t_active.add(dispatcher_name)\n",["required method to auto register this checker ."]]
["@register.filter('escapejs')\n@stringfilter\ndef escapejs_filter(value):\n\treturn escapejs(value)\n",["hex encodes characters for use in javascript strings ."]]
["def brand():\n\treturn s3_rest_controller('supply', 'brand')\n",["restful crud controller ."]]
["@csrf_protect\n@permission_required('comments.can_moderate')\ndef delete(request, comment_id, next=None):\n\tcomment = get_object_or_404(comments.get_model(), pk=comment_id, site__pk=settings.SITE_ID)\n\tif (request.method == 'POST'):\n\t\tperform_delete(request, comment)\n\t\treturn next_redirect(request, next, delete_done, c=comment.pk)\n\telse:\n\t\treturn render_to_response('comments\/delete.html', {'comment': comment, 'next': next}, template.RequestContext(request))\n",["remove tags from a file ."]]
["def register(func, msg_type=None, dispatcher_name=None, active_by_default=True):\n\tif (msg_type and (msg_type not in MSG_TYPE_MAP)):\n\t\traise InvalidHandlerType(('Invalid\tmessage\ttype\t[%s]:\ttype\tshould\tbe\tin\t%s' % (msg_type, str(MSG_TYPES))))\n\thandler = Handler(func=func, name=dispatcher_name)\n\tif (msg_type is None):\n\t\t_registry[RAW_TYPE].append(handler)\n\telse:\n\t\t_registry[msg_type].append(handler)\n\tif active_by_default:\n\t\t_active.add(dispatcher_name)\n",["the mandatory cobbler module registration hook ."]]
["@register.tag('include')\ndef do_include(parser, token):\n\tbits = token.split_contents()\n\tif (len(bits) < 2):\n\t\traise TemplateSyntaxError(('%r\ttag\ttakes\tat\tleast\tone\targument:\tthe\tname\tof\tthe\ttemplate\tto\tbe\tincluded.' % bits[0]))\n\toptions = {}\n\tremaining_bits = bits[2:]\n\twhile remaining_bits:\n\t\toption = remaining_bits.pop(0)\n\t\tif (option in options):\n\t\t\traise TemplateSyntaxError(('The\t%r\toption\twas\tspecified\tmore\tthan\tonce.' % option))\n\t\tif (option == 'with'):\n\t\t\tvalue = token_kwargs(remaining_bits, parser, support_legacy=False)\n\t\t\tif (not value):\n\t\t\t\traise TemplateSyntaxError(('\"with\"\tin\t%r\ttag\tneeds\tat\tleast\tone\tkeyword\targument.' % bits[0]))\n\t\telif (option == 'only'):\n\t\t\tvalue = True\n\t\telse:\n\t\t\traise TemplateSyntaxError(('Unknown\targument\tfor\t%r\ttag:\t%r.' % (bits[0], option)))\n\t\toptions[option] = value\n\tisolated_context = options.get('only', False)\n\tnamemap = options.get('with', {})\n\tpath = bits[1]\n\tif ((path[0] in ('\"', \"'\")) and (path[(-1)] == path[0])):\n\t\treturn ConstantIncludeNode(path[1:(-1)], extra_context=namemap, isolated_context=isolated_context)\n\treturn IncludeNode(parser.compile_filter(bits[1]), extra_context=namemap, isolated_context=isolated_context)\n",["loads a template and renders it with the current context ."]]
["def transferClosestFillLoop(extrusionHalfWidth, oldOrderedLocation, remainingFillLoops, skein):\n\tclosestDistance = 9.876543219876543e+17\n\tclosestFillLoop = None\n\tfor remainingFillLoop in remainingFillLoops:\n\t\tdistance = getClosestDistanceIndexToLine(oldOrderedLocation.dropAxis(), remainingFillLoop).distance\n\t\tif (distance < closestDistance):\n\t\t\tclosestDistance = distance\n\t\t\tclosestFillLoop = remainingFillLoop\n\tnewClosestFillLoop = getLoopInsideContainingLoop(closestFillLoop, remainingFillLoops)\n\twhile (newClosestFillLoop != None):\n\t\tclosestFillLoop = newClosestFillLoop\n\t\tnewClosestFillLoop = getLoopInsideContainingLoop(closestFillLoop, remainingFillLoops)\n\tremainingFillLoops.remove(closestFillLoop)\n\taddToThreadsFromLoop(extrusionHalfWidth, 'loop', closestFillLoop[:], oldOrderedLocation, skein)\n",["transfer the closest remaining fill loop ."]]
["def testing_engine(url=None, options=None):\n\tfrom sqlalchemy import create_engine\n\tfrom sqlalchemy.engine.url import make_url\n\tif (not options):\n\t\tuse_reaper = True\n\telse:\n\t\tuse_reaper = options.pop('use_reaper', True)\n\turl = (url or config.db.url)\n\turl = make_url(url)\n\tif (options is None):\n\t\tif ((config.db is None) or (url.drivername == config.db.url.drivername)):\n\t\t\toptions = config.db_opts\n\t\telse:\n\t\t\toptions = {}\n\tengine = create_engine(url, **options)\n\tengine._has_events = True\n\tif isinstance(engine.pool, pool.QueuePool):\n\t\tengine.pool._timeout = 0\n\t\tengine.pool._max_overflow = 0\n\tif use_reaper:\n\t\tevent.listen(engine.pool, 'connect', testing_reaper.connect)\n\t\tevent.listen(engine.pool, 'checkout', testing_reaper.checkout)\n\t\tevent.listen(engine.pool, 'invalidate', testing_reaper.invalidate)\n\t\ttesting_reaper.add_engine(engine)\n\treturn engine\n",["produce an engine configured by --options with optional overrides ."]]
["def get_real_filter(layers, img_size):\n\treal_filter = np.zeros((len(layers), 2))\n\tconv_mode = True\n\tfirst_conv_layer = True\n\texpon = np.ones((1, 2))\n\tfor (i, layer) in enumerate(layers[1:]):\n\t\tj = (i + 1)\n\t\tif (not conv_mode):\n\t\t\treal_filter[j] = img_size\n\t\t\tcontinue\n\t\tif is_conv2d(layer):\n\t\t\tif (not first_conv_layer):\n\t\t\t\tnew_filter = (np.array(layer.filter_size) * expon)\n\t\t\t\treal_filter[j] = new_filter\n\t\t\telse:\n\t\t\t\tnew_filter = (np.array(layer.filter_size) * expon)\n\t\t\t\treal_filter[j] = new_filter\n\t\t\t\tfirst_conv_layer = False\n\t\telif is_maxpool2d(layer):\n\t\t\treal_filter[j] = real_filter[i]\n\t\t\texpon *= np.array(layer.pool_size)\n\t\telse:\n\t\t\tconv_mode = False\n\t\t\treal_filter[j] = img_size\n\treal_filter[0] = img_size\n\treturn real_filter\n",["get the real filter sizes of each layer involved in convoluation ."]]
["def test_run_json_dump(mocker, mock_ensure_success, mock_user_config, template_name, context, replay_test_dir, replay_file):\n\tspy_get_replay_file = mocker.spy(replay, 'get_file_name')\n\tmock_json_dump = mocker.patch('json.dump', side_effect=json.dump)\n\treplay.dump(replay_test_dir, template_name, context)\n\tassert (not mock_user_config.called)\n\tmock_ensure_success.assert_called_once_with(replay_test_dir)\n\tspy_get_replay_file.assert_called_once_with(replay_test_dir, template_name)\n\tassert (mock_json_dump.call_count == 1)\n\t((dumped_context, outfile_handler), kwargs) = mock_json_dump.call_args\n\tassert (outfile_handler.name == replay_file)\n\tassert (dumped_context == context)\n",["test that replay ."]]
["def user_data_dir(appname, roaming=False):\n\tif WINDOWS:\n\t\tconst = ((roaming and 'CSIDL_APPDATA') or 'CSIDL_LOCAL_APPDATA')\n\t\tpath = os.path.join(os.path.normpath(_get_win_folder(const)), appname)\n\telif (sys.platform == 'darwin'):\n\t\tpath = os.path.join(expanduser('~\/Library\/Application\tSupport\/'), appname)\n\telse:\n\t\tpath = os.path.join(os.getenv('XDG_DATA_HOME', expanduser('~\/.local\/share')), appname)\n\treturn path\n",["return full path to the user-specific data dir for this application ."]]
["def pearson_log_likelihood_ratio(observed=[], expected=[], df=None, tail=UPPER):\n\to = list(observed)\n\te = (list(expected) or _expected(o))\n\tn = len(o)\n\tm = (len(o[0]) if o else 0)\n\tdf = (df or ((n - 1) * (m - 1)))\n\tdf = (df or (((m == 1) and (n - 1)) or (m - 1)))\n\tg = 0.0\n\tfor i in xrange(n):\n\t\tfor j in xrange(m):\n\t\t\tif ((o[i][j] != 0) and (e[i][j] != 0)):\n\t\t\t\tg += (o[i][j] * log((o[i][j] \/ e[i][j])))\n\tg = (g * 2)\n\tp = gammai((df * 0.5), (g * 0.5), tail)\n\treturn (g, p)\n",["returns for the n x m observed and expected data ."]]
["def save_instance(form, instance, fields=None, fail_message=u'saved', commit=True, exclude=None, construct=True):\n\tif construct:\n\t\tinstance = construct_instance(form, instance, fields, exclude)\n\topts = instance._meta\n\tif form.errors:\n\t\traise ValueError((u\"The\t%s\tcould\tnot\tbe\t%s\tbecause\tthe\tdata\tdidn't\tvalidate.\" % (opts.object_name, fail_message)))\n\tdef save_m2m():\n\t\tcleaned_data = form.cleaned_data\n\t\tfor f in opts.many_to_many:\n\t\t\tif (fields and (f.name not in fields)):\n\t\t\t\tcontinue\n\t\t\tif (f.name in cleaned_data):\n\t\t\t\tf.save_form_data(instance, cleaned_data[f.name])\n\tif commit:\n\t\tinstance.save()\n\t\tsave_m2m()\n\telse:\n\t\tform.save_m2m = save_m2m\n\treturn instance\n",["saves bound form forms cleaned_data into model instance instance ."]]
["def isLineIntersectingLoops(loops, pointBegin, pointEnd):\n\tnormalizedSegment = (pointEnd - pointBegin)\n\tnormalizedSegmentLength = abs(normalizedSegment)\n\tif (normalizedSegmentLength > 0.0):\n\t\tnormalizedSegment \/= normalizedSegmentLength\n\t\tsegmentYMirror = complex(normalizedSegment.real, (- normalizedSegment.imag))\n\t\tpointBeginRotated = (segmentYMirror * pointBegin)\n\t\tpointEndRotated = (segmentYMirror * pointEnd)\n\t\tif isLoopListIntersectingInsideXSegment(loops, pointBeginRotated.real, pointEndRotated.real, segmentYMirror, pointBeginRotated.imag):\n\t\t\treturn True\n\treturn False\n",["determine if the line is intersecting loops ."]]
["def add_lazy_relation(cls, field, relation, operation):\n\tif (relation == RECURSIVE_RELATIONSHIP_CONSTANT):\n\t\tapp_label = cls._meta.app_label\n\t\tmodel_name = cls.__name__\n\telse:\n\t\ttry:\n\t\t\t(app_label, model_name) = relation.split('.')\n\t\texcept ValueError:\n\t\t\tapp_label = cls._meta.app_label\n\t\t\tmodel_name = relation\n\t\texcept AttributeError:\n\t\t\tapp_label = relation._meta.app_label\n\t\t\tmodel_name = relation._meta.object_name\n\tmodel = get_model(app_label, model_name, seed_cache=False, only_installed=False)\n\tif model:\n\t\toperation(field, model, cls)\n\telse:\n\t\tkey = (app_label, model_name)\n\t\tvalue = (cls, field, operation)\n\t\tpending_lookups.setdefault(key, []).append(value)\n",["adds a lookup on cls when a related field is defined using a string ."]]
["@register.tag('cache')\ndef do_cache(parser, token):\n\tnodelist = parser.parse(('endcache',))\n\tparser.delete_first_token()\n\ttokens = token.split_contents()\n\tif (len(tokens) < 3):\n\t\traise TemplateSyntaxError((\"'%r'\ttag\trequires\tat\tleast\t2\targuments.\" % tokens[0]))\n\tif ((len(tokens) > 3) and tokens[(-1)].startswith('using=')):\n\t\tcache_name = parser.compile_filter(tokens[(-1)][len('using='):])\n\t\ttokens = tokens[:(-1)]\n\telse:\n\t\tcache_name = None\n\treturn CacheNode(nodelist, parser.compile_filter(tokens[1]), tokens[2], [parser.compile_filter(t) for t in tokens[3:]], cache_name)\n",["this will cache the contents of a template fragment for a given amount of time ."]]
["def parseParam(line):\n\tif (line == ''):\n\t\treturn (None, '')\n\telif (line[0] != '\"'):\n\t\tmode = 1\n\telse:\n\t\tmode = 2\n\tres = ''\n\tio = StringIO(line)\n\tif (mode == 2):\n\t\tio.read(1)\n\twhile 1:\n\t\ta = io.read(1)\n\t\tif (a == '\"'):\n\t\t\tif (mode == 2):\n\t\t\t\tio.read(1)\n\t\t\t\treturn (res, io.read())\n\t\telif (a == '\\\\'):\n\t\t\ta = io.read(1)\n\t\t\tif (a == ''):\n\t\t\t\treturn (None, line)\n\t\telif (a == ''):\n\t\t\tif (mode == 1):\n\t\t\t\treturn (res, io.read())\n\t\t\telse:\n\t\t\t\treturn (None, line)\n\t\telif (a == '\t'):\n\t\t\tif (mode == 1):\n\t\t\t\treturn (res, io.read())\n\t\tres += a\n",["chew one dqstring or atom from beginning of line and return ."]]
["def processSVGElementpolygon(elementNode, svgReader):\n\tif ('points' not in elementNode.attributes):\n\t\tprint 'Warning,\tin\tprocessSVGElementpolygon\tin\tsvgReader\tcan\tnot\tget\ta\tvalue\tfor\td\tin:'\n\t\tprint elementNode.attributes\n\t\treturn\n\tloopLayer = svgReader.getLoopLayer()\n\twords = getRightStripMinusSplit(elementNode.attributes['points'].replace(',', '\t'))\n\tloop = []\n\tfor wordIndex in xrange(0, len(words), 2):\n\t\tloop.append(euclidean.getComplexByWords(words[wordIndex:]))\n\tloopLayer.loops += getTransformedFillOutline(elementNode, loop, svgReader.yAxisPointingUpward)\n",["process xmlelement by svgreader ."]]
["def run_migrations_online():\n\tconnectable = settings.engine\n\twith connectable.connect() as connection:\n\t\tcontext.configure(connection=connection, target_metadata=target_metadata, compare_type=COMPARE_TYPE)\n\t\twith context.begin_transaction():\n\t\t\tcontext.run_migrations()\n",["run migrations in online mode ."]]
["def forbid_multi_line_headers(name, val, encoding):\n\tencoding = (encoding or settings.DEFAULT_CHARSET)\n\tval = force_text(val)\n\tif (('\\n' in val) or ('\\r' in val)):\n\t\traise BadHeaderError((\"Header\tvalues\tcan't\tcontain\tnewlines\t(got\t%r\tfor\theader\t%r)\" % (val, name)))\n\ttry:\n\t\tval.encode('ascii')\n\texcept UnicodeEncodeError:\n\t\tif (name.lower() in ADDRESS_HEADERS):\n\t\t\tval = ',\t'.join((sanitize_address(addr, encoding) for addr in getaddresses((val,))))\n\t\telse:\n\t\t\tval = Header(val, encoding).encode()\n\telse:\n\t\tif (name.lower() == 'subject'):\n\t\t\tval = Header(val).encode()\n\treturn (name, val)\n",["forbids multi-line headers ."]]
["def latex_output_graph(self, node):\n\tgraph = node['graph']\n\tparts = node['parts']\n\tgraph_hash = get_graph_hash(node)\n\tname = ('inheritance%s' % graph_hash)\n\tdest_path = os.path.abspath(os.path.join(setup.app.builder.outdir, '_images'))\n\tif (not os.path.exists(dest_path)):\n\t\tos.makedirs(dest_path)\n\tpdf_path = os.path.abspath(os.path.join(dest_path, (name + '.pdf')))\n\tgraph.run_dot(['-Tpdf', ('-o%s' % pdf_path)], name, parts, graph_options={'size': '\"6.0,6.0\"'})\n\treturn ('\\n\\\\includegraphics{%s}\\n\\n' % pdf_path)\n",["output the graph for latex ."]]
["@pytest.mark.network\ndef test_outdated_editables_columns_flag(script, data):\n\tscript.pip('install', '-f', data.find_links, '--no-index', 'simple==1.0')\n\tresult = script.pip('install', '-e', 'git+https:\/\/github.com\/pypa\/pip-test-package.git@0.1#egg=pip-test-package')\n\tresult = script.pip('list', '-f', data.find_links, '--no-index', '--editable', '--outdated', '--format=columns')\n\tassert ('Package' in result.stdout)\n\tassert ('Version' in result.stdout)\n\tassert ('Location' in result.stdout)\n\tassert (os.path.join('src', 'pip-test-package') in result.stdout), str(result)\n",["test the behavior of --editable --outdated flag in the list command ."]]
["def apply_nms(all_boxes, thresh):\n\tnum_classes = len(all_boxes)\n\tnum_images = len(all_boxes[0])\n\tnms_boxes = [[[] for _ in xrange(num_images)] for _ in xrange(num_classes)]\n\tfor cls_ind in xrange(num_classes):\n\t\tfor im_ind in xrange(num_images):\n\t\t\tdets = all_boxes[cls_ind][im_ind]\n\t\t\tif (dets == []):\n\t\t\t\tcontinue\n\t\t\tkeep = nms(dets, thresh)\n\t\t\tif (len(keep) == 0):\n\t\t\t\tcontinue\n\t\t\tnms_boxes[cls_ind][im_ind] = dets[keep, :].copy()\n\treturn nms_boxes\n",["apply non-maximum suppression to all predicted boxes output by the test_net method ."]]
["def _create_glance_client(context, netloc, use_ssl, version=None):\n\tif (version is None):\n\t\tversion = CONF.glance_api_version\n\tparams = {}\n\tif use_ssl:\n\t\tscheme = 'https'\n\t\tparams['insecure'] = CONF.glance_api_insecure\n\t\tparams['ssl_compression'] = CONF.glance_api_ssl_compression\n\t\tparams['cacert'] = CONF.glance_ca_certificates_file\n\telse:\n\t\tscheme = 'http'\n\tif (CONF.auth_strategy == 'keystone'):\n\t\tparams['token'] = context.auth_token\n\tif (CONF.glance_request_timeout is not None):\n\t\tparams['timeout'] = CONF.glance_request_timeout\n\tendpoint = ('%s:\/\/%s' % (scheme, netloc))\n\treturn glanceclient.Client(str(version), endpoint, **params)\n",["instantiate a new glanceclient ."]]
["def _suffix_rules(token, tag='NN'):\n\tif isinstance(token, (list, tuple)):\n\t\t(token, tag) = token\n\tif token.endswith('ing'):\n\t\ttag = 'VBG'\n\tif token.endswith('ly'):\n\t\ttag = 'RB'\n\tif (token.endswith('s') and (not token.endswith(('is', 'ous', 'ss')))):\n\t\ttag = 'NNS'\n\tif (token.endswith(('able', 'al', 'ful', 'ible', 'ient', 'ish', 'ive', 'less', 'tic', 'ous')) or ('-' in token)):\n\t\ttag = 'JJ'\n\tif token.endswith('ed'):\n\t\ttag = 'VBN'\n\tif token.endswith(('ate', 'ify', 'ise', 'ize')):\n\t\ttag = 'VBP'\n\treturn [token, tag]\n",["default morphological tagging rules for english ."]]
["def test_rgb_to_hsl_part_0():\n\tassert (rgb_to_hsl(255, 0, 0) == (0, 100, 50))\n\tassert (rgb_to_hsl(255, 255, 0) == (60, 100, 50))\n\tassert (rgb_to_hsl(0, 255, 0) == (120, 100, 50))\n\tassert (rgb_to_hsl(0, 255, 255) == (180, 100, 50))\n\tassert (rgb_to_hsl(0, 0, 255) == (240, 100, 50))\n\tassert (rgb_to_hsl(255, 0, 255) == (300, 100, 50))\n",["test rgb to hsl color function ."]]
["def list_nodes(call=None):\n\tif (call == 'action'):\n\t\traise SaltCloudSystemExit('The\tlist_nodes\tfunction\tmust\tbe\tcalled\twith\t-f\tor\t--function.')\n\tnodes = list_nodes_full()\n\tret = {}\n\tfor (instance_id, full_node) in nodes.items():\n\t\tret[instance_id] = {'id': full_node['id'], 'image': full_node['image'], 'size': full_node['size'], 'state': full_node['state'], 'public_ips': full_node['public_ips'], 'private_ips': full_node['private_ips']}\n\treturn ret\n",["return a list of the vms that are on the provider ."]]
["def test_extract_array_1d_trim():\n\tassert np.all((extract_array(np.arange(4), (2,), (0,), mode=u'trim') == np.array([0])))\n\tfor i in [1, 2, 3]:\n\t\tassert np.all((extract_array(np.arange(4), (2,), (i,), mode=u'trim') == np.array([(i - 1), i])))\n\tassert np.all((extract_array(np.arange(4.0), (2,), (4,), mode=u'trim') == np.array([3])))\n",["extract 1 d arrays ."]]
["def Array(typecode_or_type, size_or_initializer, **kwds):\n\tlock = kwds.pop('lock', None)\n\tif kwds:\n\t\traise ValueError(('unrecognized\tkeyword\targument(s):\t%s' % kwds.keys()))\n\tobj = RawArray(typecode_or_type, size_or_initializer)\n\tif (lock is None):\n\t\tlock = RLock()\n\tassert hasattr(lock, 'acquire')\n\treturn synchronized(obj, lock)\n",["return a synchronization wrapper for a rawarray ."]]
["def _get_splunk(profile):\n\tconfig = __salt__['config.option'](profile)\n\tkey = 'splunk_search.{0}:{1}:{2}:{3}'.format(config.get('host'), config.get('port'), config.get('username'), config.get('password'))\n\tif (key not in __context__):\n\t\t__context__[key] = splunklib.client.connect(host=config.get('host'), port=config.get('port'), username=config.get('username'), password=config.get('password'))\n\treturn __context__[key]\n",["return the splunk client ."]]
["def bare_error(extrabody=None):\n\tbody = 'Unrecoverable\terror\tin\tthe\tserver.'\n\tif (extrabody is not None):\n\t\tif (not isinstance(extrabody, str)):\n\t\t\textrabody = extrabody.encode('utf-8')\n\t\tbody += ('\\n' + extrabody)\n\treturn ('500\tInternal\tServer\tError', [('Content-Type', 'text\/plain'), ('Content-Length', str(len(body)))], [body])\n",["produce status ."]]
["def safeRef(target, onDelete=None):\n\tif hasattr(target, '__self__'):\n\t\tif (target.__self__ is not None):\n\t\t\tassert hasattr(target, '__func__'), (\"safeRef\ttarget\t%r\thas\t__self__,\tbut\tno\t__func__,\tdon't\tknow\thow\tto\tcreate\treference\" % (target,))\n\t\t\treference = get_bound_method_weakref(target=target, onDelete=onDelete)\n\t\t\treturn reference\n\tif callable(onDelete):\n\t\treturn weakref.ref(target, onDelete)\n\telse:\n\t\treturn weakref.ref(target)\n",["return a *safe* weak reference to a callable target target -- the object to be weakly referenced ."]]
["def fetch_from_url_to_file(url, config, output_file, data=None, handlers=None):\n\t(return_code, return_message, response) = open_url(url, config, data=data, handlers=handlers)\n\tif (return_code == http_client_.OK):\n\t\treturn_data = response.read()\n\t\tresponse.close()\n\t\toutfile = open(output_file, 'w')\n\t\toutfile.write(return_data)\n\t\toutfile.close()\n\treturn (return_code, return_message, (return_code == http_client_.OK))\n",["writes data retrieved from a url to a file ."]]
["def _spectrogram(x, fs=1.0, window=('tukey', 0.25), nperseg=256, noverlap=None, nfft=None, detrend='constant', return_onesided=True, scaling='density', axis=(-1), mode='psd'):\n\tif (noverlap is None):\n\t\tnoverlap = (nperseg \/\/ 8)\n\t(freqs, time, Pxy) = _spectral_helper(x, x, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, mode=mode)\n\treturn (freqs, time, Pxy)\n",["compute a spectrogram with consecutive fourier transforms ."]]
["def test_prompt_should_not_ask_if_no_input_and_rm_repo_dir(mocker, tmpdir):\n\tmock_read_user = mocker.patch('cookiecutter.vcs.read_user_yes_no', return_value=True, autospec=True)\n\trepo_dir = tmpdir.mkdir('repo')\n\tvcs.prompt_and_delete_repo(str(repo_dir), no_input=True)\n\tassert (not mock_read_user.called)\n\tassert (not repo_dir.exists())\n",["in prompt_and_delete_repo() ."]]
["def address_in_network(ip, net):\n\tipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]\n\t(netaddr, bits) = net.split('\/')\n\tnetmask = struct.unpack('=L', socket.inet_aton(dotted_netmask(int(bits))))[0]\n\tnetwork = (struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask)\n\treturn ((ipaddr & netmask) == (network & netmask))\n",["this function allows you to check if on ip belongs to a network subnet example: returns true if ip = 192 ."]]
["def _shared_login(request):\n\tcsession = request.session\n\tplayer = request.user\n\tsesslogin = csession.get('logged_in', None)\n\tif (csession.session_key is None):\n\t\tcsession.save()\n\telif player.is_authenticated():\n\t\tif (not sesslogin):\n\t\t\tcsession['logged_in'] = player.id\n\telif sesslogin:\n\t\tplayer = PlayerDB.objects.get(id=sesslogin)\n\t\ttry:\n\t\t\tplayer = authenticate(autologin=player)\n\t\t\tlogin(request, player)\n\t\texcept AttributeError:\n\t\t\tlogger.log_trace()\n",["handle the shared login between website and webclient ."]]
["def transform_hits(hits):\n\tpackages = {}\n\tfor hit in hits:\n\t\tname = hit['name']\n\t\tsummary = hit['summary']\n\t\tversion = hit['version']\n\t\tscore = hit['_pypi_ordering']\n\t\tif (name not in packages.keys()):\n\t\t\tpackages[name] = {'name': name, 'summary': summary, 'versions': [version], 'score': score}\n\t\telse:\n\t\t\tpackages[name]['versions'].append(version)\n\t\t\tif (version == highest_version(packages[name]['versions'])):\n\t\t\t\tpackages[name]['summary'] = summary\n\t\t\t\tpackages[name]['score'] = score\n\tpackage_list = sorted(packages.values(), key=(lambda x: x['score']), reverse=True)\n\treturn package_list\n",["the list from pypi is really a list of versions ."]]
["def new(rsa_key):\n\treturn PKCS115_SigScheme(rsa_key)\n",["return a fresh instance of the hash object ."]]
["def run(cmd, **kwargs):\n\tlog('-', cmd)\n\tcmd = cmd.split()\n\targ0 = cmd[0]\n\tif (not find_executable(arg0)):\n\t\traise Exception((('Cannot\tfind\texecutable\t\"%s\";' % arg0) + ('you\tmight\ttry\t%s\t--depend' % argv[0])))\n\treturn check_output(cmd, **kwargs)\n",["run the given command ."]]
["def recursive_keypairs(d, separator=':'):\n\tfor (name, value) in sorted(six.iteritems(d)):\n\t\tif isinstance(value, dict):\n\t\t\tfor (subname, subvalue) in recursive_keypairs(value, separator):\n\t\t\t\t(yield (('%s%s%s' % (name, separator, subname)), subvalue))\n\t\telif isinstance(value, (tuple, list)):\n\t\t\t(yield (name, decode_unicode(value)))\n\t\telse:\n\t\t\t(yield (name, value))\n",["generator that produces sequence of keypairs for nested dictionaries ."]]
["def import_library(taglib_module):\n\ttry:\n\t\tmod = import_module(taglib_module)\n\texcept ImportError as e:\n\t\tif is_library_missing(taglib_module):\n\t\t\treturn None\n\t\telse:\n\t\t\traise InvalidTemplateLibrary(('ImportError\traised\tloading\t%s:\t%s' % (taglib_module, e)))\n\ttry:\n\t\treturn mod.register\n\texcept AttributeError:\n\t\traise InvalidTemplateLibrary((\"Template\tlibrary\t%s\tdoes\tnot\thave\ta\tvariable\tnamed\t'register'\" % taglib_module))\n",["load a template tag library module ."]]
["def yaml_load(source, loader=yaml.Loader):\n\tdef construct_yaml_str(self, node):\n\t\tu'Override\tthe\tdefault\tstring\thandling\tfunction\tto\talways\treturn\tUnicode\tobjects.'\n\t\treturn self.construct_scalar(node)\n\tclass Loader(loader, ):\n\t\tu'Define\ta\tcustom\tloader\tto\tleave\tthe\tglobal\tloader\tunaltered.'\n\tLoader.add_constructor(u'tag:yaml.org,2002:str', construct_yaml_str)\n\treturn yaml.load(source, Loader)\n",["wrap pyyamls loader so we can extend it to suit our needs ."]]
["def make_analysator(f):\n\tdef text_analyse(text):\n\t\trv = f(text)\n\t\tif (not rv):\n\t\t\treturn 0.0\n\t\treturn min(1.0, max(0.0, float(rv)))\n\ttext_analyse.__doc__ = f.__doc__\n\treturn staticmethod(text_analyse)\n",["return a static text analysation function that returns float values ."]]
["def fromChunk(data):\n\t(prefix, rest) = data.split('\\r\\n', 1)\n\tlength = int(prefix, 16)\n\tif (length < 0):\n\t\traise ValueError(('Chunk\tlength\tmust\tbe\t>=\t0,\tnot\t%d' % (length,)))\n\tif (rest[length:(length + 2)] != '\\r\\n'):\n\t\traise ValueError('chunk\tmust\tend\twith\tCRLF')\n\treturn (rest[:length], rest[(length + 2):])\n",["convert chunk to string ."]]
["def read_cached_file(filename, cache_info, reload_func=None):\n\tmtime = os.path.getmtime(filename)\n\tif ((not cache_info) or (mtime != cache_info.get('mtime'))):\n\t\tLOG.debug((_('Reloading\tcached\tfile\t%s') % filename))\n\t\twith open(filename) as fap:\n\t\t\tcache_info['data'] = fap.read()\n\t\tcache_info['mtime'] = mtime\n\t\tif reload_func:\n\t\t\treload_func(cache_info['data'])\n\treturn cache_info['data']\n",["read from a file if it has been modified ."]]
["def set_mindays(name, mindays):\n\tpre_info = info(name)\n\tif (mindays == pre_info['min']):\n\t\treturn True\n\tcmd = 'passwd\t-n\t{0}\t{1}'.format(mindays, name)\n\t__salt__['cmd.run'](cmd, python_shell=False)\n\tpost_info = info(name)\n\tif (post_info['min'] != pre_info['min']):\n\t\treturn (post_info['min'] == mindays)\n\treturn False\n",["set the minimum number of days between password changes ."]]
["def net_if_stats():\n\tnames = net_io_counters().keys()\n\tret = {}\n\tfor name in names:\n\t\t(isup, duplex, speed, mtu) = cext_posix.net_if_stats(name)\n\t\tif hasattr(_common, 'NicDuplex'):\n\t\t\tduplex = _common.NicDuplex(duplex)\n\t\tret[name] = _common.snicstats(isup, duplex, speed, mtu)\n\treturn ret\n",["get nic stats ."]]
["def execute(*cmd, **kwargs):\n\tprocess_input = kwargs.pop('process_input', None)\n\tcheck_exit_code = kwargs.pop('check_exit_code', [0])\n\tignore_exit_code = False\n\tif isinstance(check_exit_code, bool):\n\t\tignore_exit_code = (not check_exit_code)\n\t\tcheck_exit_code = [0]\n\telif isinstance(check_exit_code, int):\n\t\tcheck_exit_code = [check_exit_code]\n\tdelay_on_retry = kwargs.pop('delay_on_retry', True)\n\tattempts = kwargs.pop('attempts', 1)\n\trun_as_root = kwargs.pop('run_as_root', False)\n\tshell = kwargs.pop('shell', False)\n\tif len(kwargs):\n\t\traise exception.NovaException((_('Got\tunknown\tkeyword\targs\tto\tutils.execute:\t%r') % kwargs))\n\tif (run_as_root and (os.geteuid() != 0)):\n\t\tcmd = (['sudo', 'nova-rootwrap', CONF.rootwrap_config] + list(cmd))\n\tcmd = map(str, cmd)\n\twhile (attempts > 0):\n\t\tattempts -= 1\n\t\ttry:\n\t\t\tLOG.debug(_('Running\tcmd\t(subprocess):\t%s'), '\t'.join(cmd))\n\t\t\t_PIPE = subprocess.PIPE\n\t\t\tif (os.name == 'nt'):\n\t\t\t\tpreexec_fn = None\n\t\t\t\tclose_fds = False\n\t\t\telse:\n\t\t\t\tpreexec_fn = _subprocess_setup\n\t\t\t\tclose_fds = True\n\t\t\tobj = subprocess.Popen(cmd, stdin=_PIPE, stdout=_PIPE, stderr=_PIPE, close_fds=close_fds, preexec_fn=preexec_fn, shell=shell)\n\t\t\tresult = None\n\t\t\tif (process_input is not None):\n\t\t\t\tresult = obj.communicate(process_input)\n\t\t\telse:\n\t\t\t\tresult = obj.communicate()\n\t\t\tobj.stdin.close()\n\t\t\t_returncode = obj.returncode\n\t\t\tLOG.debug((_('Result\twas\t%s') % _returncode))\n\t\t\tif ((not ignore_exit_code) and (_returncode not in check_exit_code)):\n\t\t\t\t(stdout, stderr) = result\n\t\t\t\traise exception.ProcessExecutionError(exit_code=_returncode, stdout=stdout, stderr=stderr, cmd='\t'.join(cmd))\n\t\t\treturn result\n\t\texcept exception.ProcessExecutionError:\n\t\t\tif (not attempts):\n\t\t\t\traise\n\t\t\telse:\n\t\t\t\tLOG.debug(_('%r\tfailed.\tRetrying.'), cmd)\n\t\t\t\tif delay_on_retry:\n\t\t\t\t\tgreenthread.sleep((random.randint(20, 200) \/ 100.0))\n\t\tfinally:\n\t\t\tgreenthread.sleep(0)\n",["perform some action that affects the outside world ."]]
["def get_mapped_batch(dataset, design_batch):\n\tif (design_batch.ndim != 2):\n\t\tdesign_batch = dataset.get_design_matrix(design_batch.copy())\n\tmapped_design = dataset.mapback(design_batch.copy())\n\tmapped_batch = dataset.get_topological_view(mapped_design.copy())\n\treturn mapped_batch\n",["get mapped batch if mapback_for_viewer is available with the dataset ."]]
["def bytestring_path(path):\n\tif isinstance(path, bytes):\n\t\treturn path\n\tif ((os.path.__name__ == 'ntpath') and path.startswith(WINDOWS_MAGIC_PREFIX)):\n\t\tpath = path[len(WINDOWS_MAGIC_PREFIX):]\n\ttry:\n\t\treturn path.encode(_fsencoding())\n\texcept (UnicodeError, LookupError):\n\t\treturn path.encode('utf-8')\n",["given a path ."]]
["def commit_manually(using=None):\n\twarnings.warn('commit_manually\tis\tdeprecated\tin\tfavor\tof\tset_autocommit.', PendingDeprecationWarning, stacklevel=2)\n\tdef entering(using):\n\t\tenter_transaction_management(using=using)\n\tdef exiting(exc_type, using):\n\t\tleave_transaction_management(using=using)\n\treturn _transaction_func(entering, exiting, using)\n",["decorator that activates manual transaction control ."]]
["def mark_safe(s):\n\tif hasattr(s, '__html__'):\n\t\treturn s\n\tif (isinstance(s, bytes) or (isinstance(s, Promise) and s._delegate_bytes)):\n\t\treturn SafeBytes(s)\n\tif isinstance(s, (str, Promise)):\n\t\treturn SafeText(s)\n\tif callable(s):\n\t\treturn _safety_decorator(mark_safe, s)\n\treturn SafeString(str(s))\n",["explicitly mark a string as safe for output purposes ."]]
["def getcaps():\n\tcaps = {}\n\tfor mailcap in listmailcapfiles():\n\t\ttry:\n\t\t\tfp = open(mailcap, 'r')\n\t\texcept IOError:\n\t\t\tcontinue\n\t\twith fp:\n\t\t\tmorecaps = readmailcapfile(fp)\n\t\tfor (key, value) in morecaps.iteritems():\n\t\t\tif (not (key in caps)):\n\t\t\t\tcaps[key] = value\n\t\t\telse:\n\t\t\t\tcaps[key] = (caps[key] + value)\n\treturn caps\n",["return a dictionary containing the mailcap database ."]]
["def _untranslate_snapshot_summary_view(context, snapshot):\n\td = {}\n\td['id'] = snapshot.id\n\td['status'] = snapshot.status\n\td['progress'] = snapshot.progress\n\td['size'] = snapshot.size\n\td['created_at'] = snapshot.created_at\n\td['display_name'] = snapshot.display_name\n\td['display_description'] = snapshot.display_description\n\td['volume_id'] = snapshot.volume_id\n\td['project_id'] = snapshot.project_id\n\td['volume_size'] = snapshot.size\n\treturn d\n",["maps keys for snapshots summary view ."]]
["def truncate(content, length=100, suffix='...'):\n\tif (len(content) <= length):\n\t\treturn content\n\telse:\n\t\treturn (content[:length].rsplit('\t', 1)[0] + suffix)\n",["wrapper for jinjas truncate that checks if the object has a __truncate__ attribute first ."]]
["def _default_selem(ndim):\n\treturn ndi.morphology.generate_binary_structure(ndim, 1)\n",["decorator to add a default structuring element to morphology functions ."]]
["def never_cache(view_func):\n\t@wraps(view_func)\n\tdef _wrapped_view_func(request, *args, **kwargs):\n\t\tresponse = view_func(request, *args, **kwargs)\n\t\tadd_never_cache_headers(response)\n\t\treturn response\n\treturn _wrapped_view_func\n",["decorator that adds headers to a response so that it will never be cached ."]]
["@require_POST\n@login_required\ndef post_preview_async(request, document_slug):\n\tstatsd.incr('forums.preview')\n\tpost = Post(creator=request.user, content=request.POST.get('content', ''))\n\treturn render(request, 'kbforums\/includes\/post_preview.html', {'post_preview': post})\n",["ajax preview of posts ."]]
["def _get_model(model_identifier):\n\ttry:\n\t\tModel = models.get_model(*model_identifier.split('.'))\n\texcept TypeError:\n\t\tModel = None\n\tif (Model is None):\n\t\traise base.DeserializationError((\"Invalid\tmodel\tidentifier:\t'%s'\" % model_identifier))\n\treturn Model\n",["helper to look up a model from an \"app_label ."]]
["def _strip_once(value):\n\ts = MLStripper()\n\ttry:\n\t\ts.feed(value)\n\texcept HTMLParseError:\n\t\treturn value\n\ttry:\n\t\ts.close()\n\texcept HTMLParseError:\n\t\treturn (s.get_data() + s.rawdata)\n\telse:\n\t\treturn s.get_data()\n",["internal tag stripping utility used by strip_tags ."]]
["def resume(vm_):\n\twith _get_xapi_session() as xapi:\n\t\tvm_uuid = _get_label_uuid(xapi, 'VM', vm_)\n\t\tif (vm_uuid is False):\n\t\t\treturn False\n\t\ttry:\n\t\t\txapi.VM.unpause(vm_uuid)\n\t\t\treturn True\n\t\texcept Exception:\n\t\t\treturn False\n",["resume the named vm cli example: ."]]
["def getResolver():\n\tglobal theResolver\n\tif (theResolver is None):\n\t\ttry:\n\t\t\ttheResolver = createResolver()\n\t\texcept ValueError:\n\t\t\ttheResolver = createResolver(servers=[('127.0.0.1', 53)])\n\treturn theResolver\n",["get a resolver instance ."]]
["def _convert_other(other, raiseit=False, allow_float=False):\n\tif isinstance(other, Decimal):\n\t\treturn other\n\tif isinstance(other, (int, long)):\n\t\treturn Decimal(other)\n\tif (allow_float and isinstance(other, float)):\n\t\treturn Decimal.from_float(other)\n\timport sys\n\tif (sys.platform == 'cli'):\n\t\timport System\n\t\tif isinstance(other, System.Decimal):\n\t\t\treturn Decimal(other)\n\tif raiseit:\n\t\traise TypeError(('Unable\tto\tconvert\t%s\tto\tDecimal' % other))\n\treturn NotImplemented\n",["convert other to decimal ."]]
["def is_library_missing(name):\n\t(path, module) = name.rsplit(u'.', 1)\n\ttry:\n\t\tpackage = import_module(path)\n\t\treturn (not module_has_submodule(package, module))\n\texcept ImportError:\n\t\treturn is_library_missing(path)\n",["check if library that failed to load cannot be found under any templatetags directory or does exist but fails to import ."]]
["def send_from_directory(directory, filename, **options):\n\tfilename = safe_join(directory, filename)\n\tif (not os.path.isfile(filename)):\n\t\traise NotFound()\n\toptions.setdefault('conditional', True)\n\treturn send_file(filename, **options)\n",["send a file from a given directory with :func:send_file ."]]
["def assert_aws_environ():\n\ttry:\n\t\timport boto\n\texcept ImportError as e:\n\t\traise SkipTest(str(e))\n\tif ('AWS_ACCESS_KEY_ID' not in os.environ):\n\t\traise SkipTest('AWS\tkeys\tnot\tfound')\n",["asserts the current environment is suitable for running aws testsi ."]]
["def restart(name, jail=None):\n\tcmd = '{0}\t{1}\tonerestart'.format(_cmd(jail), name)\n\treturn (not __salt__['cmd.retcode'](cmd, python_shell=False))\n",["restart the named service cli example: ."]]
["def DeleteCampaignFeed(client, campaign_feed):\n\tcampaign_feed_service = client.GetService('CampaignFeedService', 'v201607')\n\toperation = {'operand': campaign_feed, 'operator': 'REMOVE'}\n\tcampaign_feed_service.mutate([operation])\n",["deletes a campaign feed ."]]
["def splittag(url):\n\t(path, delim, tag) = url.rpartition('#')\n\tif delim:\n\t\treturn (path, tag)\n\treturn (url, None)\n",["splittag --> \/path ."]]
["def mail_managers(subject, message, fail_silently=False, connection=None):\n\tif (not settings.MANAGERS):\n\t\treturn\n\tEmailMessage((u'%s%s' % (settings.EMAIL_SUBJECT_PREFIX, subject)), message, settings.SERVER_EMAIL, [a[1] for a in settings.MANAGERS], connection=connection).send(fail_silently=fail_silently)\n",["sends a message to the managers ."]]
["def FixedOffset(offset, _tzinfos={}):\n\tif (offset == 0):\n\t\treturn UTC\n\tinfo = _tzinfos.get(offset)\n\tif (info is None):\n\t\tinfo = _tzinfos.setdefault(offset, _FixedOffset(offset))\n\treturn info\n",["return a fixed-offset timezone based off a number of minutes ."]]
["def subn(pattern, repl, string, count=0, flags=0, pos=None, endpos=None, concurrent=None, **kwargs):\n\treturn _compile(pattern, flags, kwargs).subn(repl, string, count, pos, endpos, concurrent)\n",["return a 2-tuple containing ."]]
["def task_project():\n\tif (auth.permission.format != 's3json'):\n\t\treturn ''\n\tdef prep(r):\n\t\tif (r.method != 'options'):\n\t\t\treturn False\n\t\treturn True\n\ts3.prep = prep\n\treturn s3_rest_controller()\n",["restful crud controller for options ."]]
["def vary_on_cookie(func):\n\tdef inner_func(*args, **kwargs):\n\t\tresponse = func(*args, **kwargs)\n\t\tpatch_vary_headers(response, ('Cookie',))\n\t\treturn response\n\treturn inner_func\n",["a view decorator that adds \"cookie\" to the vary header of a response ."]]
["def print_last(limit=None, file=None):\n\tif (not hasattr(sys, 'last_type')):\n\t\traise ValueError('no\tlast\texception')\n\tif (file is None):\n\t\tfile = sys.stderr\n\tprint_exception(sys.last_type, sys.last_value, sys.last_traceback, limit, file)\n",["this is a shorthand for print_exception ."]]
["def getGeometryOutputByArguments(arguments, elementNode):\n\tevaluate.setAttributesByArguments(['sides', 'radius'], arguments, elementNode)\n\treturn getGeometryOutput(None, elementNode)\n",["get vector3 vertexes from attribute dictionary by arguments ."]]
["def urljoin(base_uri, uri_reference):\n\treturn urlunsplit(urljoin_parts(urlsplit(base_uri), urlsplit(uri_reference)))\n",["return the given path *atoms ."]]
["def get_authorization_header(request):\n\tauth = request.META.get(u'HTTP_AUTHORIZATION', '')\n\tif isinstance(auth, text_type):\n\t\tauth = auth.encode(HTTP_HEADER_ENCODING)\n\treturn auth\n",["return requests authorization: header ."]]
["def quota_usage_update(context, project_id, resource, **kwargs):\n\treturn IMPL.quota_usage_update(context, project_id, resource, **kwargs)\n",["update a quota usage or raise if it does not exist ."]]
["def volume_type_extra_specs_delete(context, volume_type_id, key):\n\tIMPL.volume_type_extra_specs_delete(context, volume_type_id, key)\n",["delete the given extra specs item ."]]
["def getManipulatedGeometryOutput(elementNode, geometryOutput, prefix):\n\tflippedGeometryOutput = triangle_mesh.getGeometryOutputCopy(geometryOutput)\n\tflip.flipPoints(elementNode, matrix.getVertexes(flippedGeometryOutput), prefix)\n\tif flip.getShouldReverse(elementNode, prefix):\n\t\tflippedFaces = face.getFaces(flippedGeometryOutput)\n\t\tfor flippedFace in flippedFaces:\n\t\t\tflippedFace.vertexIndexes.reverse()\n\treturn {'union': {'shapes': [flippedGeometryOutput, geometryOutput]}}\n",["get equated geometryoutput ."]]
["def resize_image(image, height, width, channels=None, resize_mode=None):\n\tif (resize_mode is None):\n\t\tresize_mode = 'squash'\n\tif (resize_mode not in ['crop', 'squash', 'fill', 'half_crop']):\n\t\traise ValueError(('resize_mode\t\"%s\"\tnot\tsupported' % resize_mode))\n\timage = image_to_array(image, channels)\n\tif ((image.shape[0] == height) and (image.shape[1] == width)):\n\t\treturn image\n\tinterp = 'bilinear'\n\twidth_ratio = (float(image.shape[1]) \/ width)\n\theight_ratio = (float(image.shape[0]) \/ height)\n\tif ((resize_mode == 'squash') or (width_ratio == height_ratio)):\n\t\treturn scipy.misc.imresize(image, (height, width), interp=interp)\n\telif (resize_mode == 'crop'):\n\t\tif (width_ratio > height_ratio):\n\t\t\tresize_height = height\n\t\t\tresize_width = int(round((image.shape[1] \/ height_ratio)))\n\t\telse:\n\t\t\tresize_width = width\n\t\t\tresize_height = int(round((image.shape[0] \/ width_ratio)))\n\t\timage = scipy.misc.imresize(image, (resize_height, resize_width), interp=interp)\n\t\tif (width_ratio > height_ratio):\n\t\t\tstart = int(round(((resize_width - width) \/ 2.0)))\n\t\t\treturn image[:, start:(start + width)]\n\t\telse:\n\t\t\tstart = int(round(((resize_height - height) \/ 2.0)))\n\t\t\treturn image[start:(start + height), :]\n\telse:\n\t\tif (resize_mode == 'fill'):\n\t\t\tif (width_ratio > height_ratio):\n\t\t\t\tresize_width = width\n\t\t\t\tresize_height = int(round((image.shape[0] \/ width_ratio)))\n\t\t\t\tif (((height - resize_height) % 2) == 1):\n\t\t\t\t\tresize_height += 1\n\t\t\telse:\n\t\t\t\tresize_height = height\n\t\t\t\tresize_width = int(round((image.shape[1] \/ height_ratio)))\n\t\t\t\tif (((width - resize_width) % 2) == 1):\n\t\t\t\t\tresize_width += 1\n\t\t\timage = scipy.misc.imresize(image, (resize_height, resize_width), interp=interp)\n\t\telif (resize_mode == 'half_crop'):\n\t\t\tnew_ratio = ((width_ratio + height_ratio) \/ 2.0)\n\t\t\tresize_width = int(round((image.shape[1] \/ new_ratio)))\n\t\t\tresize_height = int(round((image.shape[0] \/ new_ratio)))\n\t\t\tif ((width_ratio > height_ratio) and (((height - resize_height) % 2) == 1)):\n\t\t\t\tresize_height += 1\n\t\t\telif ((width_ratio < height_ratio) and (((width - resize_width) % 2) == 1)):\n\t\t\t\tresize_width += 1\n\t\t\timage = scipy.misc.imresize(image, (resize_height, resize_width), interp=interp)\n\t\t\tif (width_ratio > height_ratio):\n\t\t\t\tstart = int(round(((resize_width - width) \/ 2.0)))\n\t\t\t\timage = image[:, start:(start + width)]\n\t\t\telse:\n\t\t\t\tstart = int(round(((resize_height - height) \/ 2.0)))\n\t\t\t\timage = image[start:(start + height), :]\n\t\telse:\n\t\t\traise Exception(('unrecognized\tresize_mode\t\"%s\"' % resize_mode))\n\t\tif (width_ratio > height_ratio):\n\t\t\tpadding = ((height - resize_height) \/ 2)\n\t\t\tnoise_size = (padding, width)\n\t\t\tif (channels > 1):\n\t\t\t\tnoise_size += (channels,)\n\t\t\tnoise = np.random.randint(0, 255, noise_size).astype('uint8')\n\t\t\timage = np.concatenate((noise, image, noise), axis=0)\n\t\telse:\n\t\t\tpadding = ((width - resize_width) \/ 2)\n\t\t\tnoise_size = (height, padding)\n\t\t\tif (channels > 1):\n\t\t\t\tnoise_size += (channels,)\n\t\t\tnoise = np.random.randint(0, 255, noise_size).astype('uint8')\n\t\t\timage = np.concatenate((noise, image, noise), axis=1)\n\t\treturn image\n",["returns path to an image resized to maxwidth ."]]
["def writeOutput(fileName=''):\n\tfileName = fabmetheus_interpret.getFirstTranslatorFileNameUnmodified(fileName)\n\tif (fileName != ''):\n\t\tskeinforge_craft.writeChainTextWithNounMessage(fileName, 'unpause')\n",["flow a gcode linear move file ."]]
["def backends(user):\n\treturn user_backends_data(user, get_helper('AUTHENTICATION_BACKENDS'), get_helper('STORAGE', do_import=True))\n",["load social auth current user data to context under the key backends ."]]
["def article(word, function=INDEFINITE):\n\treturn (((function == DEFINITE) and definite_article(word)) or indefinite_article(word))\n",["returns the indefinite or definite article for the given word ."]]
["def sort(filename, key, outputFile, fields=None, watermark=((1024 * 1024) * 100)):\n\tif (fields is not None):\n\t\tassert set(key).issubset(set([f[0] for f in fields]))\n\twith FileRecordStream(filename) as f:\n\t\tif fields:\n\t\t\tfieldNames = [ff[0] for ff in fields]\n\t\t\tindices = [f.getFieldNames().index(name) for name in fieldNames]\n\t\t\tassert (len(indices) == len(fields))\n\t\telse:\n\t\t\tfileds = f.getFields()\n\t\t\tfieldNames = f.getFieldNames()\n\t\t\tindices = None\n\t\tkey = [fieldNames.index(name) for name in key]\n\t\tchunk = 0\n\t\trecords = []\n\t\tfor (i, r) in enumerate(f):\n\t\t\tif indices:\n\t\t\t\ttemp = []\n\t\t\t\tfor i in indices:\n\t\t\t\t\ttemp.append(r[i])\n\t\t\t\tr = temp\n\t\t\trecords.append(r)\n\t\t\tavailable_memory = psutil.avail_phymem()\n\t\t\tif (available_memory < watermark):\n\t\t\t\t_sortChunk(records, key, chunk, fields)\n\t\t\t\trecords = []\n\t\t\t\tchunk += 1\n\t\tif (len(records) > 0):\n\t\t\t_sortChunk(records, key, chunk, fields)\n\t\t\tchunk += 1\n\t\t_mergeFiles(key, chunk, outputFile, fields)\n",["sort the given list of items by dependency ."]]
["def is_aware(value):\n\treturn ((value.tzinfo is not None) and (value.tzinfo.utcoffset(value) is not None))\n",["determines if a given datetime ."]]
["def check_isinstance(obj, cls):\n\tif isinstance(obj, cls):\n\t\treturn obj\n\traise Exception((_('Expected\tobject\tof\ttype:\t%s') % str(cls)))\n\treturn cls()\n",["checks that obj is of type cls ."]]
["def get_template(path, dest, template='jinja', saltenv='base', makedirs=False, **kwargs):\n\tif ('salt' not in kwargs):\n\t\tkwargs['salt'] = __salt__\n\tif ('pillar' not in kwargs):\n\t\tkwargs['pillar'] = __pillar__\n\tif ('grains' not in kwargs):\n\t\tkwargs['grains'] = __grains__\n\tif ('opts' not in kwargs):\n\t\tkwargs['opts'] = __opts__\n\treturn _client().get_template(path, dest, template, makedirs, saltenv, **kwargs)\n",["this is a proxy function to hide microsite_configuration behind comprehensive theming ."]]
["@pytest.fixture\ndef project0():\n\tfrom pootle_project.models import Project\n\treturn Project.objects.get(code='project0')\n",["project0 project ."]]
["def _project_cert_subject(project_id):\n\treturn (CONF.project_cert_subject % (project_id, timeutils.isotime()))\n",["helper to generate user cert subject ."]]
["@library.global_function\ndef unlocalized_url(viewname, *args, **kwargs):\n\treturn django_reverse(viewname, args=args, kwargs=kwargs)\n",["helper for djangos reverse in templates ."]]
["def getTextLines(text):\n\tif ('\\r' in text):\n\t\ttext = text.replace('\\r', '\\n').replace('\\n\\n', '\\n')\n\ttextLines = text.split('\\n')\n\tif (len(textLines) == 1):\n\t\tif (textLines[0] == ''):\n\t\t\treturn []\n\treturn textLines\n",["get the all the lines of text of a text ."]]
["def connections_support_transactions():\n\treturn all((conn.settings_dict['SUPPORTS_TRANSACTIONS'] for conn in connections.all()))\n",["returns true if all connections support transactions ."]]
["def test_sarcasm():\n\tdirty = u'Yeah\tright\t<sarcasm\/>'\n\tclean = u'Yeah\tright\t&lt;sarcasm\/&gt;'\n\teq_(clean, linkify(dirty))\n",["jokes should crash ."]]
["def force_escape(value):\n\tfrom django.utils.html import escape\n\treturn mark_safe(escape(value))\n",["escapes a strings html ."]]
["def update_last_login(sender, user, **kwargs):\n\tuser.last_login = timezone.now()\n\tuser.save()\n",["a signal receiver which updates the last_login date for the user logging in ."]]
["def getPluginsDirectoryPath():\n\treturn archive.getAbsoluteFolderPath(os.path.dirname(__file__), os.path.join('skeinforge_plugins', 'meta_plugins'))\n",["get the plugins directory path ."]]
["def virtual_interface_create(name, net_name, **kwargs):\n\tconn = get_conn()\n\treturn conn.virtual_interface_create(name, net_name)\n",["create a virtual interface record in the database ."]]
["def consistencygroup_get(context, consistencygroup_id):\n\treturn IMPL.consistencygroup_get(context, consistencygroup_id)\n",["get a consistencygroup or raise if it does not exist ."]]
["def restart_httpd(request):\n\tnotifier().restart('http')\n\treturn HttpResponse('OK')\n",["restart httpd ."]]
["def is_executable(exe):\n\treturn os.access(exe, os.X_OK)\n",["checks a file is executable ."]]
["def processElementNode(elementNode):\n\tpath.convertElementNode(elementNode, getGeometryOutput(None, elementNode))\n",["process the xml element ."]]
["def getNewRepository():\n\treturn ExportRepository()\n",["get new repository ."]]
["def _approximate_mode(class_counts, n_draws, rng):\n\tcontinuous = ((n_draws * class_counts) \/ class_counts.sum())\n\tfloored = np.floor(continuous)\n\tneed_to_add = int((n_draws - floored.sum()))\n\tif (need_to_add > 0):\n\t\tremainder = (continuous - floored)\n\t\tvalues = np.sort(np.unique(remainder))[::(-1)]\n\t\tfor value in values:\n\t\t\t(inds,) = np.where((remainder == value))\n\t\t\tadd_now = min(len(inds), need_to_add)\n\t\t\tinds = choice(inds, size=add_now, replace=False, random_state=rng)\n\t\t\tfloored[inds] += 1\n\t\t\tneed_to_add -= add_now\n\t\t\tif (need_to_add == 0):\n\t\t\t\tbreak\n\treturn floored.astype(np.int)\n",["computes approximate mode of multivariate hypergeometric ."]]
["def reverse_field_path(model, path):\n\treversed_path = []\n\tparent = model\n\tpieces = path.split(LOOKUP_SEP)\n\tfor piece in pieces:\n\t\tfield = parent._meta.get_field(piece)\n\t\tdirect = ((not field.auto_created) or field.concrete)\n\t\tif (len(reversed_path) == (len(pieces) - 1)):\n\t\t\ttry:\n\t\t\t\tget_model_from_relation(field)\n\t\t\texcept NotRelationField:\n\t\t\t\tbreak\n\t\tif direct:\n\t\t\trelated_name = field.related_query_name()\n\t\t\tparent = field.rel.to\n\t\telse:\n\t\t\trelated_name = field.field.name\n\t\t\tparent = field.model\n\t\treversed_path.insert(0, related_name)\n\treturn (parent, LOOKUP_SEP.join(reversed_path))\n",["create a reversed field path ."]]
["def model_query(context, model, *args, **kwargs):\n\tsession = (kwargs.get('session') or get_session())\n\tread_deleted = (kwargs.get('read_deleted') or context.read_deleted)\n\tproject_only = kwargs.get('project_only', False)\n\tdef issubclassof_nova_base(obj):\n\t\treturn (isinstance(obj, type) and issubclass(obj, models.NovaBase))\n\tbase_model = model\n\tif (not issubclassof_nova_base(base_model)):\n\t\tbase_model = kwargs.get('base_model', None)\n\t\tif (not issubclassof_nova_base(base_model)):\n\t\t\traise Exception(_('model\tor\tbase_model\tparameter\tshould\tbe\tsubclass\tof\tNovaBase'))\n\tquery = session.query(model, *args)\n\tdefault_deleted_value = base_model.__mapper__.c.deleted.default.arg\n\tif (read_deleted == 'no'):\n\t\tquery = query.filter((base_model.deleted == default_deleted_value))\n\telif (read_deleted == 'yes'):\n\t\tpass\n\telif (read_deleted == 'only'):\n\t\tquery = query.filter((base_model.deleted != default_deleted_value))\n\telse:\n\t\traise Exception((_(\"Unrecognized\tread_deleted\tvalue\t'%s'\") % read_deleted))\n\tif (nova.context.is_user_context(context) and project_only):\n\t\tif (project_only == 'allow_none'):\n\t\t\tquery = query.filter(or_((base_model.project_id == context.project_id), (base_model.project_id == None)))\n\t\telse:\n\t\t\tquery = query.filter_by(project_id=context.project_id)\n\treturn query\n",["query helper that accounts for contexts read_deleted field ."]]
["def get_properties_of_kind(kind, start=None, end=None):\n\tq = Property.all(keys_only=True)\n\tq.ancestor(Property.key_for_kind(kind))\n\tif ((start is not None) and (start != '')):\n\t\tq.filter('__key__\t>=', Property.key_for_property(kind, start))\n\tif (end is not None):\n\t\tif (end == ''):\n\t\t\treturn []\n\t\tq.filter('__key__\t<', Property.key_for_property(kind, end))\n\treturn [Property.key_to_property(x) for x in q.run()]\n",["return all properties of kind in the specified range ."]]
["def add_credential(tenant_id, credential_name, user_name, password):\n\tsession = db.get_session()\n\ttry:\n\t\tcred = session.query(network_models_v2.Credential).filter_by(tenant_id=tenant_id).filter_by(credential_name=credential_name).one()\n\t\traise c_exc.CredentialAlreadyExists(credential_name=credential_name, tenant_id=tenant_id)\n\texcept exc.NoResultFound:\n\t\tcred = network_models_v2.Credential(tenant_id, credential_name, user_name, password)\n\t\tsession.add(cred)\n\t\tsession.flush()\n\t\treturn cred\n",["adds a qos to tenant association ."]]
["def uninstall(pecls):\n\tif isinstance(pecls, six.string_types):\n\t\tpecls = [pecls]\n\treturn _pecl('uninstall\t{0}'.format(_cmd_quote('\t'.join(pecls))))\n",["remove one or more packages ."]]
["def removeElementFromListTable(element, key, listTable):\n\tif (key not in listTable):\n\t\treturn\n\telementList = listTable[key]\n\tif (len(elementList) < 2):\n\t\tdel listTable[key]\n\t\treturn\n\tif (element in elementList):\n\t\telementList.remove(element)\n",["remove an element from the list table ."]]
["def _reraise_translated_image_exception(image_id):\n\t(_exc_type, exc_value, exc_trace) = sys.exc_info()\n\tnew_exc = _translate_image_exception(image_id, exc_value)\n\tsix.reraise(type(new_exc), new_exc, exc_trace)\n",["transform the exception for the image but keep its traceback intact ."]]
["def parse_qsl(qs, keep_blank_values=0, strict_parsing=0):\n\tpairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]\n\tr = []\n\tfor name_value in pairs:\n\t\tif ((not name_value) and (not strict_parsing)):\n\t\t\tcontinue\n\t\tnv = name_value.split('=', 1)\n\t\tif (len(nv) != 2):\n\t\t\tif strict_parsing:\n\t\t\t\traise ValueError, ('bad\tquery\tfield:\t%r' % (name_value,))\n\t\t\tif keep_blank_values:\n\t\t\t\tnv.append('')\n\t\t\telse:\n\t\t\t\tcontinue\n\t\tif (len(nv[1]) or keep_blank_values):\n\t\t\tname = unquote(nv[0].replace('+', '\t'))\n\t\t\tvalue = unquote(nv[1].replace('+', '\t'))\n\t\t\tr.append((name, value))\n\treturn r\n",["parse a query given as a string argument ."]]
["def get_plural(locale=LC_CTYPE):\n\tlocale = Locale.parse(locale)\n\ttry:\n\t\ttup = PLURALS[str(locale)]\n\texcept KeyError:\n\t\ttry:\n\t\t\ttup = PLURALS[locale.language]\n\t\texcept KeyError:\n\t\t\ttup = DEFAULT_PLURAL\n\treturn _PluralTuple(tup)\n",["a tuple with the information catalogs need to perform proper pluralization ."]]
["def errno_from_exception(e):\n\tif hasattr(e, 'errno'):\n\t\treturn e.errno\n\telif e.args:\n\t\treturn e.args[0]\n\telse:\n\t\treturn None\n",["provides the errno from an exception object ."]]
["def inverse_cosine_transform(F, k, x, **hints):\n\treturn InverseCosineTransform(F, k, x).doit(**hints)\n",["compute the unitary ."]]
["def arg(*args, **kwargs):\n\tkwargs = dict(((k, v) for (k, v) in six.iteritems(kwargs) if (not k.startswith('__'))))\n\tret = {'args': args, 'kwargs': kwargs}\n\treturn ret\n",["decorator for cli args ."]]
["def __virtual__():\n\tif (HAS_NAPALM and ('proxy' in __opts__)):\n\t\treturn __virtualname__\n\telse:\n\t\treturn (False, 'The\tnetwork\tNTP\tstate\t(netntp)\tcannot\tbe\tloaded:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tNAPALM\tor\tproxy\tcould\tnot\tbe\tloaded.')\n",["only load if boto is available ."]]

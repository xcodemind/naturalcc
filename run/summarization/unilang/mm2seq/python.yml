##################### default args #####################
common:
  thread_num: 2 # load and process data with multi-process
  seed: 666 # init seed for numpy/torch/random/scipy and so on
  device: 0 # GPU device. if None, as CPU

  init_weights: ~  # None，xulu， xulu2，xulu3
  #init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/ruby/train_sl/tok-bs128-lr0.001-attngeneral-pointerTrue-ep50.pt'
  #init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/ruby/train_sl/tok-bs128-lr0.001-attndot-pointerFalse-ep50.pt'
  #init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/ruby/train_sl/tok-bs128-lr0.001-attndot-pointerTrue-ep50.pt'
  #init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/ruby/train_sl/tok8tree-bs128-lr0.001-attnNone-pointerFalse-ep50.pt'
  #init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/ruby/train_ac/tok-bs128-lr0.0001-attnNone-pointerFalse-ep50.pt'
  #init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/ruby/train_sc/tok-bs128-lr0.001-attnNone-pointerFalse-ep50.pt'
  #init_weights: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/ruby/train_pg/tok-bs128-lr0.001-attnNone-pointerFalse-ep50.pt'

  init_bound: 1e-1
  init_normal_std: 1e-4
  result_table_format: 'github' # latex (https://pypi.org/project/tabulate/)

dataset:
  #  dataset_dir: '/data/wanyao/yang/ghproj_d/GitHub/datasetv2/key/200'
  dataset_dir: '/data/wanyao/yang/ghproj_d/GitHub/datasetv2/key/100_small'
  # save your model and eval files
  # save_dir: 'C:\Users\GS65_2070mq\Documents\GitHub\naturalcodev2_save'
  save_dir: '/data/wanyao/yang/ghproj_d/GitHub/naturalcodev2'
  ##################### dataset and other args #####################
  # dict files(tok, tree, path sbt, sbt2, code_tokens, docstring_tokens, method)
  # load from code_modalities

  tree_leaf_subtoken: True
  portion: ~ # 0.4 # train data portion, 0~1.0
  leaf_path_k: 10
  # if you need 2 or more than 2 languages to merge together before feeding networks, set it as True
  # generally used for transfer reinforcement learning (TRL)
  merge_xlng: False
  source:
    dataset_lng:
#      - 'java8javascript8php8python'
#      - 'python'
      - 'ruby'
#      - 'javascript'
#      - 'php'
#      - 'java'
#      - 'go'


    mode:
      - 'train'
      - 'valid'
      - 'test'

  target:
    ~


training:
  ##################### model args #####################
  #  summarization: tok, ast, path sbt, sbt2
  #  retrieval: code_tokens, docstring_tokens, method
  code_modalities:
    - tok
#    - sbt
    - ast
    - path

  train_epoch: 50
  batch_size: 2
  log_interval: 5 # write log info per log_interval iteration

  # network: encoder
  rnn_type: 'LSTM' # RNN type: 'GRU','LSTM', LSTM as default
  rnn_layer_num: 1 # RNN layer num
  rnn_hidden_size: 512 # RNN hidden size
  rnn_bidirectional: False

  embed_size: 300 # word-embedding size
  embed_pooling: ~

  tree_lstm_cell_type: 'nary' # DGL tree LSTM cell, if nary -> TreeLSTMCell, else    -> ChildSumTreeLSTMCell
  code_modal_transform: True # code modalities transform with FCs
  conv2d_out_channels: 512
  conv2d_kernels:
    - 2
    - 3
    - 4
    - 5
  hash_activation: 'sign'

  # encoder RNN's output as decoder RNN's hidden_state(h, c)
  # 1) None, for zero_init as decoder RNN's hidden_state
  # 2) h, for only encoder RNN's output as decoder RNN's hidden_state(h)
  # 3) c, for only encoder RNN's output as decoder RNN's hidden_state(c)
  # 3) hc, for encoder RNN's output as decoder RNN's hidden_state(h, c)
  enc_hc2dec_hc: 'h'

  # network: decoder
  attn_type: ~ # ~, general, dot, mlp, intra
  attn_unit: 512
  self_attn_size: 50
  intra_attn_eps: 1e-12
  pointer: False # pointer-generator
  max_predict_length: 65 # max generation length for decoder
  dropout: 0.0 # dropout


# inference
testing:
  beam_size: 10
  max_predict_length: ~ # max generation length for decoder

  metrics:
    - 'bleu'
    #    - 'meteor'
    #    - 'rouge'
    - 'cider'

# supervised learning
sl:
  optim: 'Adam'
  lr: 1e-3
  lr_gamma: 0.1 # 0.1
  lr_milestones: # epochs when lr => lr * lr_gamma
    - 20 # 1e-4
    - 40 # 1e-5
  warmup_factor: 0.2
  warmup_epochs: 1

# supervised learning
pg:
  optim: 'Adam'
  lr: 1e-4
  reward_func: 'bleu'

#  lr_gamma: 0.1 # 0.1
#  lr_milestones: # epochs when lr => lr * lr_gamma
#    - 20 # 1e-4
#    - 40 # 1e-5
#  warmup_factor: 0.2
#  warmup_epochs: 1

sc:
  optim: 'Adam'
  lr: 1e-4
  reward_func: 'bleu'


ac:
  optim: 'Adam'
  optim_critic: 'Adam'
  lr: 1e-4
  lr_critic: 1e-4
  #  init_weights_critic: ~ # ~ None，xulu， xulu2，xulu3
  init_weights_critic: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/ruby/train_critic/critic-tok-bs128-lr0.001-attnNone-pointerFalse-ep50.pt'
  train_epoch_critic: 50
  train_epoch_ac: 50

gan:
  optim: 'Adam'
  optim_disc: 'Adam'
  lr: 1e-4
  lr_disc: 1e-4
  comment_fake_path: '/data/wanyao/yang/ghproj_d/GitHub/datasetv2/key/100/ruby/comment_fake'
  init_weights_disc: ~ # ~ None，xulu， xulu2，xulu3
  #  init_weights_disc: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/ruby/train_critic/critic-tok-bs128-lr0.001-attnNone-pointerFalse-ep50.pt'
  train_epoch_disc: 50
  train_epoch_gan: 50

arel:
  optim: 'Adam'
  optim_reward_model: 'Adam'
  lr: 1e-4
  lr_reward_model: 1e-4
  init_weights_reward_model: ~ # ~ None，xulu， xulu2，xulu3
  #  init_weights_reward_model: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization/mm2seq/ruby/train_critic/critic-tok-bs128-lr0.001-attnNone-pointerFalse-ep50.pt'
  train_epoch_reward_model: 50
  train_epoch_arel: 50

  D_iter: 100
  G_iter: 100
  always: False
# fine tuning


# meta learning
meta:
  meta_step: 5
  meta_iteration: 10
  meta_optim: 'Adam' # meta-optimizer
  meta_lr: 1e-3 # meta-learning rate



dtrl:
  # if you need 2 or more than 2 languages to merge together before feeding networks, set it as True
  # generally used for depp transfer reinforcement learning (DTRL)
  merge_xlng: True

  optim: 'Adam'
  lr: 1e-3
  lr_gamma: 0.1 # 0.1
  lr_milestones: # epochs when lr => lr * lr_gamma
    - 20 # 1e-4
    - 40 # 1e-5
  warmup_factor: 0.2
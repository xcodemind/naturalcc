##################### default args #####################
common:
  thread_num: 8  # load and process data with multi-process
  seed: 666 # init seed for numpy/torch/random/scipy and so on
  device: 5       # GPU device. if None, as CPU  # moved to s.yml

#  init_weights: ~  # None  c
  init_weights: '/data/sjd/d/p_d/fse20all/100_small/result/codenn/ruby_p0.2/train_sl/model_sl_codenn_e2_lng_ruby.pt'


  init_bound: 0.35
#  init_normal_std: 1e-4
  result_table_format: 'github' # latex (https://pypi.org/project/tabulate/)


dataset:

  # 100_small
  save_dir: '/data/sjd/d/p_d/fse20all/100_small/result'  # realdoctor
  dataset_dir: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/key/100_small' # realdoctor
  #dir: '/data/wy/ghproj_d/fse20all/100_small' # rtx
  #dataset_dir: '/data/wanyao/yang/ghproj_d/GitHub/datasetv2/key/100_small' # rtx

  ##################### dataset and other args #####################
  # dict files(tok, tree, path sbt, sbt2, code_tokens, docstring_tokens, method)
  # load from code_modalities

#  tree_leaf_subtoken: True
  portion: 0.2  # 0.4 # train data portion, 0~1.0
#  leaf_path_k: 10
  # if you need 2 or more than 2 languages to merge together before feeding networks, set it as True
  # generally used for transfer reinforcement learning (TRL)
#  merge_xlng: False
  source:
    dataset_lng:
#      - 'python'
      - 'ruby'
#      - 'javascript'
#      - 'php'
#      - 'java'
#      - 'go'

    mode:
      - 'train'
      - 'valid'
      - 'test'

  target:
    ~


training:
  ##################### model args #####################
  #  summarization: tok, ast, path sbt, sbt2
  #  retrieval: code_tokens, docstring_tokens, method
  code_modalities:
    - tok


#  train_epoch: 50
  batch_size: 100
  log_interval: 5 # write log info per log_interval iteration

#  all_epoch: 1000

  # network: encoder
#  rnn_type: 'LSTM' # RNN type: 'GRU','LSTM', LSTM as default
#  rnn_layer_num: 1 # RNN layer num
#  rnn_hidden_size: 512 # RNN hidden size # setted in naturalcodev2/src/module/summarization/codenn/decoder.py
#  rnn_bidirectional: False
#  embed_size: 300 # word-embedding size  # setted in naturalcodev2/src/module/summarization/codenn/decoder.py
#  tree_lstm_cell_type: 'nary' # DGL tree LSTM cell, if nary -> TreeLSTMCell, else    -> ChildSumTreeLSTMCell
#  code_modal_transform: True # code modalities transform with FCs
#  conv2d_out_channels: 512
#  conv2d_kernels:
#    - 2
#    - 3
#    - 4
#    - 5

  # encoder RNN's output as decoder RNN's hidden_state(h, c)
  # 1) None, for zero_init as decoder RNN's hidden_state
  # 2) h, for only encoder RNN's output as decoder RNN's hidden_state(h)
  # 3) c, for only encoder RNN's output as decoder RNN's hidden_state(c)
  # 3) hc, for encoder RNN's output as decoder RNN's hidden_state(h, c)
#  enc_hc2dec_hc: 'h'

  # network: decoder
#  attn_type: ~ # ~, general, dot, mlp, intra
#  attn_unit: 512
#  self_attn_size: 50
#  intra_attn_eps: 1e-12
  pointer: False # pointer-generator, True, False
  max_predict_length: 65  # max generation length for decoder
  dropout: 0.5



# inference
testing:
  beam_size: 10
#  max_predict_length: ~ # max generation length for decoder
  metrics:
    - 'bleu'
    - 'cider'
#    - 'rouge'
#    - 'meteor'

# supervised learning
sl:
#  optim: 'Adam'
  lr: 0.5
#  lr_gamma: ~
#  lr_milestones:  ~# epochs when lr => lr * lr_gamma
##    - 20 # 1e-4
##    - 40 # 1e-5
#  warmup_factor: ~
#  warmup_epochs: ~



##################### default args #####################
common:
  thread_num: 2 # （固定，不要调）load and process data with multi-process
  seed: 666 # （固定，不要调）init seed for numpy/torch/random/scipy and so on
  device: 2 # GPU device. if None, as CPU

#  init_weights: ~  # None, xulu, xulu2, xulu3
  init_weights: '/data/sjd/d/p_d/fse20all/100_small/result/mm2seq/FT_prt_php_ft_ruby_p0.4/train_ft/tok8path-bs128-lr0.0004-attndot-pointerTrue-ttSLTrainer-best-bleu1.pt'

  init_bound: 1e-1 # （固定，不要调）
  init_normal_std: 1e-4 # （固定，不要调）
  result_table_format: 'github' # （固定，不要调）latex (https://pypi.org/project/tabulate/)


dataset:
  # 100_small
  save_dir: '/data/sjd/d/p_d/fse20all/100_small/result_dtrl_lr4e-4'  # realdoctor
  dataset_dir: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/key/100_small' # realdoctor

#  dataset_dir: '/data/wanyao/yang/ghproj_d/GitHub/datasetv2/key/100_small'
#  save_dir: '/data/wanyao/ghproj_d/naturalcodev2/datasetv2/result/summarization'

  ##################### dataset and other args #####################
  # dict files(tok, tree, path sbt, sbt2, code_tokens, docstring_tokens, method)
  # load from code_modalities

  tree_leaf_subtoken: True
  portion: 0.4          # 0.4 # train data portion, 0~1.0
  leaf_path_k: 30 #（固定，不要调）
  source:
    dataset_lng:
      - 'php'
      - 'ruby'

    mode:
      - 'train'

  target:
    dataset_lng:
      - 'ruby'

    mode:
      - 'train'
      - 'valid'
      - 'test'


training:
  ##################### model args #####################
  #  summarization: tok, ast, path sbt, sbt2
  #  retrieval: code_tokens, docstring_tokens, method
  code_modalities:
    - tok
    - path

  train_epoch: 50 # （固定，不要调）50已经足够，一般20-30epoch就基本收敛了
  batch_size: 128 # 128
  log_interval: 5 # （固定，不要调）write log info per log_interval iteration

  # network: encoder
  rnn_type: 'LSTM' # （固定，不要调）RNN type: 'GRU','LSTM', LSTM as default
  rnn_layer_num: 1 # （固定，不要调）RNN layer num
  rnn_hidden_size: 512 # （固定，不要调）RNN hidden size
  rnn_bidirectional: True  # （固定，不要调）
  embed_size: 300 # （固定，不要调）word-embedding size
  tree_lstm_cell_type: 'nary' # （固定，不要调）DGL tree LSTM cell, if nary -> TreeLSTMCell, else    -> ChildSumTreeLSTMCell
  code_modal_transform: False # （固定，不要调）False # code modalities transform with FCs
  conv2d_out_channels: 512 # （固定，不要调）
  conv2d_kernels: # （固定，不要调）
    - 2
    - 3
    - 4
    - 5

  # encoder RNN's output as decoder RNN's hidden_state(h, c)
  # 1) None, for zero_init as decoder RNN's hidden_state
  # 2) h, for only encoder RNN's output as decoder RNN's hidden_state(h)
  # 3) c, for only encoder RNN's output as decoder RNN's hidden_state(c)
  # 3) hc, for encoder RNN's output as decoder RNN's hidden_state(h, c)
  enc_hc2dec_hc: 'h' # （固定，不要调）'hc', 'h'

  # network: decoder
  attn_type: dot # ~, general, dot, mlp, intra
  attn_unit: 512 # （固定，不要调）
  self_attn_size: 50 # （固定，不要调）
  #  intra_attn_eps: 1e-12
  pointer: True # pointer-generator, True, False
  max_predict_length: 65  # （固定，不要调）max generation length for decoder
  dropout: 0.2 # （固定，不要调）dropout
  decoder_input_feed: False # （固定，不要调）True, False

  # used to create the corresponding saved folder
  # ~, enc_hc2dec_hc, optim_AdamW, optim_Adagrad, optim_RMSprop, optim_SGD, decoder_input_feed_True
  # lr_0.01, lr_0.0001, lr_0.0004, lr_0.001_milestones1040,
  # max_grad_norm1.0, max_grad_norm10, max_grad_norm20, max_grad_norm100,
  # dropout_0.2, dropout_0.4, dropout_0.6, dropout_0.8,
  # rnn_layer_num_2, rnn_bidirectional_True
  # tok8path_lr_0.001, tok8path_lr_0.0001
  # tok8path_lr0.01_gamma0.1_milestones1040, tok8path_lr0.001_gamma0.5_milestones1040, tok8path_lr0.001_gamma0.5_milestones10203040,     tok8path_lr0.0004_gamma0.5_milestones2040, tok8path_lr0.0004_gamma0.5_milestones2040_code_modal_transformFalse
  #  tok8path_lr0.001ep20-lr0.0004
  #  attnDot, pointerTrue, tok8path_attnDot, tok8path_pointerTrue
  tuning: ~ # 这里的tuning是parameter tuning，我在这里加一个组合名字后，我的模型会存到相应的目录中，因为我们现在的model_name有点简单，容易重名字


# inference
testing:
  beam_size: 10
  max_predict_length: ~ # max generation length for decoder
  metrics:
    - 'bleu'
    - 'cider'
#    - 'rouge'
#    - 'meteor'

# supervised learning
sl:
  optim: 'Adam' # （固定，不要调）'Adam', 'AdamW', 'Adagrad', 'RMSprop', 'SGD'

  # default
  lr: 4e-4 # 默认最优
  lr_gamma: 0.5 # （固定，不要调）0.1
  lr_milestones: # 默认最优，epochs when lr => lr * lr_gamma
    - 20 # 1e-4
    - 40 # 1e-5

  warmup_factor: 0.2 #（固定，不要调）
  warmup_epochs: -1 # （固定，不要调）-1: no warmup
  max_grad_norm: -1 # （固定，不要调）-1: no gradient clips  1.0, 10, 20, 100

# policy gradient
pg:
  optim: 'Adam'
  lr: 4e-4
  max_grad_norm: -1 # -1: no gradient clips  1.0
  reward_func: 'bleu'
  rl_weight: 1 # 0.8
#  lr_gamma: 0.1 # 0.1
#  lr_milestones: # epochs when lr => lr * lr_gamma
#    - 20 # 1e-4
#    - 40 # 1e-5
#  warmup_factor: 0.2
#  warmup_epochs: 1

# self-critical
sc:
  optim: 'Adam'
  lr: 4e-4
  max_grad_norm: -1 # -1: no gradient clips  1.0
  reward_func: 'bleu'
  #  rl_weight: 0.8
  rl_weight: 1




# meta learning
meta:
  meta_step: 5
  meta_iteration: 10
  meta_optim: 'Adam' # meta-optimizer
  meta_lr: 1e-3 # meta-learning rate


# deep transfer reinforcement learning
dtrl:
  # if you need 2 or more than 2 languages to merge together before feeding networks, set it as True
  # generally used for transfer reinforcement learning (TRL)
  merge_xlng: True
  # best pair for eta and zeta: 0.5, 0.9

  #  0.1, 0.9
  #  | 0.2311 | 0.0758 | 0.021 | 0.0066 |   0.1112 | 0.2675 | 0.0666 | 0.0198 | 0.0054 | 0.2429 |  0.5043 |
  #  | 0.2311 | 0.0758 | 0.021 | 0.0066 |   0.1112 | 0.2675 | 0.0666 | 0.0198 | 0.0054 | 0.2429 |  0.5043 |

  #  0.5, 0.1
  #  | 0.2293 | 0.0743 | 0.0201 | 0.0059 |   0.1102 | 0.2678 | 0.0662 | 0.0198 | 0.0053 | 0.2427 |  0.4919 |
  #  | 0.2293 | 0.0743 | 0.0201 | 0.0059 |   0.1102 | 0.2678 | 0.0662 | 0.0198 | 0.0053 | 0.2427 |  0.4919 |

  #  0.4, 0.9
  #  | 0.2339 | 0.0768 | 0.0225 | 0.007 |   0.1116 | 0.2682 | 0.0688 | 0.022 | 0.0066 | 0.2452 |  0.5081 |
  #  | 0.2339 | 0.0768 | 0.0225 | 0.007 |   0.1116 | 0.2682 | 0.0688 | 0.022 | 0.0066 | 0.2452 |  0.5081 |

  #  0.5, 0.9
  #| 0.235 | 0.0782 | 0.0234 | 0.0079 |   0.1128 | 0.2693 | 0.0691 | 0.0224 | 0.0071 | 0.246 |  0.5106 |
  #| 0.235 | 0.0782 | 0.0234 | 0.0079 |   0.1128 | 0.2693 | 0.0691 | 0.0224 | 0.0071 | 0.246 |  0.5106 |

  #  0.6, 0.9
  #  | 0.2335 | 0.0778 | 0.0223 | 0.0073 |   0.1125 | 0.269 | 0.0695 | 0.0224 | 0.0072 | 0.2468 |  0.5033 |
  #  | 0.2335 | 0.0778 | 0.0223 | 0.0073 |   0.1125 | 0.269 | 0.0695 | 0.0224 | 0.0072 | 0.2468 |  0.5033 |

  #  0.5, 0.99
  #  | 0.2328 | 0.0769 | 0.0239 | 0.0074 |   0.1113 | 0.2677 | 0.0695 | 0.0224 | 0.007 | 0.2446 |  0.5011 |
  #  | 0.2328 | 0.0769 | 0.0239 | 0.0074 |   0.1113 | 0.2677 | 0.0695 | 0.0224 | 0.007 | 0.2446 |  0.5011 |

  #  0.7, 0.9
  #  | 0.2337 | 0.078 | 0.0229 | 0.0075 |   0.1123 | 0.2683 | 0.0706 | 0.0232 | 0.0077 | 0.2463 |  0.5051 |
  #  | 0.2269 | 0.0709 | 0.0215 | 0.0078 |   0.1093 | 0.2644 | 0.0659 | 0.0227 | 0.0074 | 0.244 |  0.4905 |

  #  0.9, 0.1
  #  | 0.2166 | 0.0675 | 0.0206 | 0.007 |   0.1083 | 0.2615 | 0.0657 | 0.0232 | 0.0074 | 0.2407 |   0.496 |
  #  | 0.2166 | 0.0675 | 0.0206 | 0.007 |   0.1083 | 0.2615 | 0.0657 | 0.0232 | 0.0074 | 0.2407 |   0.496 |

  #  0.9, 0.9
  #  | 0.2263 | 0.0708 | 0.0238 | 0.0097 |   0.1093 | 0.255 | 0.0693 | 0.0237 | 0.0094 | 0.2322 |  0.4618 |
  #  | 0.2128 | 0.0696 | 0.022 | 0.0073 |   0.1081 | 0.2588 | 0.068 | 0.0226 | 0.007 | 0.2396 |  0.5129 |

  #  0.99, 0.99
  #  | 0.2097 | 0.0633 | 0.017 | 0.0053 |    0.106 | 0.2573 | 0.0573 | 0.0164 | 0.0046 | 0.2368 |  0.4925 |
  #  | 0.2097 | 0.0633 | 0.017 | 0.0053 |    0.106 | 0.2573 | 0.0573 | 0.0164 | 0.0046 | 0.2368 |  0.4925 |

  #  portion=0.01
  #  0.5, 0.9
  #  | 0.1581 | 0.0087 | 0.0007 |    0 |   0.0563 | 0.1626 | 0.0085 |    0 |    0 | 0.1499 |  0.1114 |
  #  | 0.1492 | 0.0186 |    0 |    0 |   0.0655 | 0.1673 | 0.0174 |    0 |    0 | 0.1544 |  0.2073 |

  #  0.9, 0.9
  #  | 0.1267 | 0.0071 |    0 |    0 |   0.0569 | 0.1572 | 0.0125 |    0 |    0 | 0.1477 |  0.1759 |
  #  | 0.1267 | 0.0071 |    0 |    0 |   0.0569 | 0.1572 | 0.0125 |    0 |    0 | 0.1477 |  0.1759 |

  eta: 0.5
  zeta: 0.9
  reward_func: 'bleu'

  optim: 'Adam' # （固定，不要调）'Adam', 'AdamW', 'Adagrad', 'RMSprop', 'SGD'
  # default
  lr: 4e-4 # 默认最优

  #  0.2083	0.0476	0.0111	0.0029	0.0925	0.213	0.0423	0.0124	0.0032	0.1913	0.3284

  #  lr=1e-3
  #| 0.1718 | 0.0258 | 0.0041 | 0.0003 |   0.0782 | 0.1608 | 0.0198 | 0.0041 | 0.0004 | 0.1452 |  0.2527 |
  #| 0.1718 | 0.0258 | 0.0041 | 0.0003 |   0.0782 | 0.1608 | 0.0198 | 0.0041 | 0.0004 | 0.1452 |  0.2527 |

  #  lr=4e-4
  #  | 0.2072 | 0.0491 | 0.0137 | 0.0039 |   0.0934 | 0.2112 | 0.044 | 0.013 | 0.0041 | 0.191 |  0.3603 |
  #  | 0.2072 | 0.0491 | 0.0137 | 0.0039 |   0.0934 | 0.2112 | 0.044 | 0.013 | 0.0041 | 0.191 |  0.3603 |

  #  lr=1e-4
  #  | 0.2068 | 0.0502 | 0.0123 | 0.0034 |   0.0933 | 0.2136 | 0.0444 | 0.0131 | 0.0036 | 0.1938 |  0.3413 |
  #  | 0.2068 | 0.0502 | 0.0123 | 0.0034 |   0.0933 | 0.2136 | 0.0444 | 0.0131 | 0.0036 | 0.1938 |  0.3413 |

  #  lr: 5e-5
  #  | 0.207 | 0.0501 | 0.012 | 0.0032 |   0.0929 | 0.213 | 0.0446 | 0.013 | 0.0037 | 0.1931 |  0.3389 |
  #  | 0.2066 | 0.0498 | 0.0123 | 0.0034 |   0.0932 | 0.214 | 0.0444 | 0.0132 | 0.0036 | 0.194 |  0.3408 |


  lr_gamma: 0.5 # （固定，不要调）0.1
  lr_milestones: # 默认最优，epochs when lr => lr * lr_gamma
    - 20 # 1e-4
    - 40 # 1e-5

  warmup_factor: 0.2 #（固定，不要调）
  warmup_epochs: -1 # （固定，不要调）-1: no warmup
  max_grad_norm: -1 # （固定，不要调）-1: no gradient clips  1.0, 10, 20, 100



 